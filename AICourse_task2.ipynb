{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNOHt7WN2Sjc4N0LVVq9/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zendellll/Road2_AI_Course/blob/main/AICourse_task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task\n",
        "Initiate ChatGPT to work on Google colab, python and keras.\n",
        "We will build a machine to convert celsius deg to far. deg.\n",
        "We will create a DS with a single feature - deg in cel.\n",
        "Also, a single response - deg in far.\n",
        "Effectively, creating a cel. vector and a far. vector.\n",
        "Vector length should be at least 10.\n",
        "We will build a cel. vector and a corresponding far. vector.\n",
        "\n",
        "This is a regression problem, meaning there's no need for an activation function.\n",
        "\n",
        "The network will have a single neuron and a single layer.\n",
        "The network gets as input the cel. vector, and it's response is the far. vector.\n",
        "We will make sure to split the DS into 3 section - train, test and validate.\n",
        "We will ask Keras, after building the neuron, to check the error (Cost) using mean-square-error, and a learning rate of 0.01 (can be changed), with an Adam optimization algorithm.\n",
        "Then we will ask to train the network.\n",
        "The training will be done with 500 epochs, not specifing batch size (could set it to 1).\n",
        "We will tell this to ChatGPT.\n",
        "\n",
        "The next step is to ask for a prediction. We take a certain cel. value (not existant in the DS), and ask for a prediction.\n",
        "\n",
        "Afterwards, we will ask for a represntation of the weights.\n",
        "\n",
        "After seeing our networks are pretty shitty, our goal is to improve our network somehow (I'm guessing we could increase the DS size).\n",
        "\n",
        "Make another experiment with increasing the network size (increasing the neoron and layers amount).\n",
        "\n",
        "We will include a graph describing the learning process. The X axis is the epoch num and the Y axis is the MSE\n",
        "\n",
        "The goal is to build a network which will obtain the corrcet weights (a and b). We can check our results by comparing them to the right formula."
      ],
      "metadata": {
        "id": "q6LUXOvNstGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1:\n",
        "You are my software engineer assistant. We will be doing some AI programming, working in Google Colab, using python and keras.\n",
        "We will be building a ML models to convert degrees from celsius to fahrenheit.\n",
        "First, I need you to create a small dataset, with a single feature - degrees in celsius. Also, it should have a single response - degrees in fahrenheit.\n",
        "I need you to write a script to create the described dataset, and store it as a vector. You can choose any values for inputs in the dataset, as long as each input has the right pair of celsius and fahrenheit degrees.\n",
        "\n",
        "## P2:\n",
        "good. now i need you to save this into a file, which will be suitable for training and testing an AI network"
      ],
      "metadata": {
        "id": "5xBtbNT8WIHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrGm8lSbsryk",
        "outputId": "4eeda200-1122-4ca6-d47e-9f6074ed432a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Celsius\tFahrenheit\n",
            "-------------------\n",
            "-10.0\t14.0\n",
            "0.0\t32.0\n",
            "10.0\t50.0\n",
            "20.0\t68.0\n",
            "30.0\t86.0\n",
            "40.0\t104.0\n",
            "50.0\t122.0\n",
            "60.0\t140.0\n",
            "70.0\t158.0\n",
            "80.0\t176.0\n",
            "Dataset saved to 'celsius_to_fahrenheit_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to convert Celsius to Fahrenheit\n",
        "def celsius_to_fahrenheit(celsius):\n",
        "    return (9/5) * celsius + 32\n",
        "\n",
        "# Generating a small dataset\n",
        "celsius_values = np.array([-10, 0, 10, 20, 30, 40, 50, 60, 70, 80])\n",
        "fahrenheit_values = celsius_to_fahrenheit(celsius_values)\n",
        "\n",
        "# Storing the dataset as vectors\n",
        "dataset = np.column_stack((celsius_values, fahrenheit_values))\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "np.savetxt('celsius_to_fahrenheit_dataset.csv', dataset, delimiter=',')\n",
        "\n",
        "# Displaying the dataset\n",
        "print(\"Celsius\\tFahrenheit\")\n",
        "print(\"-------------------\")\n",
        "for c, f in dataset:\n",
        "    print(f\"{c}\\t{f}\")\n",
        "\n",
        "print(\"Dataset saved to 'celsius_to_fahrenheit_dataset.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1:\n",
        "Good, now we will be using this dataset to train an AI network to be able to get degrees in celsius as input, and output the corresponding degrees in fahrenheit.\n",
        "\n",
        "First, we will make sure to split the dataset we have previously built into 3 section - train, test and validate.\n",
        "\n",
        "Next, we will build the network using Keras. The network will have a single neuron and a single layer. The network gets as input the celsius vector, and it's response is the fahrenheit. vector.  We will check the error (cost) using mean-square-error, a learning rate of 0.01 with an Adam optimization algorithm.\n",
        "\n",
        "Afterwards, we will train the network. The training will be done with 500 epochs, with batch size of 1."
      ],
      "metadata": {
        "id": "uJpNz9X2Z60R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "dataset = np.loadtxt('celsius_to_fahrenheit_dataset.csv', delimiter=',')\n",
        "\n",
        "# Split the dataset into features (X) and labels (y)\n",
        "X = dataset[:, 0]  # Celsius values\n",
        "y = dataset[:, 1]  # Fahrenheit values\n",
        "\n",
        "# Split the dataset into training, testing, and validation sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Build the neural network model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(1, input_shape=[1])  # Single neuron, single layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=500, batch_size=1, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# Display the training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-bG1-vASYjwV",
        "outputId": "b2999e5d-f378-4599-81f2-621490fb7792"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 52ms/step - loss: 14957.2217 - val_loss: 592.7062\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 14323.7217 - val_loss: 595.8752\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 13792.6689 - val_loss: 599.2902\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 13279.8096 - val_loss: 602.9017\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 12796.2344 - val_loss: 606.7728\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 12281.0518 - val_loss: 610.6489\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 11904.2158 - val_loss: 615.1941\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 11448.5820 - val_loss: 619.8379\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 10933.6787 - val_loss: 624.1630\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 10436.6787 - val_loss: 628.1116\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 10135.8760 - val_loss: 633.0291\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 9627.8164 - val_loss: 637.2615\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 9266.2207 - val_loss: 641.8894\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 8916.3896 - val_loss: 646.8468\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 8484.6641 - val_loss: 651.2870\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 8206.3154 - val_loss: 656.5728\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 7795.2539 - val_loss: 661.2732\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 7475.8237 - val_loss: 666.1930\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 7200.4702 - val_loss: 671.6747\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 6856.0591 - val_loss: 676.8580\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 6521.1157 - val_loss: 681.6207\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 6279.4297 - val_loss: 687.0707\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 5967.5610 - val_loss: 692.0867\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 5711.3140 - val_loss: 697.3479\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 5439.5312 - val_loss: 702.4324\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 5248.5728 - val_loss: 708.3757\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 4948.0186 - val_loss: 713.4577\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 4746.3979 - val_loss: 719.0165\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 4515.0083 - val_loss: 724.4362\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 4284.9907 - val_loss: 729.5535\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 4085.9446 - val_loss: 734.7640\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 3880.8948 - val_loss: 739.7781\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 3729.1604 - val_loss: 745.5199\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3531.8152 - val_loss: 750.9301\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 3345.6345 - val_loss: 756.0260\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 3188.2551 - val_loss: 761.2900\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 3042.3025 - val_loss: 766.8052\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 2868.5295 - val_loss: 771.7422\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 2718.4021 - val_loss: 776.4757\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2598.1799 - val_loss: 781.6983\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2457.7773 - val_loss: 786.6443\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 2346.2373 - val_loss: 791.9745\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 2212.9480 - val_loss: 796.8684\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2108.6694 - val_loss: 802.0232\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 2002.7789 - val_loss: 807.2363\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 1885.1388 - val_loss: 811.9331\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1788.5542 - val_loss: 816.6487\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1694.5560 - val_loss: 821.2990\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1600.2550 - val_loss: 825.6423\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1532.9537 - val_loss: 830.6335\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1438.9424 - val_loss: 835.0010\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 1360.8715 - val_loss: 839.1819\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1297.5153 - val_loss: 843.7083\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1226.6669 - val_loss: 848.0302\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1158.4912 - val_loss: 852.0834\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 1101.4518 - val_loss: 856.2776\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 1047.9219 - val_loss: 860.6222\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 987.3566 - val_loss: 864.5433\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 937.5941 - val_loss: 868.4829\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 890.5462 - val_loss: 872.4222\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 838.5436 - val_loss: 875.8203\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 801.2206 - val_loss: 879.5560\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 762.7388 - val_loss: 883.3518\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 724.0031 - val_loss: 887.0170\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 682.9917 - val_loss: 890.1525\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 653.5726 - val_loss: 893.5587\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 619.7415 - val_loss: 896.6550\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 591.5024 - val_loss: 899.7873\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 564.9168 - val_loss: 902.9326\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 536.8345 - val_loss: 905.7458\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 514.2680 - val_loss: 908.6643\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 490.9376 - val_loss: 911.4146\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 471.3560 - val_loss: 914.2769\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 449.7343 - val_loss: 916.7842\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 432.7363 - val_loss: 919.4276\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 414.2822 - val_loss: 921.7667\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 398.1381 - val_loss: 923.9888\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 384.0027 - val_loss: 926.2477\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 370.0834 - val_loss: 928.4349\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 357.3583 - val_loss: 930.4965\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 345.6820 - val_loss: 932.5783\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 335.0248 - val_loss: 934.6608\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 324.5524 - val_loss: 936.6189\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 313.9252 - val_loss: 938.2841\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 305.7617 - val_loss: 940.0325\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 297.8217 - val_loss: 941.6808\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 290.1079 - val_loss: 943.2529\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 282.6863 - val_loss: 944.7661\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 276.3074 - val_loss: 946.2343\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 270.4196 - val_loss: 947.5446\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 264.7552 - val_loss: 948.8531\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 259.8371 - val_loss: 950.0985\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 255.3642 - val_loss: 951.3726\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 250.3746 - val_loss: 952.3892\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 246.6486 - val_loss: 953.4446\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 243.4939 - val_loss: 954.4806\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 239.7969 - val_loss: 955.4694\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 236.9120 - val_loss: 956.3242\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 233.3685 - val_loss: 957.0745\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 231.0063 - val_loss: 957.8369\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 228.5406 - val_loss: 958.4592\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 226.9444 - val_loss: 959.1692\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 224.0423 - val_loss: 959.6315\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 222.9935 - val_loss: 960.2352\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 220.8904 - val_loss: 960.7241\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 219.5597 - val_loss: 961.3058\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 217.4883 - val_loss: 961.5219\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 216.3135 - val_loss: 961.7662\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 215.4769 - val_loss: 962.1799\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 214.8241 - val_loss: 962.6365\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 212.9197 - val_loss: 962.6870\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 212.3049 - val_loss: 962.8887\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 211.7454 - val_loss: 963.1881\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 210.8834 - val_loss: 963.4391\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 209.9737 - val_loss: 963.5908\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 208.9559 - val_loss: 963.4507\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 208.4021 - val_loss: 963.2838\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 208.6089 - val_loss: 963.5588\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 207.5027 - val_loss: 963.5453\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 207.3586 - val_loss: 963.5947\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 206.4988 - val_loss: 963.4912\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 205.9001 - val_loss: 963.1704\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 205.7116 - val_loss: 962.9158\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 205.2933 - val_loss: 962.5844\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 205.1077 - val_loss: 962.4999\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 204.7191 - val_loss: 962.2842\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 204.6988 - val_loss: 962.2942\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 203.9965 - val_loss: 961.7672\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 203.9911 - val_loss: 961.5359\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 203.7346 - val_loss: 961.4341\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 203.2494 - val_loss: 960.9447\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 203.0615 - val_loss: 960.5587\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 202.8759 - val_loss: 960.0085\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 202.6823 - val_loss: 959.6533\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 202.5403 - val_loss: 959.2806\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 202.2266 - val_loss: 958.9734\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 202.2499 - val_loss: 958.7922\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 201.9347 - val_loss: 958.2971\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 201.7834 - val_loss: 957.9471\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 201.4919 - val_loss: 957.2382\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 201.3392 - val_loss: 956.6410\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 201.2592 - val_loss: 956.1855\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 201.0424 - val_loss: 955.6454\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 200.8102 - val_loss: 955.0210\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 200.7086 - val_loss: 954.6080\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 200.6394 - val_loss: 954.2876\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 200.6187 - val_loss: 953.9436\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 200.1273 - val_loss: 953.3511\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 200.0204 - val_loss: 952.5605\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 199.9093 - val_loss: 951.7381\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 199.6821 - val_loss: 951.1519\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 199.6758 - val_loss: 950.3459\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 199.4516 - val_loss: 949.7717\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 199.3723 - val_loss: 949.0466\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 199.3365 - val_loss: 948.8484\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 199.0592 - val_loss: 948.0621\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 198.8121 - val_loss: 947.4490\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 198.7090 - val_loss: 947.0012\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 198.6166 - val_loss: 946.1141\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 198.6014 - val_loss: 945.7096\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 198.3428 - val_loss: 944.8165\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 198.1128 - val_loss: 944.0720\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 198.0264 - val_loss: 943.7216\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 197.7642 - val_loss: 943.1456\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 197.6749 - val_loss: 942.3075\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 197.7036 - val_loss: 941.9233\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 197.3207 - val_loss: 941.2250\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 197.2885 - val_loss: 940.6980\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 197.0224 - val_loss: 939.9800\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 196.9880 - val_loss: 938.9246\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 196.7098 - val_loss: 938.2675\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 196.7188 - val_loss: 937.2358\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 196.4311 - val_loss: 936.5049\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 196.3499 - val_loss: 936.0419\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 196.1286 - val_loss: 935.4619\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 195.9292 - val_loss: 934.6340\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 195.7531 - val_loss: 933.8350\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 195.5707 - val_loss: 933.2825\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 195.5209 - val_loss: 932.7166\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 195.2779 - val_loss: 931.7593\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 195.4829 - val_loss: 930.6277\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 195.0632 - val_loss: 929.7725\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 194.9073 - val_loss: 928.9528\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 194.7178 - val_loss: 928.4613\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 194.4890 - val_loss: 927.8768\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 194.7195 - val_loss: 927.5066\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 194.3776 - val_loss: 926.3704\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 194.1715 - val_loss: 925.3704\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 194.0430 - val_loss: 925.0405\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 193.8441 - val_loss: 923.9449\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 193.5141 - val_loss: 923.3137\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 193.6224 - val_loss: 922.8243\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 193.2870 - val_loss: 921.7302\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 193.0878 - val_loss: 920.8318\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 193.1776 - val_loss: 920.3491\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 192.7274 - val_loss: 919.3641\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 192.4755 - val_loss: 918.6473\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 192.4227 - val_loss: 917.8706\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 192.2208 - val_loss: 916.9183\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 192.0894 - val_loss: 915.9382\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 191.9703 - val_loss: 915.5042\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 191.9039 - val_loss: 915.0397\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 191.6770 - val_loss: 913.8600\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 191.7200 - val_loss: 912.6228\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 191.2116 - val_loss: 911.9570\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 191.0342 - val_loss: 911.2878\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 191.0688 - val_loss: 910.6210\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 190.9031 - val_loss: 909.4647\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 190.5128 - val_loss: 908.8187\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 190.3951 - val_loss: 907.9205\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 190.3654 - val_loss: 907.4352\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 190.1041 - val_loss: 906.8590\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 190.1099 - val_loss: 905.5220\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 189.7331 - val_loss: 904.8531\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 189.6655 - val_loss: 903.7411\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 189.5162 - val_loss: 902.6544\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 189.6503 - val_loss: 902.4581\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 188.9814 - val_loss: 901.4961\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 188.8150 - val_loss: 900.6858\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 188.7196 - val_loss: 900.0564\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 188.6230 - val_loss: 898.8392\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 188.2833 - val_loss: 897.9298\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 188.0519 - val_loss: 897.0322\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 188.2213 - val_loss: 896.4978\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 187.7204 - val_loss: 895.4672\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 187.5075 - val_loss: 894.7476\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 187.3420 - val_loss: 894.0189\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 187.2778 - val_loss: 893.0782\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 187.1404 - val_loss: 891.9802\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 186.9940 - val_loss: 891.2880\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 186.7677 - val_loss: 890.1898\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 186.8596 - val_loss: 888.9934\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 186.6021 - val_loss: 888.7087\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 186.1640 - val_loss: 888.0163\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 186.1302 - val_loss: 887.4680\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 186.1351 - val_loss: 885.9839\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 185.6360 - val_loss: 885.2864\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 185.3669 - val_loss: 884.3365\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 185.2899 - val_loss: 883.5546\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 185.0725 - val_loss: 882.7186\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 184.8323 - val_loss: 881.5779\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 184.6500 - val_loss: 880.7626\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 184.4859 - val_loss: 879.8459\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 184.3567 - val_loss: 879.1820\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 184.1274 - val_loss: 877.9719\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 183.9616 - val_loss: 877.1761\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 183.8804 - val_loss: 875.9691\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 183.5378 - val_loss: 874.9634\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 183.6415 - val_loss: 873.8490\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 183.4376 - val_loss: 873.5900\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 183.1822 - val_loss: 873.0974\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 183.1455 - val_loss: 871.6768\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 183.0150 - val_loss: 871.2742\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 182.7594 - val_loss: 869.8762\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 182.2696 - val_loss: 868.9061\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 182.0629 - val_loss: 868.1061\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 181.9112 - val_loss: 867.3721\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 181.6425 - val_loss: 866.2533\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 181.5295 - val_loss: 865.1371\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 181.5300 - val_loss: 864.6459\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 181.1948 - val_loss: 863.4705\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 180.9449 - val_loss: 862.7469\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 180.9407 - val_loss: 861.4508\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 181.0001 - val_loss: 860.9734\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 180.5557 - val_loss: 859.7277\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 180.1743 - val_loss: 858.6412\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 179.8853 - val_loss: 857.9214\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 179.9085 - val_loss: 857.4368\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 179.5015 - val_loss: 856.4192\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 179.7691 - val_loss: 854.9855\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 179.2128 - val_loss: 853.9398\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 178.9906 - val_loss: 853.2850\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 178.7196 - val_loss: 852.4901\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 178.6953 - val_loss: 851.8236\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 178.3489 - val_loss: 850.6216\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 178.4105 - val_loss: 850.0150\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 177.8636 - val_loss: 848.9175\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 177.6741 - val_loss: 847.8746\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 177.7239 - val_loss: 847.2340\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 177.5139 - val_loss: 845.7530\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 177.1302 - val_loss: 844.9828\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 177.2341 - val_loss: 844.1834\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 177.0395 - val_loss: 843.4208\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 176.5805 - val_loss: 841.9562\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 176.7922 - val_loss: 840.4218\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 176.6851 - val_loss: 840.0410\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 175.9634 - val_loss: 839.1136\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 175.8846 - val_loss: 838.2987\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 175.4965 - val_loss: 837.1077\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 175.2692 - val_loss: 836.0613\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 6ms/step - loss: 175.2853 - val_loss: 835.4182\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 175.0371 - val_loss: 834.4697\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 174.7441 - val_loss: 833.4724\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 174.5610 - val_loss: 832.3013\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 174.3477 - val_loss: 830.9830\n",
            "Epoch 296/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 174.4539 - val_loss: 830.4696\n",
            "Epoch 297/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 173.8815 - val_loss: 829.3394\n",
            "Epoch 298/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 173.7803 - val_loss: 828.5195\n",
            "Epoch 299/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 173.4788 - val_loss: 827.6893\n",
            "Epoch 300/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 173.2275 - val_loss: 826.5568\n",
            "Epoch 301/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 173.1085 - val_loss: 825.6909\n",
            "Epoch 302/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 173.0939 - val_loss: 824.7246\n",
            "Epoch 303/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 172.6338 - val_loss: 823.5408\n",
            "Epoch 304/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 172.4151 - val_loss: 822.4735\n",
            "Epoch 305/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 172.4164 - val_loss: 820.9841\n",
            "Epoch 306/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 172.6341 - val_loss: 820.4576\n",
            "Epoch 307/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 171.8903 - val_loss: 819.2664\n",
            "Epoch 308/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 172.0413 - val_loss: 818.6992\n",
            "Epoch 309/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 171.4460 - val_loss: 817.3110\n",
            "Epoch 310/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 171.4460 - val_loss: 816.6732\n",
            "Epoch 311/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 171.0980 - val_loss: 815.2733\n",
            "Epoch 312/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 171.1665 - val_loss: 813.7707\n",
            "Epoch 313/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 170.6746 - val_loss: 813.0936\n",
            "Epoch 314/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 170.5923 - val_loss: 812.4788\n",
            "Epoch 315/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 170.5424 - val_loss: 810.9462\n",
            "Epoch 316/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 170.1979 - val_loss: 810.4556\n",
            "Epoch 317/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 170.0882 - val_loss: 809.8812\n",
            "Epoch 318/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 169.5320 - val_loss: 808.6004\n",
            "Epoch 319/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 169.7852 - val_loss: 807.9978\n",
            "Epoch 320/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 169.2349 - val_loss: 807.0532\n",
            "Epoch 321/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 168.8623 - val_loss: 805.7845\n",
            "Epoch 322/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 168.7735 - val_loss: 804.8755\n",
            "Epoch 323/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 169.0541 - val_loss: 803.1767\n",
            "Epoch 324/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 168.7874 - val_loss: 802.5988\n",
            "Epoch 325/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 168.4709 - val_loss: 801.1115\n",
            "Epoch 326/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 168.0327 - val_loss: 799.8286\n",
            "Epoch 327/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 167.7951 - val_loss: 799.0939\n",
            "Epoch 328/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 167.5286 - val_loss: 797.8855\n",
            "Epoch 329/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 167.2962 - val_loss: 796.8539\n",
            "Epoch 330/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 167.0293 - val_loss: 795.9952\n",
            "Epoch 331/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 167.1647 - val_loss: 795.3312\n",
            "Epoch 332/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 166.8405 - val_loss: 794.7972\n",
            "Epoch 333/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 166.6060 - val_loss: 793.9069\n",
            "Epoch 334/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 166.8159 - val_loss: 792.0801\n",
            "Epoch 335/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 166.4350 - val_loss: 791.5606\n",
            "Epoch 336/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 165.7514 - val_loss: 790.5809\n",
            "Epoch 337/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 165.5433 - val_loss: 789.3564\n",
            "Epoch 338/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 165.4436 - val_loss: 787.9181\n",
            "Epoch 339/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 165.0979 - val_loss: 786.8442\n",
            "Epoch 340/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 165.0823 - val_loss: 785.5489\n",
            "Epoch 341/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 164.7550 - val_loss: 784.8621\n",
            "Epoch 342/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 164.4352 - val_loss: 783.7727\n",
            "Epoch 343/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 164.2575 - val_loss: 782.7195\n",
            "Epoch 344/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 163.9910 - val_loss: 781.7119\n",
            "Epoch 345/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 163.9072 - val_loss: 780.9781\n",
            "Epoch 346/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 164.0861 - val_loss: 779.4711\n",
            "Epoch 347/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 163.3904 - val_loss: 778.5231\n",
            "Epoch 348/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 163.2125 - val_loss: 777.4977\n",
            "Epoch 349/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 163.0522 - val_loss: 776.4429\n",
            "Epoch 350/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 162.9685 - val_loss: 776.0649\n",
            "Epoch 351/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 162.6206 - val_loss: 774.8555\n",
            "Epoch 352/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 162.9139 - val_loss: 774.4219\n",
            "Epoch 353/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 162.4800 - val_loss: 772.8120\n",
            "Epoch 354/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 161.9922 - val_loss: 772.0280\n",
            "Epoch 355/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 161.7487 - val_loss: 770.5965\n",
            "Epoch 356/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 161.5155 - val_loss: 769.4012\n",
            "Epoch 357/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 161.2838 - val_loss: 768.4109\n",
            "Epoch 358/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 161.0082 - val_loss: 767.4105\n",
            "Epoch 359/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 161.0990 - val_loss: 766.9024\n",
            "Epoch 360/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 160.6079 - val_loss: 766.1333\n",
            "Epoch 361/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 160.3669 - val_loss: 764.6804\n",
            "Epoch 362/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 160.1949 - val_loss: 763.3040\n",
            "Epoch 363/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 159.8974 - val_loss: 762.0652\n",
            "Epoch 364/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 159.8431 - val_loss: 761.2464\n",
            "Epoch 365/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 159.7264 - val_loss: 759.7284\n",
            "Epoch 366/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 159.4793 - val_loss: 759.1697\n",
            "Epoch 367/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 159.1250 - val_loss: 758.3630\n",
            "Epoch 368/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 159.2975 - val_loss: 757.6880\n",
            "Epoch 369/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 158.5560 - val_loss: 756.1146\n",
            "Epoch 370/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 158.3508 - val_loss: 754.8856\n",
            "Epoch 371/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 158.7564 - val_loss: 753.2042\n",
            "Epoch 372/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 158.4082 - val_loss: 752.6619\n",
            "Epoch 373/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 157.8997 - val_loss: 751.9752\n",
            "Epoch 374/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 157.7305 - val_loss: 750.4014\n",
            "Epoch 375/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 157.4997 - val_loss: 748.9308\n",
            "Epoch 376/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 157.1081 - val_loss: 748.2053\n",
            "Epoch 377/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 156.9232 - val_loss: 746.8998\n",
            "Epoch 378/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 156.7053 - val_loss: 746.0001\n",
            "Epoch 379/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 156.3156 - val_loss: 744.9995\n",
            "Epoch 380/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 156.2859 - val_loss: 744.1591\n",
            "Epoch 381/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 155.9635 - val_loss: 743.0753\n",
            "Epoch 382/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 156.0656 - val_loss: 742.4372\n",
            "Epoch 383/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 155.6985 - val_loss: 741.5710\n",
            "Epoch 384/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 155.2713 - val_loss: 740.6381\n",
            "Epoch 385/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 155.0750 - val_loss: 739.4464\n",
            "Epoch 386/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 154.9009 - val_loss: 737.8442\n",
            "Epoch 387/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 154.9239 - val_loss: 736.1371\n",
            "Epoch 388/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 154.7572 - val_loss: 735.6004\n",
            "Epoch 389/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 154.1166 - val_loss: 734.2861\n",
            "Epoch 390/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 154.1449 - val_loss: 733.5535\n",
            "Epoch 391/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 153.8359 - val_loss: 732.7969\n",
            "Epoch 392/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 153.4599 - val_loss: 731.9079\n",
            "Epoch 393/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 153.3227 - val_loss: 730.7897\n",
            "Epoch 394/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 152.9539 - val_loss: 729.5934\n",
            "Epoch 395/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 152.8267 - val_loss: 728.4938\n",
            "Epoch 396/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 153.2966 - val_loss: 726.5366\n",
            "Epoch 397/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 152.3162 - val_loss: 725.6855\n",
            "Epoch 398/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 152.3064 - val_loss: 724.9257\n",
            "Epoch 399/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 151.9572 - val_loss: 723.4123\n",
            "Epoch 400/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 151.6872 - val_loss: 722.2438\n",
            "Epoch 401/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 151.5769 - val_loss: 720.9280\n",
            "Epoch 402/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 151.2885 - val_loss: 720.2195\n",
            "Epoch 403/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 151.0769 - val_loss: 719.0367\n",
            "Epoch 404/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 151.1443 - val_loss: 718.6254\n",
            "Epoch 405/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 150.6256 - val_loss: 717.5453\n",
            "Epoch 406/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 150.4598 - val_loss: 716.0046\n",
            "Epoch 407/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 150.0762 - val_loss: 715.1279\n",
            "Epoch 408/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 150.3550 - val_loss: 713.5344\n",
            "Epoch 409/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 149.7609 - val_loss: 712.2531\n",
            "Epoch 410/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 149.6133 - val_loss: 711.7668\n",
            "Epoch 411/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 149.7054 - val_loss: 711.1697\n",
            "Epoch 412/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 149.3076 - val_loss: 709.4432\n",
            "Epoch 413/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 148.6973 - val_loss: 708.4095\n",
            "Epoch 414/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 148.7582 - val_loss: 707.8460\n",
            "Epoch 415/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 148.5721 - val_loss: 706.2120\n",
            "Epoch 416/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 148.0393 - val_loss: 705.2599\n",
            "Epoch 417/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 147.9794 - val_loss: 703.9498\n",
            "Epoch 418/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 147.7879 - val_loss: 702.5726\n",
            "Epoch 419/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 147.4227 - val_loss: 701.9647\n",
            "Epoch 420/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 147.5003 - val_loss: 701.4581\n",
            "Epoch 421/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 147.2621 - val_loss: 700.5837\n",
            "Epoch 422/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 146.6833 - val_loss: 699.0309\n",
            "Epoch 423/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 146.4014 - val_loss: 697.8976\n",
            "Epoch 424/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 146.7010 - val_loss: 697.2963\n",
            "Epoch 425/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 145.9653 - val_loss: 696.2518\n",
            "Epoch 426/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 146.6289 - val_loss: 694.2075\n",
            "Epoch 427/500\n",
            "6/6 [==============================] - 0s 9ms/step - loss: 145.8679 - val_loss: 693.5024\n",
            "Epoch 428/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 145.7610 - val_loss: 691.7787\n",
            "Epoch 429/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 145.2345 - val_loss: 690.4842\n",
            "Epoch 430/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 144.7637 - val_loss: 689.5001\n",
            "Epoch 431/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 144.7231 - val_loss: 688.2225\n",
            "Epoch 432/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 144.7481 - val_loss: 687.7120\n",
            "Epoch 433/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 144.0340 - val_loss: 686.7564\n",
            "Epoch 434/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 144.4453 - val_loss: 685.2035\n",
            "Epoch 435/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 143.9783 - val_loss: 684.8057\n",
            "Epoch 436/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 143.4574 - val_loss: 683.6277\n",
            "Epoch 437/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 143.3847 - val_loss: 682.2599\n",
            "Epoch 438/500\n",
            "6/6 [==============================] - 0s 8ms/step - loss: 143.2768 - val_loss: 681.7588\n",
            "Epoch 439/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 143.5131 - val_loss: 679.9877\n",
            "Epoch 440/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 142.4865 - val_loss: 679.0284\n",
            "Epoch 441/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 142.2430 - val_loss: 678.0048\n",
            "Epoch 442/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 142.0724 - val_loss: 676.9141\n",
            "Epoch 443/500\n",
            "6/6 [==============================] - 0s 7ms/step - loss: 142.0731 - val_loss: 676.1786\n",
            "Epoch 444/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 141.7613 - val_loss: 674.7026\n",
            "Epoch 445/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 141.3448 - val_loss: 673.8162\n",
            "Epoch 446/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 141.2588 - val_loss: 672.6799\n",
            "Epoch 447/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 141.3760 - val_loss: 671.0311\n",
            "Epoch 448/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 141.0094 - val_loss: 670.6470\n",
            "Epoch 449/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 140.4942 - val_loss: 669.2954\n",
            "Epoch 450/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 140.4143 - val_loss: 668.5981\n",
            "Epoch 451/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 140.2186 - val_loss: 667.0110\n",
            "Epoch 452/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 140.0093 - val_loss: 666.4149\n",
            "Epoch 453/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 139.8656 - val_loss: 664.6912\n",
            "Epoch 454/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 139.4638 - val_loss: 663.9146\n",
            "Epoch 455/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 139.3784 - val_loss: 663.3051\n",
            "Epoch 456/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 139.2898 - val_loss: 661.3724\n",
            "Epoch 457/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 138.7831 - val_loss: 660.7349\n",
            "Epoch 458/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 138.3299 - val_loss: 659.4067\n",
            "Epoch 459/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 138.3201 - val_loss: 657.9293\n",
            "Epoch 460/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 137.9266 - val_loss: 657.1519\n",
            "Epoch 461/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 137.8161 - val_loss: 656.1839\n",
            "Epoch 462/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 137.5430 - val_loss: 654.9234\n",
            "Epoch 463/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 137.3103 - val_loss: 654.0099\n",
            "Epoch 464/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 137.0699 - val_loss: 652.9749\n",
            "Epoch 465/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 136.7103 - val_loss: 651.6294\n",
            "Epoch 466/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 136.6854 - val_loss: 650.8290\n",
            "Epoch 467/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 136.6066 - val_loss: 649.9081\n",
            "Epoch 468/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 136.5889 - val_loss: 647.8889\n",
            "Epoch 469/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 136.0512 - val_loss: 646.4567\n",
            "Epoch 470/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 135.5872 - val_loss: 645.4846\n",
            "Epoch 471/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 135.3988 - val_loss: 644.8466\n",
            "Epoch 472/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 135.2268 - val_loss: 643.6471\n",
            "Epoch 473/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 134.9545 - val_loss: 642.6715\n",
            "Epoch 474/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 134.9524 - val_loss: 641.9671\n",
            "Epoch 475/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 134.5723 - val_loss: 640.3746\n",
            "Epoch 476/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 134.4781 - val_loss: 638.9561\n",
            "Epoch 477/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 134.6486 - val_loss: 638.5245\n",
            "Epoch 478/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 134.0980 - val_loss: 636.8592\n",
            "Epoch 479/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 133.5040 - val_loss: 636.0310\n",
            "Epoch 480/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 133.5538 - val_loss: 635.1332\n",
            "Epoch 481/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 133.2091 - val_loss: 634.0230\n",
            "Epoch 482/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 133.1058 - val_loss: 632.5216\n",
            "Epoch 483/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 132.5577 - val_loss: 631.6538\n",
            "Epoch 484/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 132.3622 - val_loss: 630.6107\n",
            "Epoch 485/500\n",
            "6/6 [==============================] - 0s 15ms/step - loss: 132.6425 - val_loss: 628.9584\n",
            "Epoch 486/500\n",
            "6/6 [==============================] - 0s 10ms/step - loss: 132.1292 - val_loss: 628.2109\n",
            "Epoch 487/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 131.7668 - val_loss: 626.8675\n",
            "Epoch 488/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 131.8910 - val_loss: 626.5531\n",
            "Epoch 489/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 131.2884 - val_loss: 625.2206\n",
            "Epoch 490/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 131.0432 - val_loss: 624.3533\n",
            "Epoch 491/500\n",
            "6/6 [==============================] - 0s 16ms/step - loss: 130.8337 - val_loss: 623.0646\n",
            "Epoch 492/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 130.6279 - val_loss: 622.0986\n",
            "Epoch 493/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 130.3905 - val_loss: 620.6875\n",
            "Epoch 494/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 130.1722 - val_loss: 619.2418\n",
            "Epoch 495/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 129.9834 - val_loss: 618.2361\n",
            "Epoch 496/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 129.7913 - val_loss: 616.8456\n",
            "Epoch 497/500\n",
            "6/6 [==============================] - 0s 11ms/step - loss: 129.4055 - val_loss: 616.2714\n",
            "Epoch 498/500\n",
            "6/6 [==============================] - 0s 14ms/step - loss: 129.3186 - val_loss: 615.4094\n",
            "Epoch 499/500\n",
            "6/6 [==============================] - 0s 13ms/step - loss: 129.0007 - val_loss: 614.0989\n",
            "Epoch 500/500\n",
            "6/6 [==============================] - 0s 12ms/step - loss: 129.2354 - val_loss: 612.4167\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 37.1436\n",
            "Test Loss: 37.143611907958984\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1:\n",
        "Good. Now, we will use a certain value of degrees in celsius (this value shouldn't exist in our dataset), and ask our model to predict the corresponding fahrenheit value. We will print the fahrenheit output"
      ],
      "metadata": {
        "id": "bkDlgbcyaWWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a new Celsius value for prediction\n",
        "new_celsius_value = 100.0\n",
        "\n",
        "# Reshape the input for prediction\n",
        "new_celsius_value = np.array([new_celsius_value]).reshape(-1, 1)\n",
        "\n",
        "# Use the trained model to predict the corresponding Fahrenheit value\n",
        "predicted_fahrenheit = model.predict(new_celsius_value)\n",
        "\n",
        "# Print the predicted Fahrenheit value\n",
        "print(f\"The predicted Fahrenheit value for {new_celsius_value[0, 0]} degrees Celsius is {predicted_fahrenheit[0, 0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOc2ojRyZqWT",
        "outputId": "662fa36d-7a86-405a-d7b7-80c237f556c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step\n",
            "The predicted Fahrenheit value for 100.0 degrees Celsius is 229.93966674804688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1:\n",
        "Good, now let's see the weights of our trained model"
      ],
      "metadata": {
        "id": "yG5MwovZa4ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the weights of the trained model\n",
        "weights = model.get_weights()\n",
        "\n",
        "# Print the weights\n",
        "for layer_num, layer_weights in enumerate(weights):\n",
        "    print(f\"Layer {layer_num + 1} Weights:\")\n",
        "    print(layer_weights)\n",
        "    print(\"------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSamYEUBaDaq",
        "outputId": "15265ccc-9f7e-463f-abe9-46140e8237df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Weights:\n",
            "[[2.205747]]\n",
            "------------------------\n",
            "Layer 2 Weights:\n",
            "[9.364974]\n",
            "------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## P1:\n",
        "After testing our network, it's not working properly. It produces incorrect results.\n",
        "\n",
        "I believe the problem is having a very small dataset on which the model is trained.\n",
        "Let's go ahead and enlarge our dataset - let's make it at least 100 inputs long"
      ],
      "metadata": {
        "id": "l86GP6x5bnfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to convert Celsius to Fahrenheit\n",
        "def celsius_to_fahrenheit(celsius):\n",
        "    return (9/5) * celsius + 32\n",
        "\n",
        "# Generating a larger dataset with at least 100 inputs\n",
        "np.random.seed(42)  # Set seed for reproducibility\n",
        "celsius_values_large = np.random.uniform(low=-100, high=100, size=(100,))\n",
        "fahrenheit_values_large = celsius_to_fahrenheit(celsius_values_large)\n",
        "\n",
        "# Storing the dataset as vectors\n",
        "dataset_large = np.column_stack((celsius_values_large, fahrenheit_values_large))\n",
        "\n",
        "# Save the larger dataset to a CSV file\n",
        "np.savetxt('celsius_to_fahrenheit_dataset_large.csv', dataset_large, delimiter=',')\n",
        "\n",
        "# Displaying the larger dataset\n",
        "print(\"Celsius\\tFahrenheit\")\n",
        "print(\"-------------------\")\n",
        "for c, f in dataset_large:\n",
        "    print(f\"{c}\\t{f}\")\n",
        "\n",
        "print(\"Larger dataset saved to 'celsius_to_fahrenheit_dataset_large.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw6QUY5ybYM9",
        "outputId": "d599513d-e419-4afc-bbb8-480ca3dc389d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Celsius\tFahrenheit\n",
            "-------------------\n",
            "-25.091976230527507\t-13.165557214949516\n",
            "90.14286128198324\t194.25715030756984\n",
            "46.39878836228101\t115.51781905210582\n",
            "19.73169683940732\t67.51705431093318\n",
            "-68.79627191151269\t-91.83328944072285\n",
            "-68.80109593275947\t-91.84197267896704\n",
            "-88.3832775663601\t-127.0898996194482\n",
            "73.23522915498702\t163.82341247897665\n",
            "20.223002348641756\t68.40140422755516\n",
            "41.61451555920911\t106.9061280065764\n",
            "-95.88310114083951\t-140.58958205351112\n",
            "93.98197043239887\t201.16754677831796\n",
            "66.48852816008434\t151.67935068815183\n",
            "-57.53217786434477\t-71.55792015582058\n",
            "-63.635006558579875\t-82.54301180544378\n",
            "-63.31909802931324\t-81.97437645276383\n",
            "-39.15155140809246\t-38.47279253456642\n",
            "4.951286326447573\t40.91231538760563\n",
            "-13.610996271576852\t7.500206711161667\n",
            "-41.754171960391616\t-43.15750952870491\n",
            "22.370578944475895\t72.26704210005661\n",
            "-72.10122786959164\t-97.78221016526496\n",
            "-41.57107029295637\t-42.827926527321466\n",
            "-26.727631341261656\t-16.109736414270984\n",
            "-8.786003156592813\t16.185194318132936\n",
            "57.03519227860272\t134.6633461014849\n",
            "-60.06524356832805\t-76.11743842299049\n",
            "2.8468876827223255\t37.124397828900186\n",
            "18.482913772408494\t65.26924479033529\n",
            "-90.70991745600045\t-131.27785142080083\n",
            "21.50897038028768\t70.71614668451782\n",
            "-65.8951752625417\t-86.61131547257506\n",
            "-86.9896814029441\t-124.58142652529938\n",
            "89.77710745066665\t193.59879341119998\n",
            "93.12640661491187\t199.62753190684137\n",
            "61.67946962329222\t143.02304532192602\n",
            "-39.07724616532586\t-38.339043097586554\n",
            "-80.46557719872322\t-112.8380389577018\n",
            "36.846605302431385\t98.3238895443765\n",
            "-11.969501252079738\t10.45489774625647\n",
            "-75.59235303104424\t-104.06623545587965\n",
            "-0.9646179777459594\t30.263687640057274\n",
            "-93.12229577695632\t-135.62013239852138\n",
            "81.8640804157564\t179.35534474836155\n",
            "-48.24400367999662\t-54.83920662399392\n",
            "32.50445687079639\t90.50802236743351\n",
            "-37.65778478211781\t-35.78401260781206\n",
            "4.0136042355621555\t39.22448762401188\n",
            "9.342055868655933\t48.81570056358068\n",
            "-63.02910889489459\t-81.45239601081026\n",
            "93.91692555291172\t201.0504659952411\n",
            "55.02656467222292\t131.04781641000125\n",
            "87.89978831283781\t190.21961896310808\n",
            "78.96547008552977\t174.1378461539536\n",
            "19.579995762217024\t67.24399237199064\n",
            "84.37484700462338\t183.87472460832208\n",
            "-82.3014995896161\t-116.14269926130899\n",
            "-60.80342751617096\t-77.44616952910773\n",
            "-90.95454221789238\t-131.71817599220628\n",
            "-34.933933847347134\t-30.881080925224843\n",
            "-22.2645420621036\t-8.076175711786483\n",
            "-45.730193645220815\t-50.314348561397466\n",
            "65.74750183038586\t150.34550329469454\n",
            "-28.64933466128214\t-19.56880239030785\n",
            "-43.81309806252385\t-46.863576512542934\n",
            "8.539216631649694\t47.37058993696945\n",
            "-71.81515500504747\t-97.26727900908546\n",
            "60.43939615080794\t140.79091307145427\n",
            "-85.08987126404584\t-121.16176827528253\n",
            "97.37738732010345\t207.27929717618622\n",
            "54.44895385933148\t130.0081169467967\n",
            "-60.25686369316552\t-76.46235464769794\n",
            "-98.89557657527952\t-146.01203783550315\n",
            "63.09228569096683\t145.5661142437403\n",
            "41.37146876952343\t106.46864378514218\n",
            "45.80143360819747\t114.44258049475545\n",
            "54.25406933718915\t129.6573248069405\n",
            "-85.19106965318193\t-121.34392537572748\n",
            "-28.306854291145484\t-18.95233772406187\n",
            "-76.82618809497406\t-106.28713857095332\n",
            "72.6206851751187\t162.71723331521366\n",
            "24.659625365511587\t76.38732565792085\n",
            "-33.82039502947016\t-28.876711053046293\n",
            "-87.28832994279527\t-125.11899389703149\n",
            "-37.80353565686756\t-36.046364182361614\n",
            "-34.96333559465059\t-30.93400407037106\n",
            "45.92123566761282\t114.65822420170308\n",
            "27.511494271042622\t81.52068968787671\n",
            "77.44254851526532\t171.39658732747756\n",
            "-5.557014967610144\t21.99737305830174\n",
            "-76.08115081233966\t-104.94607146221139\n",
            "42.648957444599006\t108.76812340027821\n",
            "52.157009723379474\t125.88261750208305\n",
            "12.255439513899248\t54.05979112501865\n",
            "54.19343599091221\t129.54818478364197\n",
            "-1.2408807271218478\t29.766414691180675\n",
            "4.546565876398816\t40.18381857751787\n",
            "-14.491796328290079\t5.914766609077859\n",
            "-94.91617465118097\t-138.84911437212574\n",
            "-78.42171460133912\t-109.15908628241041\n",
            "Larger dataset saved to 'celsius_to_fahrenheit_dataset_large.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No prompt\n",
        "Just re-writing previous code to train a new model on the large dataset."
      ],
      "metadata": {
        "id": "OPhtDgvJe5AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "dataset = np.loadtxt('celsius_to_fahrenheit_dataset_large.csv', delimiter=',')\n",
        "\n",
        "# Split the dataset into features (X) and labels (y)\n",
        "X = dataset[:, 0]  # Celsius values\n",
        "y = dataset[:, 1]  # Fahrenheit values\n",
        "\n",
        "# Split the dataset into training, testing, and validation sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Build the neural network model\n",
        "model_new = keras.Sequential([\n",
        "    layers.Dense(1, input_shape=[1])  # Single neuron, single layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_new.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model_new.fit(X_train, y_train, epochs=500, batch_size=1, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model_new.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# Display the training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hI9lI8h4bq-s",
        "outputId": "6c76aada-123d-4c2a-9596-a233d241a1b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "60/60 [==============================] - 1s 6ms/step - loss: 27007.6992 - val_loss: 25047.2324\n",
            "Epoch 2/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 20095.7012 - val_loss: 18120.9570\n",
            "Epoch 3/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 14628.5146 - val_loss: 12754.4785\n",
            "Epoch 4/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 10424.8027 - val_loss: 8718.8066\n",
            "Epoch 5/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7258.1357 - val_loss: 6205.0640\n",
            "Epoch 6/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 5074.0767 - val_loss: 4081.9507\n",
            "Epoch 7/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3529.6130 - val_loss: 2736.8159\n",
            "Epoch 8/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2506.7900 - val_loss: 1949.3617\n",
            "Epoch 9/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1874.0251 - val_loss: 1418.9031\n",
            "Epoch 10/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1458.0648 - val_loss: 1136.4570\n",
            "Epoch 11/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1213.8463 - val_loss: 970.5692\n",
            "Epoch 12/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1068.9808 - val_loss: 892.3011\n",
            "Epoch 13/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 987.4315 - val_loss: 850.6517\n",
            "Epoch 14/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 938.6171 - val_loss: 831.6810\n",
            "Epoch 15/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 905.5731 - val_loss: 819.1231\n",
            "Epoch 16/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 879.1605 - val_loss: 808.3560\n",
            "Epoch 17/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 859.8337 - val_loss: 796.1978\n",
            "Epoch 18/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 844.7230 - val_loss: 789.2268\n",
            "Epoch 19/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 826.7026 - val_loss: 772.3599\n",
            "Epoch 20/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 809.8209 - val_loss: 762.0920\n",
            "Epoch 21/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 795.7590 - val_loss: 744.9338\n",
            "Epoch 22/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 779.4996 - val_loss: 733.9701\n",
            "Epoch 23/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 759.8535 - val_loss: 713.8641\n",
            "Epoch 24/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 744.4867 - val_loss: 698.1198\n",
            "Epoch 25/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 729.4014 - val_loss: 690.4356\n",
            "Epoch 26/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 715.2969 - val_loss: 665.2460\n",
            "Epoch 27/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 695.2230 - val_loss: 649.5762\n",
            "Epoch 28/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 676.2762 - val_loss: 637.1976\n",
            "Epoch 29/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 663.8331 - val_loss: 621.2363\n",
            "Epoch 30/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 645.5159 - val_loss: 603.0583\n",
            "Epoch 31/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 628.2481 - val_loss: 588.3563\n",
            "Epoch 32/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 616.0245 - val_loss: 568.5328\n",
            "Epoch 33/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 598.2548 - val_loss: 558.2323\n",
            "Epoch 34/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 575.1249 - val_loss: 537.0336\n",
            "Epoch 35/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 563.0792 - val_loss: 525.7123\n",
            "Epoch 36/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 545.1198 - val_loss: 505.3166\n",
            "Epoch 37/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 528.1609 - val_loss: 489.3846\n",
            "Epoch 38/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 513.3270 - val_loss: 478.6112\n",
            "Epoch 39/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 498.6700 - val_loss: 464.7626\n",
            "Epoch 40/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 483.1461 - val_loss: 448.3979\n",
            "Epoch 41/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 465.2982 - val_loss: 435.6155\n",
            "Epoch 42/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 450.0453 - val_loss: 421.0966\n",
            "Epoch 43/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 432.0840 - val_loss: 407.9643\n",
            "Epoch 44/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 415.9301 - val_loss: 388.3936\n",
            "Epoch 45/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 401.8847 - val_loss: 373.7701\n",
            "Epoch 46/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 388.2458 - val_loss: 361.0869\n",
            "Epoch 47/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 371.8714 - val_loss: 347.3915\n",
            "Epoch 48/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 359.9574 - val_loss: 331.6055\n",
            "Epoch 49/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 345.0947 - val_loss: 323.1117\n",
            "Epoch 50/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 331.5019 - val_loss: 307.8834\n",
            "Epoch 51/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 318.4773 - val_loss: 295.6153\n",
            "Epoch 52/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 305.5058 - val_loss: 287.7530\n",
            "Epoch 53/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 293.8589 - val_loss: 271.5005\n",
            "Epoch 54/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 280.2977 - val_loss: 260.6052\n",
            "Epoch 55/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 267.7560 - val_loss: 249.6418\n",
            "Epoch 56/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 257.0910 - val_loss: 237.8543\n",
            "Epoch 57/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 245.1748 - val_loss: 226.6474\n",
            "Epoch 58/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 237.6170 - val_loss: 213.7773\n",
            "Epoch 59/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 224.7697 - val_loss: 207.5737\n",
            "Epoch 60/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 211.3672 - val_loss: 194.0697\n",
            "Epoch 61/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 204.9086 - val_loss: 189.6832\n",
            "Epoch 62/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 193.0336 - val_loss: 175.9383\n",
            "Epoch 63/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 182.9250 - val_loss: 169.6126\n",
            "Epoch 64/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 176.2138 - val_loss: 160.8120\n",
            "Epoch 65/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 166.4398 - val_loss: 148.9636\n",
            "Epoch 66/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 156.8994 - val_loss: 140.1996\n",
            "Epoch 67/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 147.4286 - val_loss: 135.5201\n",
            "Epoch 68/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 139.2558 - val_loss: 128.9318\n",
            "Epoch 69/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 131.9782 - val_loss: 120.8158\n",
            "Epoch 70/500\n",
            "60/60 [==============================] - 1s 18ms/step - loss: 124.3130 - val_loss: 111.9911\n",
            "Epoch 71/500\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 116.2295 - val_loss: 107.8132\n",
            "Epoch 72/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 110.1712 - val_loss: 103.8553\n",
            "Epoch 73/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 102.6882 - val_loss: 94.9452\n",
            "Epoch 74/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 96.4204 - val_loss: 86.4704\n",
            "Epoch 75/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 91.7207 - val_loss: 82.8072\n",
            "Epoch 76/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 85.2282 - val_loss: 76.9162\n",
            "Epoch 77/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 77.9935 - val_loss: 70.7595\n",
            "Epoch 78/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 72.5571 - val_loss: 66.0903\n",
            "Epoch 79/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 70.0176 - val_loss: 60.3074\n",
            "Epoch 80/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 62.8364 - val_loss: 56.6165\n",
            "Epoch 81/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 58.3356 - val_loss: 52.7042\n",
            "Epoch 82/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 53.7433 - val_loss: 48.1370\n",
            "Epoch 83/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 50.9086 - val_loss: 44.2529\n",
            "Epoch 84/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 46.4405 - val_loss: 42.0387\n",
            "Epoch 85/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 44.5929 - val_loss: 37.1334\n",
            "Epoch 86/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 39.6795 - val_loss: 35.6427\n",
            "Epoch 87/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 36.5671 - val_loss: 33.5219\n",
            "Epoch 88/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 32.4222 - val_loss: 29.1469\n",
            "Epoch 89/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 29.5723 - val_loss: 26.1422\n",
            "Epoch 90/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 26.9838 - val_loss: 23.8707\n",
            "Epoch 91/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 24.7138 - val_loss: 21.3429\n",
            "Epoch 92/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 22.2310 - val_loss: 19.9131\n",
            "Epoch 93/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 20.5523 - val_loss: 17.6602\n",
            "Epoch 94/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 19.3784 - val_loss: 15.6478\n",
            "Epoch 95/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 15.9502 - val_loss: 15.5546\n",
            "Epoch 96/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 14.6953 - val_loss: 13.0335\n",
            "Epoch 97/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 13.0048 - val_loss: 12.2759\n",
            "Epoch 98/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 11.8107 - val_loss: 10.4645\n",
            "Epoch 99/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 10.7832 - val_loss: 9.4025\n",
            "Epoch 100/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 8.9710 - val_loss: 8.0379\n",
            "Epoch 101/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 7.9747 - val_loss: 6.8922\n",
            "Epoch 102/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.9627 - val_loss: 5.9740\n",
            "Epoch 103/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.2052 - val_loss: 5.3474\n",
            "Epoch 104/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 5.3387 - val_loss: 4.8645\n",
            "Epoch 105/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.7215 - val_loss: 3.9739\n",
            "Epoch 106/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.0560 - val_loss: 3.6402\n",
            "Epoch 107/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.4931 - val_loss: 3.3106\n",
            "Epoch 108/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.9771 - val_loss: 2.5004\n",
            "Epoch 109/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.7604 - val_loss: 2.1409\n",
            "Epoch 110/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.2177 - val_loss: 1.9319\n",
            "Epoch 111/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.9894 - val_loss: 1.6130\n",
            "Epoch 112/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.7973 - val_loss: 1.2834\n",
            "Epoch 113/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.2763 - val_loss: 1.0566\n",
            "Epoch 114/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1392 - val_loss: 0.9497\n",
            "Epoch 115/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.9653 - val_loss: 0.8350\n",
            "Epoch 116/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.7453 - val_loss: 0.6189\n",
            "Epoch 117/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.6251 - val_loss: 0.5121\n",
            "Epoch 118/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.5136 - val_loss: 0.3932\n",
            "Epoch 119/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.4190 - val_loss: 0.3185\n",
            "Epoch 120/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.3195 - val_loss: 0.2628\n",
            "Epoch 121/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2597 - val_loss: 0.1992\n",
            "Epoch 122/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1943 - val_loss: 0.1585\n",
            "Epoch 123/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.1538 - val_loss: 0.1438\n",
            "Epoch 124/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1214 - val_loss: 0.0950\n",
            "Epoch 125/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0928 - val_loss: 0.0834\n",
            "Epoch 126/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0716 - val_loss: 0.0565\n",
            "Epoch 127/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.0460\n",
            "Epoch 128/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0435 - val_loss: 0.0320\n",
            "Epoch 129/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0306 - val_loss: 0.0275\n",
            "Epoch 130/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0181\n",
            "Epoch 131/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.0142\n",
            "Epoch 132/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0124 - val_loss: 0.0093\n",
            "Epoch 133/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.0066\n",
            "Epoch 134/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.0048\n",
            "Epoch 135/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0047 - val_loss: 0.0033\n",
            "Epoch 136/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0034 - val_loss: 0.0023\n",
            "Epoch 137/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 138/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0011\n",
            "Epoch 139/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 9.7298e-04 - val_loss: 6.8153e-04\n",
            "Epoch 140/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.4648e-04 - val_loss: 4.7003e-04\n",
            "Epoch 141/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.4335e-04 - val_loss: 3.4818e-04\n",
            "Epoch 142/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.8139e-04 - val_loss: 2.1699e-04\n",
            "Epoch 143/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.7406e-04 - val_loss: 1.2003e-04\n",
            "Epoch 144/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0596e-04 - val_loss: 7.7175e-05\n",
            "Epoch 145/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 6.7131e-05 - val_loss: 5.0482e-05\n",
            "Epoch 146/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.4020e-05 - val_loss: 3.3660e-05\n",
            "Epoch 147/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.6076e-05 - val_loss: 1.7165e-05\n",
            "Epoch 148/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5608e-05 - val_loss: 9.9188e-06\n",
            "Epoch 149/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 9.3735e-06 - val_loss: 5.7254e-06\n",
            "Epoch 150/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 5.4421e-06 - val_loss: 6.4043e-06\n",
            "Epoch 151/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.3579e-06 - val_loss: 1.8945e-06\n",
            "Epoch 152/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5993e-06 - val_loss: 1.1162e-06\n",
            "Epoch 153/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0620e-06 - val_loss: 6.1816e-07\n",
            "Epoch 154/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 4.8342e-07 - val_loss: 3.2765e-07\n",
            "Epoch 155/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 2.7918e-07 - val_loss: 1.9570e-07\n",
            "Epoch 156/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5963e-07 - val_loss: 8.8072e-08\n",
            "Epoch 157/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.5633e-08 - val_loss: 3.7179e-08\n",
            "Epoch 158/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.9688e-08 - val_loss: 1.7122e-08\n",
            "Epoch 159/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 2.3496e-08 - val_loss: 1.7349e-08\n",
            "Epoch 160/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.2570e-08 - val_loss: 1.8643e-08\n",
            "Epoch 161/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.7623e-08 - val_loss: 2.8896e-08\n",
            "Epoch 162/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.3345e-08 - val_loss: 1.5495e-08\n",
            "Epoch 163/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.9432e-08 - val_loss: 1.3820e-08\n",
            "Epoch 164/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.1001e-08 - val_loss: 1.8997e-08\n",
            "Epoch 165/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.6692e-08 - val_loss: 1.8997e-08\n",
            "Epoch 166/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5975e-08 - val_loss: 2.4979e-08\n",
            "Epoch 167/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.5034e-08 - val_loss: 1.2582e-08\n",
            "Epoch 168/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5973e-08 - val_loss: 1.6300e-08\n",
            "Epoch 169/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.2389e-08 - val_loss: 1.0649e-08\n",
            "Epoch 170/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.4599e-08 - val_loss: 9.1245e-09\n",
            "Epoch 171/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 9.7896e-09 - val_loss: 7.4739e-09\n",
            "Epoch 172/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.7018e-09 - val_loss: 6.7354e-09\n",
            "Epoch 173/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 8.7901e-09 - val_loss: 7.7951e-09\n",
            "Epoch 174/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.6982e-09 - val_loss: 6.7354e-09\n",
            "Epoch 175/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.6748e-09 - val_loss: 6.7354e-09\n",
            "Epoch 176/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.7543e-09 - val_loss: 7.2589e-09\n",
            "Epoch 177/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.1781e-09 - val_loss: 5.0830e-09\n",
            "Epoch 178/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 6.7185e-09 - val_loss: 5.0368e-09\n",
            "Epoch 179/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.2510e-09 - val_loss: 4.3471e-09\n",
            "Epoch 180/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 6.6513e-09 - val_loss: 5.1154e-09\n",
            "Epoch 181/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.3752e-09 - val_loss: 4.5242e-09\n",
            "Epoch 182/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.3007e-09 - val_loss: 4.7036e-09\n",
            "Epoch 183/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.1474e-09 - val_loss: 7.4317e-09\n",
            "Epoch 184/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.6497e-09 - val_loss: 4.4649e-09\n",
            "Epoch 185/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.6200e-09 - val_loss: 2.7886e-09\n",
            "Epoch 186/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.7075e-09 - val_loss: 2.7660e-09\n",
            "Epoch 187/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.7201e-09 - val_loss: 1.3585e-09\n",
            "Epoch 188/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.6039e-09 - val_loss: 2.6379e-09\n",
            "Epoch 189/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.0008e-09 - val_loss: 1.2832e-09\n",
            "Epoch 190/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.0797e-09 - val_loss: 1.7088e-09\n",
            "Epoch 191/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.9466e-09 - val_loss: 1.1042e-09\n",
            "Epoch 192/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.3566e-09 - val_loss: 1.2832e-09\n",
            "Epoch 193/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.7468e-09 - val_loss: 1.2832e-09\n",
            "Epoch 194/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.3530e-09 - val_loss: 1.2551e-09\n",
            "Epoch 195/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.8828e-09 - val_loss: 1.2832e-09\n",
            "Epoch 196/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.3273e-09 - val_loss: 1.8765e-09\n",
            "Epoch 197/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.9209e-09 - val_loss: 3.7341e-09\n",
            "Epoch 198/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.1401e-09 - val_loss: 1.9718e-09\n",
            "Epoch 199/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.0036e-09 - val_loss: 1.1042e-09\n",
            "Epoch 200/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.0845e-09 - val_loss: 1.2650e-09\n",
            "Epoch 201/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.7653e-09 - val_loss: 1.7088e-09\n",
            "Epoch 202/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.3161e-09 - val_loss: 1.0318e-09\n",
            "Epoch 203/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.9327e-09 - val_loss: 7.5743e-09\n",
            "Epoch 204/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.0708e-09 - val_loss: 4.2241e-10\n",
            "Epoch 205/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.4519e-09 - val_loss: 1.0893e-09\n",
            "Epoch 206/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0496e-09 - val_loss: 9.7211e-10\n",
            "Epoch 207/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5585e-09 - val_loss: 6.3123e-10\n",
            "Epoch 208/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.6567e-10 - val_loss: 1.4775e-10\n",
            "Epoch 209/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.5464e-10 - val_loss: 6.4615e-10\n",
            "Epoch 210/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.2551e-10 - val_loss: 3.5075e-10\n",
            "Epoch 211/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.7610e-10 - val_loss: 3.5075e-10\n",
            "Epoch 212/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.2275e-10 - val_loss: 3.6493e-10\n",
            "Epoch 213/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 7.0329e-10 - val_loss: 6.3123e-10\n",
            "Epoch 214/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.6075e-10 - val_loss: 2.1079e-09\n",
            "Epoch 215/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 9.7468e-10 - val_loss: 1.4775e-10\n",
            "Epoch 216/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.2916e-10 - val_loss: 2.1105e-10\n",
            "Epoch 217/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.0282e-10 - val_loss: 1.0030e-09\n",
            "Epoch 218/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.7656e-10 - val_loss: 1.0030e-09\n",
            "Epoch 219/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.3220e-09 - val_loss: 2.3251e-10\n",
            "Epoch 220/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 5.1587e-10 - val_loss: 2.3251e-10\n",
            "Epoch 221/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.6327e-10 - val_loss: 1.2374e-10\n",
            "Epoch 222/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.3838e-10 - val_loss: 8.2841e-10\n",
            "Epoch 223/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.4787e-09 - val_loss: 1.4083e-09\n",
            "Epoch 224/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0451e-09 - val_loss: 2.9032e-09\n",
            "Epoch 225/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.7948e-10 - val_loss: 6.4288e-10\n",
            "Epoch 226/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.9527e-10 - val_loss: 2.2891e-09\n",
            "Epoch 227/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.3755e-10 - val_loss: 4.9158e-11\n",
            "Epoch 228/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.8596e-10 - val_loss: 3.0454e-10\n",
            "Epoch 229/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.2748e-10 - val_loss: 3.9336e-11\n",
            "Epoch 230/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.3452e-11 - val_loss: 3.9336e-11\n",
            "Epoch 231/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.0543e-10 - val_loss: 3.5548e-10\n",
            "Epoch 232/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 2.8028e-10 - val_loss: 1.1428e-10\n",
            "Epoch 233/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.1775e-10 - val_loss: 1.6084e-09\n",
            "Epoch 234/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.2957e-10 - val_loss: 4.9158e-11\n",
            "Epoch 235/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.3616e-10 - val_loss: 6.4251e-10\n",
            "Epoch 236/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.9650e-11 - val_loss: 3.9336e-11\n",
            "Epoch 237/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.6235e-10 - val_loss: 1.8122e-10\n",
            "Epoch 238/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.1251e-10 - val_loss: 1.4832e-09\n",
            "Epoch 239/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1619e-09 - val_loss: 3.8608e-11\n",
            "Epoch 240/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.1627e-10 - val_loss: 3.6061e-11\n",
            "Epoch 241/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.9450e-09 - val_loss: 1.5698e-09\n",
            "Epoch 242/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.0068e-08 - val_loss: 1.3734e-08\n",
            "Epoch 243/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.8650e-08 - val_loss: 2.2832e-09\n",
            "Epoch 244/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.4724e-09 - val_loss: 4.5373e-09\n",
            "Epoch 245/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.9969e-09 - val_loss: 7.0545e-10\n",
            "Epoch 246/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.0034e-08 - val_loss: 3.2128e-10\n",
            "Epoch 247/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 2.9943e-06 - val_loss: 2.2760e-05\n",
            "Epoch 248/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 4.9445e-05 - val_loss: 1.6459e-05\n",
            "Epoch 249/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.9873e-04 - val_loss: 0.0056\n",
            "Epoch 250/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 5.4423e-04 - val_loss: 2.8733e-04\n",
            "Epoch 251/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.5936e-04 - val_loss: 0.0022\n",
            "Epoch 252/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0010\n",
            "Epoch 253/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.4947 - val_loss: 0.2561\n",
            "Epoch 254/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1906 - val_loss: 0.0192\n",
            "Epoch 255/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 1.1802e-04\n",
            "Epoch 256/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.6793e-05 - val_loss: 4.3086e-07\n",
            "Epoch 257/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.0937e-06 - val_loss: 2.9680e-07\n",
            "Epoch 258/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.0836e-07 - val_loss: 3.2324e-07\n",
            "Epoch 259/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.6226e-06 - val_loss: 4.5946e-08\n",
            "Epoch 260/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.0128e-07 - val_loss: 6.4823e-08\n",
            "Epoch 261/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.0112e-08 - val_loss: 2.8454e-10\n",
            "Epoch 262/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.5028e-09 - val_loss: 1.1042e-09\n",
            "Epoch 263/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.1114e-09 - val_loss: 2.8482e-09\n",
            "Epoch 264/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.1394e-09 - val_loss: 7.0545e-10\n",
            "Epoch 265/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.9298e-10 - val_loss: 7.0545e-10\n",
            "Epoch 266/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.9413e-09 - val_loss: 2.6016e-09\n",
            "Epoch 267/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.6223e-09 - val_loss: 6.9490e-10\n",
            "Epoch 268/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.1265e-08 - val_loss: 9.1323e-08\n",
            "Epoch 269/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.3753e-08 - val_loss: 4.3620e-09\n",
            "Epoch 270/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.4181e-09 - val_loss: 6.9490e-10\n",
            "Epoch 271/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.2558e-08 - val_loss: 4.2121e-07\n",
            "Epoch 272/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5409e-07 - val_loss: 1.0045e-08\n",
            "Epoch 273/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5184e-08 - val_loss: 1.9331e-07\n",
            "Epoch 274/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.2530e-06 - val_loss: 1.4309e-05\n",
            "Epoch 275/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 8.2287e-04 - val_loss: 3.6210e-04\n",
            "Epoch 276/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.6083e-04 - val_loss: 0.0043\n",
            "Epoch 277/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 5.4960e-04\n",
            "Epoch 278/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.0084\n",
            "Epoch 279/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0173\n",
            "Epoch 280/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0326 - val_loss: 0.0340\n",
            "Epoch 281/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.1025 - val_loss: 0.0717\n",
            "Epoch 282/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0044\n",
            "Epoch 283/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 8.8318e-04\n",
            "Epoch 284/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0018 - val_loss: 2.3590e-04\n",
            "Epoch 285/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0063\n",
            "Epoch 286/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0115 - val_loss: 0.0213\n",
            "Epoch 287/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 9.1836e-04\n",
            "Epoch 288/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.3910e-04 - val_loss: 0.0060\n",
            "Epoch 289/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 2.0171e-05\n",
            "Epoch 290/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0564\n",
            "Epoch 291/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0301 - val_loss: 0.1012\n",
            "Epoch 292/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.4639 - val_loss: 0.0585\n",
            "Epoch 293/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0066 - val_loss: 4.2604e-05\n",
            "Epoch 294/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0990e-04 - val_loss: 1.8745e-05\n",
            "Epoch 295/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.2639e-05 - val_loss: 4.9771e-07\n",
            "Epoch 296/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.5329e-06 - val_loss: 1.7116e-07\n",
            "Epoch 297/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.0096e-07 - val_loss: 1.6176e-08\n",
            "Epoch 298/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.7261e-08 - val_loss: 1.1082e-09\n",
            "Epoch 299/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.2302e-09 - val_loss: 6.6252e-10\n",
            "Epoch 300/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.0200e-09 - val_loss: 2.0490e-09\n",
            "Epoch 301/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.4802e-09 - val_loss: 3.9258e-10\n",
            "Epoch 302/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.2648e-09 - val_loss: 3.9163e-09\n",
            "Epoch 303/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.8407e-09 - val_loss: 7.6766e-10\n",
            "Epoch 304/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.4211e-09 - val_loss: 7.6220e-10\n",
            "Epoch 305/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 8.4768e-10 - val_loss: 2.4488e-10\n",
            "Epoch 306/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.1071e-09 - val_loss: 6.4801e-11\n",
            "Epoch 307/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1035e-09 - val_loss: 7.0890e-09\n",
            "Epoch 308/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.4056e-09 - val_loss: 1.8205e-09\n",
            "Epoch 309/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.2009e-09 - val_loss: 1.1432e-08\n",
            "Epoch 310/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.1325e-09 - val_loss: 4.8572e-10\n",
            "Epoch 311/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5839e-09 - val_loss: 1.2973e-09\n",
            "Epoch 312/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.8492e-09 - val_loss: 4.3263e-09\n",
            "Epoch 313/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.7467e-09 - val_loss: 1.7077e-07\n",
            "Epoch 314/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1043e-07 - val_loss: 1.0027e-07\n",
            "Epoch 315/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1928e-07 - val_loss: 2.8904e-09\n",
            "Epoch 316/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 9.9243e-09 - val_loss: 2.1053e-09\n",
            "Epoch 317/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.4463e-07 - val_loss: 7.9391e-07\n",
            "Epoch 318/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 3.2985e-06 - val_loss: 7.1488e-06\n",
            "Epoch 319/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 8.5476e-05 - val_loss: 7.1838e-07\n",
            "Epoch 320/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 5.2560e-05 - val_loss: 1.4598e-05\n",
            "Epoch 321/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.6341e-04 - val_loss: 1.4625e-05\n",
            "Epoch 322/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0969 - val_loss: 0.0242\n",
            "Epoch 323/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0130 - val_loss: 0.0014\n",
            "Epoch 324/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0145 - val_loss: 1.3942e-04\n",
            "Epoch 325/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0042\n",
            "Epoch 326/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0339 - val_loss: 0.0027\n",
            "Epoch 327/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2734 - val_loss: 0.0215\n",
            "Epoch 328/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.0049\n",
            "Epoch 329/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0022 - val_loss: 9.5969e-04\n",
            "Epoch 330/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 5.3691e-05 - val_loss: 7.4764e-07\n",
            "Epoch 331/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 2.1531e-07 - val_loss: 6.6252e-10\n",
            "Epoch 332/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.9099e-08 - val_loss: 4.3933e-09\n",
            "Epoch 333/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.8899e-09 - val_loss: 1.0409e-10\n",
            "Epoch 334/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 8.6384e-10 - val_loss: 1.1649e-09\n",
            "Epoch 335/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.2080e-10 - val_loss: 1.2628e-10\n",
            "Epoch 336/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.4502e-09 - val_loss: 1.4039e-09\n",
            "Epoch 337/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.6180e-10 - val_loss: 1.2810e-10\n",
            "Epoch 338/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.9643e-09 - val_loss: 1.1529e-09\n",
            "Epoch 339/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.7951e-09 - val_loss: 2.2887e-09\n",
            "Epoch 340/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 6.8588e-09 - val_loss: 8.9109e-09\n",
            "Epoch 341/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0710e-08 - val_loss: 3.7112e-10\n",
            "Epoch 342/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.9811e-08 - val_loss: 1.8089e-07\n",
            "Epoch 343/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.4797e-07 - val_loss: 1.4173e-06\n",
            "Epoch 344/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5186e-06 - val_loss: 2.5451e-06\n",
            "Epoch 345/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 5.4817e-06 - val_loss: 4.0265e-07\n",
            "Epoch 346/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.1647e-06 - val_loss: 1.4268e-06\n",
            "Epoch 347/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 8.1720e-06 - val_loss: 8.2938e-05\n",
            "Epoch 348/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.6489e-05 - val_loss: 3.4798e-07\n",
            "Epoch 349/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.0052\n",
            "Epoch 350/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.0153\n",
            "Epoch 351/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0528 - val_loss: 0.0189\n",
            "Epoch 352/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.8503 - val_loss: 0.9996\n",
            "Epoch 353/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.4233 - val_loss: 0.0066\n",
            "Epoch 354/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 3.2426e-04\n",
            "Epoch 355/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.2331e-04 - val_loss: 2.8108e-06\n",
            "Epoch 356/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.1541e-06 - val_loss: 1.1584e-06\n",
            "Epoch 357/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5965e-07 - val_loss: 4.7145e-09\n",
            "Epoch 358/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.8937e-09 - val_loss: 1.2810e-10\n",
            "Epoch 359/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.3747e-10 - val_loss: 1.2810e-10\n",
            "Epoch 360/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 8.4920e-11 - val_loss: 4.2610e-11\n",
            "Epoch 361/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.1020e-10 - val_loss: 1.2810e-10\n",
            "Epoch 362/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.2373e-10 - val_loss: 1.2810e-10\n",
            "Epoch 363/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.8545e-11 - val_loss: 4.2610e-11\n",
            "Epoch 364/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.2574e-10 - val_loss: 6.4801e-11\n",
            "Epoch 365/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.2218e-10 - val_loss: 6.4801e-11\n",
            "Epoch 366/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.0496e-10 - val_loss: 6.4801e-11\n",
            "Epoch 367/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.0300e-10 - val_loss: 2.3742e-09\n",
            "Epoch 368/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.3217e-10 - val_loss: 1.5721e-10\n",
            "Epoch 369/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.3634e-10 - val_loss: 1.5721e-10\n",
            "Epoch 370/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.9623e-10 - val_loss: 1.5721e-10\n",
            "Epoch 371/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 6.0879e-11 - val_loss: 6.4801e-11\n",
            "Epoch 372/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.2715e-10 - val_loss: 6.6652e-10\n",
            "Epoch 373/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.0738e-10 - val_loss: 1.5721e-10\n",
            "Epoch 374/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.7368e-10 - val_loss: 7.2619e-10\n",
            "Epoch 375/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.5517e-10 - val_loss: 1.2810e-10\n",
            "Epoch 376/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.2275e-10 - val_loss: 4.2610e-11\n",
            "Epoch 377/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.0352e-10 - val_loss: 1.5721e-10\n",
            "Epoch 378/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 5.4613e-10 - val_loss: 7.2619e-10\n",
            "Epoch 379/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.3422e-10 - val_loss: 1.1587e-09\n",
            "Epoch 380/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.8916e-09 - val_loss: 5.3952e-09\n",
            "Epoch 381/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.4549e-09 - val_loss: 6.9526e-09\n",
            "Epoch 382/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.8075e-08 - val_loss: 3.9331e-10\n",
            "Epoch 383/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.4083e-08 - val_loss: 8.7059e-08\n",
            "Epoch 384/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.2594e-08 - val_loss: 5.3287e-08\n",
            "Epoch 385/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0958e-06 - val_loss: 3.1309e-06\n",
            "Epoch 386/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0563e-06 - val_loss: 2.9961e-07\n",
            "Epoch 387/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.0956e-06 - val_loss: 4.2008e-05\n",
            "Epoch 388/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0290 - val_loss: 0.8879\n",
            "Epoch 389/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.2591 - val_loss: 2.0629e-04\n",
            "Epoch 390/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 9.2016e-05 - val_loss: 2.6595e-05\n",
            "Epoch 391/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.6712e-06 - val_loss: 4.7395e-07\n",
            "Epoch 392/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.1917e-06 - val_loss: 2.5820e-06\n",
            "Epoch 393/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.3683e-06 - val_loss: 6.0268e-07\n",
            "Epoch 394/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.6137e-06 - val_loss: 8.6029e-06\n",
            "Epoch 395/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.9115e-07 - val_loss: 6.0702e-08\n",
            "Epoch 396/500\n",
            "60/60 [==============================] - 1s 18ms/step - loss: 7.0151e-07 - val_loss: 1.7341e-06\n",
            "Epoch 397/500\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 4.6217e-07 - val_loss: 1.9607e-06\n",
            "Epoch 398/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.0786e-05 - val_loss: 9.7701e-08\n",
            "Epoch 399/500\n",
            "60/60 [==============================] - 1s 18ms/step - loss: 3.7841e-07 - val_loss: 9.1154e-09\n",
            "Epoch 400/500\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 4.0943e-08 - val_loss: 7.8120e-08\n",
            "Epoch 401/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.2530e-07 - val_loss: 2.1311e-07\n",
            "Epoch 402/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.1089e-08 - val_loss: 2.9412e-08\n",
            "Epoch 403/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.3491e-08 - val_loss: 1.2636e-08\n",
            "Epoch 404/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 5.2116e-08 - val_loss: 1.2419e-07\n",
            "Epoch 405/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.2574e-06 - val_loss: 1.1682e-09\n",
            "Epoch 406/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.7945e-08 - val_loss: 3.5548e-10\n",
            "Epoch 407/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 2.9574e-07 - val_loss: 9.8090e-06\n",
            "Epoch 408/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.2984e-06 - val_loss: 8.1343e-07\n",
            "Epoch 409/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.6319e-06 - val_loss: 6.6658e-07\n",
            "Epoch 410/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.0406e-07 - val_loss: 3.8868e-07\n",
            "Epoch 411/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.9081e-07 - val_loss: 4.2507e-09\n",
            "Epoch 412/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.5282e-08 - val_loss: 1.7696e-09\n",
            "Epoch 413/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.4986e-09 - val_loss: 2.4488e-10\n",
            "Epoch 414/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.3142e-08 - val_loss: 5.9997e-07\n",
            "Epoch 415/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.6385e-07 - val_loss: 1.0722e-09\n",
            "Epoch 416/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 7.0508e-07 - val_loss: 1.6786e-06\n",
            "Epoch 417/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.2633e-04 - val_loss: 8.5518e-05\n",
            "Epoch 418/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2366 - val_loss: 3.7029\n",
            "Epoch 419/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1421 - val_loss: 0.2811\n",
            "Epoch 420/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.0215\n",
            "Epoch 421/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0037 - val_loss: 1.7212e-05\n",
            "Epoch 422/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.3924e-05 - val_loss: 8.1367e-06\n",
            "Epoch 423/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.7020e-07 - val_loss: 7.7951e-09\n",
            "Epoch 424/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.2341e-09 - val_loss: 4.9158e-11\n",
            "Epoch 425/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0978e-10 - val_loss: 1.1428e-10\n",
            "Epoch 426/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1284e-10 - val_loss: 3.9336e-11\n",
            "Epoch 427/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.8421e-10 - val_loss: 3.9336e-11\n",
            "Epoch 428/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.4522e-11 - val_loss: 3.9336e-11\n",
            "Epoch 429/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 8.7649e-11 - val_loss: 1.8122e-10\n",
            "Epoch 430/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.1728e-10 - val_loss: 1.1428e-10\n",
            "Epoch 431/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.6803e-10 - val_loss: 1.8122e-10\n",
            "Epoch 432/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.8858e-10 - val_loss: 3.5548e-10\n",
            "Epoch 433/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.5803e-09 - val_loss: 7.0545e-10\n",
            "Epoch 434/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.1423e-09 - val_loss: 6.7889e-10\n",
            "Epoch 435/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.2019e-10 - val_loss: 1.2883e-10\n",
            "Epoch 436/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.4431e-11 - val_loss: 3.9699e-11\n",
            "Epoch 437/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.1806e-10 - val_loss: 3.6425e-11\n",
            "Epoch 438/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.7830e-10 - val_loss: 6.7889e-10\n",
            "Epoch 439/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.1235e-10 - val_loss: 2.8563e-10\n",
            "Epoch 440/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.2074e-10 - val_loss: 3.9699e-11\n",
            "Epoch 441/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 7.9403e-11 - val_loss: 1.2883e-10\n",
            "Epoch 442/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 9.6259e-11 - val_loss: 3.9699e-11\n",
            "Epoch 443/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1099e-10 - val_loss: 3.8822e-10\n",
            "Epoch 444/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.5444e-10 - val_loss: 1.3101e-10\n",
            "Epoch 445/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.8494e-10 - val_loss: 3.8822e-10\n",
            "Epoch 446/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.2836e-09 - val_loss: 3.3219e-10\n",
            "Epoch 447/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.8177e-09 - val_loss: 7.2676e-09\n",
            "Epoch 448/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.2304e-09 - val_loss: 9.1168e-09\n",
            "Epoch 449/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 6.9269e-09 - val_loss: 1.2108e-09\n",
            "Epoch 450/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.2339e-09 - val_loss: 6.0072e-11\n",
            "Epoch 451/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.1146e-09 - val_loss: 2.8409e-08\n",
            "Epoch 452/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.6130e-06 - val_loss: 3.9902e-06\n",
            "Epoch 453/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.4916e-06 - val_loss: 4.5466e-07\n",
            "Epoch 454/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 1.7508e-06 - val_loss: 3.5220e-08\n",
            "Epoch 455/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.1255e-06 - val_loss: 1.0399e-05\n",
            "Epoch 456/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 4.5539e-06 - val_loss: 6.8297e-06\n",
            "Epoch 457/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 5.1647e-06 - val_loss: 4.0014e-06\n",
            "Epoch 458/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0389e-04 - val_loss: 4.9718e-05\n",
            "Epoch 459/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 5.2113e-04 - val_loss: 3.2344e-05\n",
            "Epoch 460/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.0199\n",
            "Epoch 461/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 0.1897\n",
            "Epoch 462/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.0049\n",
            "Epoch 463/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.1644e-04 - val_loss: 4.1823e-04\n",
            "Epoch 464/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.8150e-04 - val_loss: 3.6560e-05\n",
            "Epoch 465/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 2.8918e-04 - val_loss: 1.7870e-04\n",
            "Epoch 466/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1606e-04 - val_loss: 5.1370e-05\n",
            "Epoch 467/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.4836e-04 - val_loss: 4.4694e-05\n",
            "Epoch 468/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 2.8013e-05 - val_loss: 1.7408e-06\n",
            "Epoch 469/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 3.3648e-05 - val_loss: 3.4135e-04\n",
            "Epoch 470/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.7271e-04 - val_loss: 2.7933e-05\n",
            "Epoch 471/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.4520e-04 - val_loss: 1.6127e-04\n",
            "Epoch 472/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.0425e-04 - val_loss: 2.5989e-05\n",
            "Epoch 473/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0238 - val_loss: 0.0034\n",
            "Epoch 474/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3427 - val_loss: 0.0341\n",
            "Epoch 475/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0043 - val_loss: 4.2238e-04\n",
            "Epoch 476/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.8661e-04 - val_loss: 1.9654e-04\n",
            "Epoch 477/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.4810e-05 - val_loss: 1.3417e-06\n",
            "Epoch 478/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 4.0036e-07 - val_loss: 4.1494e-08\n",
            "Epoch 479/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.9577e-08 - val_loss: 4.6789e-10\n",
            "Epoch 480/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.0588e-09 - val_loss: 2.2596e-09\n",
            "Epoch 481/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 3.3154e-10 - val_loss: 3.6061e-11\n",
            "Epoch 482/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.4376e-10 - val_loss: 1.2628e-10\n",
            "Epoch 483/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.1572e-10 - val_loss: 3.3838e-10\n",
            "Epoch 484/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.2312e-10 - val_loss: 3.8608e-11\n",
            "Epoch 485/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 3.3776e-10 - val_loss: 3.6061e-11\n",
            "Epoch 486/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 7.7614e-11 - val_loss: 1.2628e-10\n",
            "Epoch 487/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 6.0503e-10 - val_loss: 1.2628e-10\n",
            "Epoch 488/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.2728e-09 - val_loss: 4.2714e-09\n",
            "Epoch 489/500\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 3.5472e-08 - val_loss: 6.7589e-08\n",
            "Epoch 490/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.3688e-06 - val_loss: 3.7189e-08\n",
            "Epoch 491/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.9175e-08 - val_loss: 1.1082e-09\n",
            "Epoch 492/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 3.1727e-09 - val_loss: 9.0961e-09\n",
            "Epoch 493/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 8.4319e-08 - val_loss: 2.0636e-07\n",
            "Epoch 494/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 1.1764e-07 - val_loss: 5.9429e-08\n",
            "Epoch 495/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 3.6587e-07 - val_loss: 3.3081e-07\n",
            "Epoch 496/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 4.7031e-07 - val_loss: 1.9220e-09\n",
            "Epoch 497/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 2.2854e-06 - val_loss: 3.1964e-06\n",
            "Epoch 498/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 9.3862e-07 - val_loss: 1.6054e-05\n",
            "Epoch 499/500\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 6.7492e-04 - val_loss: 0.0032\n",
            "Epoch 500/500\n",
            "60/60 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.0421\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0574\n",
            "Test Loss: 0.05742105096578598\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No prompt\n",
        "Rewriting previous code to run a prediction through our model, and get the weights."
      ],
      "metadata": {
        "id": "PMENzveCfAi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a new Celsius value for prediction (not present in the larger dataset)\n",
        "new_celsius_value_large = 150.0\n",
        "\n",
        "# Reshape the input for prediction\n",
        "new_celsius_value_large = np.array([new_celsius_value_large]).reshape(-1, 1)\n",
        "\n",
        "# Use the trained model to predict the corresponding Fahrenheit value\n",
        "predicted_fahrenheit_large = model_new.predict(new_celsius_value_large)\n",
        "\n",
        "# Print the predicted Fahrenheit value\n",
        "print(f\"The predicted Fahrenheit value for {new_celsius_value_large[0, 0]} degrees Celsius is {predicted_fahrenheit_large[0, 0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5jdqstSexAq",
        "outputId": "89a39c4b-f997-44e3-a2de-f12f48a8369b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 115ms/step\n",
            "The predicted Fahrenheit value for 150.0 degrees Celsius is 301.43310546875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the weights of the trained model\n",
        "weights = model_new.get_weights()\n",
        "\n",
        "# Print the weights\n",
        "for layer_num, layer_weights in enumerate(weights):\n",
        "    print(f\"Layer {layer_num + 1} Weights:\")\n",
        "    print(layer_weights)\n",
        "    print(\"------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAZ19Mq5fL4t",
        "outputId": "9a920f9a-acd4-4679-aab9-d2d3a5bedf73"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Weights:\n",
            "[[1.7965027]]\n",
            "------------------------\n",
            "Layer 2 Weights:\n",
            "[31.95771]\n",
            "------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are pretty good results, almost landing at the exact expected weights (1.8 and 32)."
      ],
      "metadata": {
        "id": "DZHYTUnRfT2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-running the experiment with an even larger dataset to try achieve perfect weights"
      ],
      "metadata": {
        "id": "_uzCjDwZfcJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to convert Celsius to Fahrenheit\n",
        "def celsius_to_fahrenheit(celsius):\n",
        "    return (9/5) * celsius + 32\n",
        "\n",
        "# Generating a larger dataset with at least 100 inputs\n",
        "np.random.seed(42)  # Set seed for reproducibility\n",
        "celsius_values_large = np.random.uniform(low=-100, high=100, size=(250,))\n",
        "fahrenheit_values_large = celsius_to_fahrenheit(celsius_values_large)\n",
        "\n",
        "# Storing the dataset as vectors\n",
        "dataset_large = np.column_stack((celsius_values_large, fahrenheit_values_large))\n",
        "\n",
        "# Save the larger dataset to a CSV file\n",
        "np.savetxt('celsius_to_fahrenheit_dataset_final.csv', dataset_large, delimiter=',')\n",
        "\n",
        "# Displaying the larger dataset\n",
        "print(\"Celsius\\tFahrenheit\")\n",
        "print(\"-------------------\")\n",
        "for c, f in dataset_large:\n",
        "    print(f\"{c}\\t{f}\")\n",
        "\n",
        "print(\"Final dataset saved to 'celsius_to_fahrenheit_dataset_final.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "471iXrc2fidR",
        "outputId": "cf398c68-9255-4b74-f97d-b6b35f813fd0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Celsius\tFahrenheit\n",
            "-------------------\n",
            "-25.091976230527507\t-13.165557214949516\n",
            "90.14286128198324\t194.25715030756984\n",
            "46.39878836228101\t115.51781905210582\n",
            "19.73169683940732\t67.51705431093318\n",
            "-68.79627191151269\t-91.83328944072285\n",
            "-68.80109593275947\t-91.84197267896704\n",
            "-88.3832775663601\t-127.0898996194482\n",
            "73.23522915498702\t163.82341247897665\n",
            "20.223002348641756\t68.40140422755516\n",
            "41.61451555920911\t106.9061280065764\n",
            "-95.88310114083951\t-140.58958205351112\n",
            "93.98197043239887\t201.16754677831796\n",
            "66.48852816008434\t151.67935068815183\n",
            "-57.53217786434477\t-71.55792015582058\n",
            "-63.635006558579875\t-82.54301180544378\n",
            "-63.31909802931324\t-81.97437645276383\n",
            "-39.15155140809246\t-38.47279253456642\n",
            "4.951286326447573\t40.91231538760563\n",
            "-13.610996271576852\t7.500206711161667\n",
            "-41.754171960391616\t-43.15750952870491\n",
            "22.370578944475895\t72.26704210005661\n",
            "-72.10122786959164\t-97.78221016526496\n",
            "-41.57107029295637\t-42.827926527321466\n",
            "-26.727631341261656\t-16.109736414270984\n",
            "-8.786003156592813\t16.185194318132936\n",
            "57.03519227860272\t134.6633461014849\n",
            "-60.06524356832805\t-76.11743842299049\n",
            "2.8468876827223255\t37.124397828900186\n",
            "18.482913772408494\t65.26924479033529\n",
            "-90.70991745600045\t-131.27785142080083\n",
            "21.50897038028768\t70.71614668451782\n",
            "-65.8951752625417\t-86.61131547257506\n",
            "-86.9896814029441\t-124.58142652529938\n",
            "89.77710745066665\t193.59879341119998\n",
            "93.12640661491187\t199.62753190684137\n",
            "61.67946962329222\t143.02304532192602\n",
            "-39.07724616532586\t-38.339043097586554\n",
            "-80.46557719872322\t-112.8380389577018\n",
            "36.846605302431385\t98.3238895443765\n",
            "-11.969501252079738\t10.45489774625647\n",
            "-75.59235303104424\t-104.06623545587965\n",
            "-0.9646179777459594\t30.263687640057274\n",
            "-93.12229577695632\t-135.62013239852138\n",
            "81.8640804157564\t179.35534474836155\n",
            "-48.24400367999662\t-54.83920662399392\n",
            "32.50445687079639\t90.50802236743351\n",
            "-37.65778478211781\t-35.78401260781206\n",
            "4.0136042355621555\t39.22448762401188\n",
            "9.342055868655933\t48.81570056358068\n",
            "-63.02910889489459\t-81.45239601081026\n",
            "93.91692555291172\t201.0504659952411\n",
            "55.02656467222292\t131.04781641000125\n",
            "87.89978831283781\t190.21961896310808\n",
            "78.96547008552977\t174.1378461539536\n",
            "19.579995762217024\t67.24399237199064\n",
            "84.37484700462338\t183.87472460832208\n",
            "-82.3014995896161\t-116.14269926130899\n",
            "-60.80342751617096\t-77.44616952910773\n",
            "-90.95454221789238\t-131.71817599220628\n",
            "-34.933933847347134\t-30.881080925224843\n",
            "-22.2645420621036\t-8.076175711786483\n",
            "-45.730193645220815\t-50.314348561397466\n",
            "65.74750183038586\t150.34550329469454\n",
            "-28.64933466128214\t-19.56880239030785\n",
            "-43.81309806252385\t-46.863576512542934\n",
            "8.539216631649694\t47.37058993696945\n",
            "-71.81515500504747\t-97.26727900908546\n",
            "60.43939615080794\t140.79091307145427\n",
            "-85.08987126404584\t-121.16176827528253\n",
            "97.37738732010345\t207.27929717618622\n",
            "54.44895385933148\t130.0081169467967\n",
            "-60.25686369316552\t-76.46235464769794\n",
            "-98.89557657527952\t-146.01203783550315\n",
            "63.09228569096683\t145.5661142437403\n",
            "41.37146876952343\t106.46864378514218\n",
            "45.80143360819747\t114.44258049475545\n",
            "54.25406933718915\t129.6573248069405\n",
            "-85.19106965318193\t-121.34392537572748\n",
            "-28.306854291145484\t-18.95233772406187\n",
            "-76.82618809497406\t-106.28713857095332\n",
            "72.6206851751187\t162.71723331521366\n",
            "24.659625365511587\t76.38732565792085\n",
            "-33.82039502947016\t-28.876711053046293\n",
            "-87.28832994279527\t-125.11899389703149\n",
            "-37.80353565686756\t-36.046364182361614\n",
            "-34.96333559465059\t-30.93400407037106\n",
            "45.92123566761282\t114.65822420170308\n",
            "27.511494271042622\t81.52068968787671\n",
            "77.44254851526532\t171.39658732747756\n",
            "-5.557014967610144\t21.99737305830174\n",
            "-76.08115081233966\t-104.94607146221139\n",
            "42.648957444599006\t108.76812340027821\n",
            "52.157009723379474\t125.88261750208305\n",
            "12.255439513899248\t54.05979112501865\n",
            "54.19343599091221\t129.54818478364197\n",
            "-1.2408807271218478\t29.766414691180675\n",
            "4.546565876398816\t40.18381857751787\n",
            "-14.491796328290079\t5.914766609077859\n",
            "-94.91617465118097\t-138.84911437212574\n",
            "-78.42171460133912\t-109.15908628241041\n",
            "-93.71416286265315\t-136.68549315277568\n",
            "27.282082252756084\t81.10774805496095\n",
            "-37.128803784734664\t-34.8318468125224\n",
            "1.714138232940556\t35.085448819293\n",
            "81.51329478521859\t178.72393061339346\n",
            "-50.141554170225014\t-58.254797506405026\n",
            "-17.92341539287405\t-0.26214770717329117\n",
            "51.11022770860973\t123.99840987549752\n",
            "-54.24036690167551\t-65.63266042301592\n",
            "-84.6040180342414\t-120.28723246163455\n",
            "-42.0497094172464\t-43.68947695104352\n",
            "-67.75574254919911\t-89.9603365885584\n",
            "85.93953046851462\t186.69115484332633\n",
            "61.6240759128834\t142.9233366431901\n",
            "26.680751302084687\t80.02535234375244\n",
            "74.29211803754353\t165.72581246757835\n",
            "60.734415379822906\t141.32194768368123\n",
            "-62.68598822279283\t-80.8347788010271\n",
            "78.51179969799554\t173.321239456392\n",
            "7.8684483831301435\t46.163207089634255\n",
            "61.4880310328125\t142.67845585906252\n",
            "79.21825998469865\t174.59286797245758\n",
            "-36.39930500562723\t-33.518749010129014\n",
            "-77.98961509446465\t-108.3813071700364\n",
            "-54.41296749161167\t-65.943341484901\n",
            "-14.578442274748738\t5.758803905452272\n",
            "63.60295318449863\t146.48531573209755\n",
            "72.1461166512687\t161.86300997228366\n",
            "-98.60957389376186\t-145.49723300877136\n",
            "2.149460515513141\t35.86902892792365\n",
            "-16.517799370244205\t2.26796113356043\n",
            "-55.57843790585395\t-68.04118823053712\n",
            "-76.02692653326343\t-104.84846775987418\n",
            "-32.4769657192744\t-26.45853829469393\n",
            "88.58194078250384\t191.44749340850692\n",
            "-35.35941359584895\t-31.646944472528112\n",
            "3.7581243486732205\t38.7646238276118\n",
            "40.60379177903556\t105.08682520226401\n",
            "-27.2740795241412\t-17.093343143454163\n",
            "94.35641654419214\t201.84154977954586\n",
            "92.48945898842226\t198.48102617916007\n",
            "-49.643540834927165\t-57.3583735028689\n",
            "-0.5502988215229152\t31.009462121258753\n",
            "-39.82433803664607\t-39.683808465962926\n",
            "-43.031901124506476\t-45.457422024111665\n",
            "-92.62261052909344\t-134.7206989523682\n",
            "21.91286679597937\t71.44316023276286\n",
            "0.535804645772302\t32.964448362390144\n",
            "-89.70424975000213\t-129.46764955000384\n",
            "-44.270707152677716\t-47.6872728748199\n",
            "81.65317719333075\t178.97571894799535\n",
            "-52.08762186660552\t-61.75771935988993\n",
            "-71.02102558175538\t-95.83784604715969\n",
            "-2.1094479444873997\t28.20299369992268\n",
            "97.13009082212014\t206.83416347981625\n",
            "-51.58894569769992\t-60.86010225585986\n",
            "34.427109481175705\t93.96879706611628\n",
            "52.323923065743514\t126.18306151833833\n",
            "-52.472491201520064\t-62.45048416273612\n",
            "45.64326972237191\t114.15788550026944\n",
            "-26.44337345614936\t-15.598072221068847\n",
            "26.461166118715894\t79.6300990136886\n",
            "26.705942152178935\t80.07069587392209\n",
            "7.1549368149516965\t44.87888626691306\n",
            "-81.94204598911834\t-115.495682780413\n",
            "67.0604991178476\t152.70889841212568\n",
            "-35.843987005652835\t-32.51917661017511\n",
            "-62.69629792002915\t-80.85333625605247\n",
            "-91.84497168904721\t-133.320949040285\n",
            "18.17858863764836\t64.72145954776704\n",
            "35.51287236845647\t95.92317026322165\n",
            "-96.68243421442877\t-142.02838158597177\n",
            "2.4186116598561966\t36.35350098774116\n",
            "-54.70084496041241\t-66.46152092874235\n",
            "29.03455808188997\t84.26220454740195\n",
            "-65.12671419900171\t-85.22808555820308\n",
            "38.18754762049318\t100.73758571688772\n",
            "-22.652930739892525\t-8.775275331806547\n",
            "87.3459977473469\t189.22279594522442\n",
            "-72.49581117080135\t-98.49246010744244\n",
            "-31.786729789948296\t-25.216113621906935\n",
            "-77.30529575188218\t-107.14953235338794\n",
            "84.93872365571255\t184.8897025802826\n",
            "75.46787067619621\t167.8421672171532\n",
            "-48.41167445696888\t-55.141014022543985\n",
            "31.99680920683582\t89.59425657230449\n",
            "63.444440040243165\t146.19999207243768\n",
            "11.04016231989246\t51.87229217580643\n",
            "5.930115671201293\t42.67420820816233\n",
            "-51.62954181990966\t-60.9331752758374\n",
            "-81.37944643882015\t-114.48300358987629\n",
            "79.44315159066534\t174.9976728631976\n",
            "80.08361143266609\t176.15050057879895\n",
            "26.620291454653582\t79.91652461837646\n",
            "-32.19404179025986\t-25.949275222467747\n",
            "-30.158085077467817\t-22.284553139442075\n",
            "45.191135774047865\t113.34404439328615\n",
            "79.42205199051543\t174.95969358292777\n",
            "77.41728485302346\t171.35111273544223\n",
            "55.975109171524764\t132.75519650874458\n",
            "28.40632923085755\t83.1313926155436\n",
            "-83.17200700099023\t-117.70961260178242\n",
            "-67.67425718107725\t-89.81366292593906\n",
            "79.71083770541586\t175.47950786974855\n",
            "21.28581193191799\t70.31446147745238\n",
            "-98.16058967667406\t-144.68906141801332\n",
            "-79.70569142679358\t-111.47024456822845\n",
            "32.70035382161117\t90.86063687890012\n",
            "-98.98768323075626\t-146.17782981536126\n",
            "-67.83838971650027\t-90.10910148970049\n",
            "9.746757873317222\t49.544164171971005\n",
            "38.37903953853865\t101.08227116936958\n",
            "30.392251900520108\t86.7060534209362\n",
            "-55.14613810788804\t-67.26304859419848\n",
            "42.435844269507186\t108.38451968511293\n",
            "-52.550182500639984\t-62.59032850115197\n",
            "-34.92006036814645\t-30.856108662663615\n",
            "49.29828102360483\t120.7369058424887\n",
            "29.926579809442927\t85.86784365699728\n",
            "69.84468209883559\t157.72042777790406\n",
            "31.522578460068672\t88.74064122812362\n",
            "13.661720667094329\t56.591097200769795\n",
            "-81.2650464343815\t-114.2770835818867\n",
            "-26.456839388113295\t-15.622310898603935\n",
            "-46.95952646365491\t-52.52714763457884\n",
            "-51.20207132418329\t-60.163728383529914\n",
            "94.6021109504891\t202.2837997108804\n",
            "-21.380455066647926\t-6.484819119966268\n",
            "78.40931103542266\t173.1367598637608\n",
            "26.22772519945258\t79.20990535901464\n",
            "58.962260708329694\t138.13206927499346\n",
            "0.5274186210384215\t32.949353517869156\n",
            "15.380776925271817\t59.68539846548927\n",
            "-1.4964612362272192\t29.306369774791005\n",
            "-60.9514024403911\t-77.71252439270398\n",
            "44.490423052301054\t112.0827614941419\n",
            "-43.84552751182884\t-46.92194952129192\n",
            "-95.13680671370923\t-139.24625208467663\n",
            "29.094459181433564\t84.37002652658042\n",
            "-64.57786411859021\t-84.24015541346239\n",
            "88.09171687058287\t190.56509036704918\n",
            "90.78571540051746\t195.41428772093144\n",
            "82.9728780440897\t181.35118047936146\n",
            "-25.968259948911125\t-14.742867908040026\n",
            "-96.90867669422651\t-142.43561804960774\n",
            "85.66371251754506\t186.19468253158112\n",
            "-14.363170336537138\t6.146293394233151\n",
            "93.33096380873391\t199.99573485572105\n",
            "92.72399541785057\t198.90319175213105\n",
            "70.601891093472\t159.08340396824963\n",
            "Final dataset saved to 'celsius_to_fahrenheit_dataset_final.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "dataset = np.loadtxt('celsius_to_fahrenheit_dataset_final.csv', delimiter=',')\n",
        "\n",
        "# Split the dataset into features (X) and labels (y)\n",
        "X = dataset[:, 0]  # Celsius values\n",
        "y = dataset[:, 1]  # Fahrenheit values\n",
        "\n",
        "# Split the dataset into training, testing, and validation sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Build the neural network model\n",
        "model_final = keras.Sequential([\n",
        "    layers.Dense(1, input_shape=[1])  # Single neuron, single layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_final.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model_final.fit(X_train, y_train, epochs=500, batch_size=1, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss = model_final.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# Display the training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gzlanv5mf_09",
        "outputId": "75dfd435-1f3d-4a83-e396-8d9a58e87e00"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 31507.4863 - val_loss: 22501.0781\n",
            "Epoch 2/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 15061.2285 - val_loss: 9957.6279\n",
            "Epoch 3/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6493.9849 - val_loss: 4181.7783\n",
            "Epoch 4/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2765.1797 - val_loss: 1897.5975\n",
            "Epoch 5/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1416.9086 - val_loss: 1138.9613\n",
            "Epoch 6/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1012.2812 - val_loss: 932.8133\n",
            "Epoch 7/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 899.8946 - val_loss: 869.7457\n",
            "Epoch 8/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 849.7523 - val_loss: 829.6760\n",
            "Epoch 9/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 816.1767 - val_loss: 793.4169\n",
            "Epoch 10/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 780.0975 - val_loss: 756.6957\n",
            "Epoch 11/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 739.6785 - val_loss: 717.1415\n",
            "Epoch 12/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 702.7133 - val_loss: 678.3104\n",
            "Epoch 13/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 663.0695 - val_loss: 638.1505\n",
            "Epoch 14/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 622.5802 - val_loss: 598.4805\n",
            "Epoch 15/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 585.9863 - val_loss: 558.9771\n",
            "Epoch 16/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 543.4025 - val_loss: 521.9858\n",
            "Epoch 17/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 504.3176 - val_loss: 481.9021\n",
            "Epoch 18/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 465.7432 - val_loss: 446.2294\n",
            "Epoch 19/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 432.7571 - val_loss: 408.5618\n",
            "Epoch 20/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 394.1592 - val_loss: 373.1112\n",
            "Epoch 21/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 359.9603 - val_loss: 340.5493\n",
            "Epoch 22/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 327.1751 - val_loss: 307.1653\n",
            "Epoch 23/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 293.8859 - val_loss: 279.0399\n",
            "Epoch 24/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 265.8982 - val_loss: 247.8497\n",
            "Epoch 25/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 238.2239 - val_loss: 221.6766\n",
            "Epoch 26/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 212.8135 - val_loss: 195.5336\n",
            "Epoch 27/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 184.5559 - val_loss: 172.7772\n",
            "Epoch 28/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 162.8196 - val_loss: 152.0598\n",
            "Epoch 29/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 142.8489 - val_loss: 130.4078\n",
            "Epoch 30/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 123.1074 - val_loss: 112.1255\n",
            "Epoch 31/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 106.3924 - val_loss: 95.6358\n",
            "Epoch 32/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 89.1466 - val_loss: 86.9258\n",
            "Epoch 33/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 77.1112 - val_loss: 68.7119\n",
            "Epoch 34/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 62.9508 - val_loss: 56.2765\n",
            "Epoch 35/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 51.8923 - val_loss: 46.5088\n",
            "Epoch 36/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 42.6379 - val_loss: 38.5753\n",
            "Epoch 37/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 34.7395 - val_loss: 29.8517\n",
            "Epoch 38/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 27.1988 - val_loss: 23.4929\n",
            "Epoch 39/500\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 21.4331 - val_loss: 19.7688\n",
            "Epoch 40/500\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 16.4019 - val_loss: 14.3170\n",
            "Epoch 41/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 12.3589 - val_loss: 10.4788\n",
            "Epoch 42/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.2594 - val_loss: 7.6408\n",
            "Epoch 43/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 6.6539 - val_loss: 5.4772\n",
            "Epoch 44/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 4.8964 - val_loss: 4.0744\n",
            "Epoch 45/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 3.3703 - val_loss: 2.7556\n",
            "Epoch 46/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.1898 - val_loss: 1.9053\n",
            "Epoch 47/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.5105 - val_loss: 1.1364\n",
            "Epoch 48/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.9471 - val_loss: 0.6963\n",
            "Epoch 49/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5801 - val_loss: 0.4261\n",
            "Epoch 50/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3470 - val_loss: 0.2378\n",
            "Epoch 51/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1901 - val_loss: 0.1371\n",
            "Epoch 52/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0999 - val_loss: 0.0925\n",
            "Epoch 53/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.0345\n",
            "Epoch 54/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0179\n",
            "Epoch 55/500\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.0116 - val_loss: 0.0073\n",
            "Epoch 56/500\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 0.0050 - val_loss: 0.0037\n",
            "Epoch 57/500\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 0.0020 - val_loss: 0.0012\n",
            "Epoch 58/500\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 7.9146e-04 - val_loss: 4.2590e-04\n",
            "Epoch 59/500\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 2.8221e-04 - val_loss: 1.4470e-04\n",
            "Epoch 60/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.5086e-05 - val_loss: 4.2981e-05\n",
            "Epoch 61/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 2.7042e-05 - val_loss: 1.0432e-05\n",
            "Epoch 62/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 6.4002e-06 - val_loss: 2.5621e-06\n",
            "Epoch 63/500\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 1.5167e-06 - val_loss: 5.5298e-07\n",
            "Epoch 64/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.0533e-07 - val_loss: 2.0102e-07\n",
            "Epoch 65/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.4490e-08 - val_loss: 2.1628e-08\n",
            "Epoch 66/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.1655e-08 - val_loss: 2.6396e-08\n",
            "Epoch 67/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 2.1780e-08 - val_loss: 1.2383e-08\n",
            "Epoch 68/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.4833e-08 - val_loss: 1.1904e-08\n",
            "Epoch 69/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4034e-08 - val_loss: 1.1915e-08\n",
            "Epoch 70/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.2653e-08 - val_loss: 1.0152e-08\n",
            "Epoch 71/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0559e-08 - val_loss: 6.9961e-09\n",
            "Epoch 72/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.6257e-09 - val_loss: 7.1626e-09\n",
            "Epoch 73/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.1888e-09 - val_loss: 6.4173e-09\n",
            "Epoch 74/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.4247e-09 - val_loss: 3.6879e-09\n",
            "Epoch 75/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.8231e-09 - val_loss: 2.6874e-09\n",
            "Epoch 76/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 3.2839e-09 - val_loss: 2.6874e-09\n",
            "Epoch 77/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.9147e-09 - val_loss: 2.2722e-09\n",
            "Epoch 78/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.5233e-09 - val_loss: 1.9394e-09\n",
            "Epoch 79/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.3764e-09 - val_loss: 1.7941e-09\n",
            "Epoch 80/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.5827e-09 - val_loss: 2.6645e-09\n",
            "Epoch 81/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.4053e-09 - val_loss: 3.0837e-09\n",
            "Epoch 82/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.1398e-09 - val_loss: 6.9538e-10\n",
            "Epoch 83/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.2681e-09 - val_loss: 9.0151e-10\n",
            "Epoch 84/500\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 1.0651e-09 - val_loss: 1.8879e-09\n",
            "Epoch 85/500\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 1.6043e-09 - val_loss: 5.4462e-10\n",
            "Epoch 86/500\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 5.2885e-10 - val_loss: 3.2933e-10\n",
            "Epoch 87/500\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 6.4510e-10 - val_loss: 1.9902e-10\n",
            "Epoch 88/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.3585e-10 - val_loss: 9.2181e-10\n",
            "Epoch 89/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.7309e-10 - val_loss: 4.8300e-10\n",
            "Epoch 90/500\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 5.1374e-10 - val_loss: 6.9364e-10\n",
            "Epoch 91/500\n",
            "150/150 [==============================] - 1s 7ms/step - loss: 3.0420e-10 - val_loss: 1.7639e-10\n",
            "Epoch 92/500\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 4.9764e-10 - val_loss: 6.6593e-11\n",
            "Epoch 93/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.1825e-10 - val_loss: 7.0086e-11\n",
            "Epoch 94/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.6694e-09 - val_loss: 1.5566e-09\n",
            "Epoch 95/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.0176e-10 - val_loss: 2.1628e-11\n",
            "Epoch 96/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.5385e-10 - val_loss: 2.1628e-11\n",
            "Epoch 97/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.5772e-09 - val_loss: 4.8925e-09\n",
            "Epoch 98/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.3305e-09 - val_loss: 3.7045e-08\n",
            "Epoch 99/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4688e-07 - val_loss: 3.3934e-09\n",
            "Epoch 100/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.1920e-08 - val_loss: 6.3096e-09\n",
            "Epoch 101/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.0309e-05 - val_loss: 3.9340e-05\n",
            "Epoch 102/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6838e-04 - val_loss: 0.0069\n",
            "Epoch 103/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.1166 - val_loss: 6.7352e-04\n",
            "Epoch 104/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 5.1675e-05\n",
            "Epoch 105/500\n",
            "150/150 [==============================] - 1s 5ms/step - loss: 8.6005e-05 - val_loss: 6.8338e-10\n",
            "Epoch 106/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.5846e-09 - val_loss: 6.0012e-09\n",
            "Epoch 107/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.8232e-09 - val_loss: 2.3027e-08\n",
            "Epoch 108/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.2030e-09 - val_loss: 2.7303e-11\n",
            "Epoch 109/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.5250e-07 - val_loss: 2.7146e-07\n",
            "Epoch 110/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.1026e-05 - val_loss: 1.4418e-05\n",
            "Epoch 111/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.6502e-05 - val_loss: 0.0012\n",
            "Epoch 112/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0943 - val_loss: 0.0400\n",
            "Epoch 113/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0031 - val_loss: 1.2418e-05\n",
            "Epoch 114/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.2439e-06 - val_loss: 2.4787e-07\n",
            "Epoch 115/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.8902e-08 - val_loss: 6.8301e-10\n",
            "Epoch 116/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 4.9951e-09 - val_loss: 2.2792e-11\n",
            "Epoch 117/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.2855e-09 - val_loss: 6.7959e-10\n",
            "Epoch 118/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 9.6513e-10 - val_loss: 2.2792e-11\n",
            "Epoch 119/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 8.2906e-07 - val_loss: 2.1451e-05\n",
            "Epoch 120/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.3010e-05 - val_loss: 4.3825e-05\n",
            "Epoch 121/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.0459\n",
            "Epoch 122/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5530 - val_loss: 0.0775\n",
            "Epoch 123/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 3.4545e-08\n",
            "Epoch 124/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.9404e-09 - val_loss: 1.0399e-10\n",
            "Epoch 125/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.0200e-10 - val_loss: 1.0086e-10\n",
            "Epoch 126/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.4205e-10 - val_loss: 4.8300e-10\n",
            "Epoch 127/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0037e-09 - val_loss: 3.6543e-11\n",
            "Epoch 128/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.3473e-10 - val_loss: 1.1090e-10\n",
            "Epoch 129/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.6225e-10 - val_loss: 3.6543e-11\n",
            "Epoch 130/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.7196e-10 - val_loss: 1.2570e-09\n",
            "Epoch 131/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.5223e-10 - val_loss: 3.6543e-11\n",
            "Epoch 132/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4249e-08 - val_loss: 6.9121e-09\n",
            "Epoch 133/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 9.1204e-09 - val_loss: 5.9088e-09\n",
            "Epoch 134/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.2030e-09 - val_loss: 1.1090e-10\n",
            "Epoch 135/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.6657e-06 - val_loss: 2.9648e-05\n",
            "Epoch 136/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.8648e-06 - val_loss: 1.4264e-05\n",
            "Epoch 137/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 5.8957e-04\n",
            "Epoch 138/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.0778\n",
            "Epoch 139/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.8044 - val_loss: 0.2542\n",
            "Epoch 140/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 4.7173e-06\n",
            "Epoch 141/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.3134e-06 - val_loss: 2.3387e-10\n",
            "Epoch 142/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.1549e-10 - val_loss: 1.9756e-10\n",
            "Epoch 143/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.7921e-10 - val_loss: 2.8102e-10\n",
            "Epoch 144/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.7868e-10 - val_loss: 2.8102e-10\n",
            "Epoch 145/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.4973e-10 - val_loss: 2.4140e-09\n",
            "Epoch 146/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6844e-09 - val_loss: 1.9756e-10\n",
            "Epoch 147/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6965e-09 - val_loss: 1.2161e-09\n",
            "Epoch 148/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.0148e-10 - val_loss: 3.1963e-09\n",
            "Epoch 149/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.0423e-09 - val_loss: 1.2161e-09\n",
            "Epoch 150/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.7237e-09 - val_loss: 7.0382e-10\n",
            "Epoch 151/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4456e-09 - val_loss: 1.4452e-10\n",
            "Epoch 152/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.8647e-09 - val_loss: 3.1521e-10\n",
            "Epoch 153/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.3161e-10 - val_loss: 2.7303e-11\n",
            "Epoch 154/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.1341e-09 - val_loss: 3.9212e-09\n",
            "Epoch 155/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 9.9940e-10 - val_loss: 1.6606e-10\n",
            "Epoch 156/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.6235e-09 - val_loss: 1.4263e-10\n",
            "Epoch 157/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.5450e-08 - val_loss: 3.0051e-09\n",
            "Epoch 158/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 6.3021e-07 - val_loss: 3.9987e-05\n",
            "Epoch 159/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2974 - val_loss: 0.4955\n",
            "Epoch 160/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.1717 - val_loss: 8.8741e-04\n",
            "Epoch 161/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 6.0429e-04 - val_loss: 1.5302e-06\n",
            "Epoch 162/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.2479e-07 - val_loss: 3.1521e-10\n",
            "Epoch 163/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.3827e-10 - val_loss: 3.1521e-10\n",
            "Epoch 164/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6058e-09 - val_loss: 2.3387e-10\n",
            "Epoch 165/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.5874e-10 - val_loss: 1.2900e-09\n",
            "Epoch 166/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.8817e-09 - val_loss: 7.1680e-09\n",
            "Epoch 167/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.6330e-08 - val_loss: 6.3096e-09\n",
            "Epoch 168/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.3516e-09 - val_loss: 2.1628e-11\n",
            "Epoch 169/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.7457e-10 - val_loss: 2.2719e-09\n",
            "Epoch 170/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.6449e-10 - val_loss: 4.0929e-10\n",
            "Epoch 171/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.8704e-10 - val_loss: 5.8492e-09\n",
            "Epoch 172/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4329e-06 - val_loss: 9.4511e-07\n",
            "Epoch 173/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.6169e-06 - val_loss: 2.0054e-10\n",
            "Epoch 174/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 9.3317e-07 - val_loss: 7.7110e-07\n",
            "Epoch 175/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.9625e-04 - val_loss: 3.9528e-05\n",
            "Epoch 176/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 3.3727e-05\n",
            "Epoch 177/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.0054\n",
            "Epoch 178/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 7.5189e-04\n",
            "Epoch 179/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 1.7171e-04\n",
            "Epoch 180/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.0702\n",
            "Epoch 181/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0119 - val_loss: 3.6718e-05\n",
            "Epoch 182/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.0711e-05 - val_loss: 2.6792e-05\n",
            "Epoch 183/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.3416e-05 - val_loss: 2.7298e-08\n",
            "Epoch 184/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.7145e-06 - val_loss: 2.2427e-06\n",
            "Epoch 185/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.4453e-06 - val_loss: 6.5829e-05\n",
            "Epoch 186/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.0669\n",
            "Epoch 187/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0078 - val_loss: 7.9584e-06\n",
            "Epoch 188/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.3606e-04 - val_loss: 0.0017\n",
            "Epoch 189/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 2.5424e-04\n",
            "Epoch 190/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0270 - val_loss: 6.1680e-04\n",
            "Epoch 191/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.1841 - val_loss: 0.0017\n",
            "Epoch 192/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 3.5277e-07\n",
            "Epoch 193/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.1447e-06 - val_loss: 3.3464e-06\n",
            "Epoch 194/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 1.6685e-05 - val_loss: 1.3077e-05\n",
            "Epoch 195/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 4.7610e-07 - val_loss: 1.4580e-08\n",
            "Epoch 196/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 4.3964e-08 - val_loss: 3.8091e-09\n",
            "Epoch 197/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.2907e-08 - val_loss: 4.5703e-08\n",
            "Epoch 198/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 1.9406e-07 - val_loss: 1.1628e-07\n",
            "Epoch 199/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 3.1237e-05 - val_loss: 4.9491e-04\n",
            "Epoch 200/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0153 - val_loss: 0.0085\n",
            "Epoch 201/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0021 - val_loss: 5.6560e-04\n",
            "Epoch 202/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1043 - val_loss: 0.0470\n",
            "Epoch 203/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 4.5068e-05\n",
            "Epoch 204/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.9748e-04 - val_loss: 1.7715e-05\n",
            "Epoch 205/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.8597e-07 - val_loss: 7.9601e-08\n",
            "Epoch 206/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.8332e-08 - val_loss: 3.1106e-09\n",
            "Epoch 207/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 1.4395e-04\n",
            "Epoch 208/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.2809e-04 - val_loss: 1.3697e-05\n",
            "Epoch 209/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.6814e-05 - val_loss: 3.6451e-06\n",
            "Epoch 210/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 4.2338e-04\n",
            "Epoch 211/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 1.5481e-04\n",
            "Epoch 212/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.0029\n",
            "Epoch 213/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.3141 - val_loss: 0.0023\n",
            "Epoch 214/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.0110e-04 - val_loss: 1.0019e-06\n",
            "Epoch 215/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0089e-07 - val_loss: 3.3056e-10\n",
            "Epoch 216/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 4.3527e-09 - val_loss: 1.6882e-10\n",
            "Epoch 217/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.5414e-09 - val_loss: 3.9467e-10\n",
            "Epoch 218/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.1510e-10 - val_loss: 3.0538e-09\n",
            "Epoch 219/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 9.1369e-10 - val_loss: 3.3850e-10\n",
            "Epoch 220/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.2585e-09 - val_loss: 1.0956e-09\n",
            "Epoch 221/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.1281e-10 - val_loss: 4.9116e-09\n",
            "Epoch 222/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.4463e-08 - val_loss: 1.5954e-08\n",
            "Epoch 223/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.1002e-07 - val_loss: 2.9380e-06\n",
            "Epoch 224/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0955e-05 - val_loss: 1.8488e-05\n",
            "Epoch 225/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.8270e-04 - val_loss: 5.9490e-04\n",
            "Epoch 226/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 1.0473e-04\n",
            "Epoch 227/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.3937e-04 - val_loss: 1.2054e-04\n",
            "Epoch 228/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.7842e-04 - val_loss: 7.1990e-04\n",
            "Epoch 229/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.0464\n",
            "Epoch 230/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4647 - val_loss: 0.0032\n",
            "Epoch 231/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.7665e-04 - val_loss: 1.3280e-08\n",
            "Epoch 232/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.5304e-09 - val_loss: 4.7307e-09\n",
            "Epoch 233/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.3278e-09 - val_loss: 7.6440e-09\n",
            "Epoch 234/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.1314e-09 - val_loss: 1.4263e-10\n",
            "Epoch 235/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 6.1113e-10 - val_loss: 1.1090e-10\n",
            "Epoch 236/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.2905e-10 - val_loss: 1.1090e-10\n",
            "Epoch 237/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.9902e-10 - val_loss: 2.1628e-11\n",
            "Epoch 238/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 3.5986e-10 - val_loss: 6.7763e-10\n",
            "Epoch 239/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.0739e-10 - val_loss: 3.1255e-08\n",
            "Epoch 240/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.8878e-09 - val_loss: 9.5204e-09\n",
            "Epoch 241/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.5286e-08 - val_loss: 9.6733e-09\n",
            "Epoch 242/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 1.1270e-08 - val_loss: 3.2554e-08\n",
            "Epoch 243/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.1707e-08 - val_loss: 1.0849e-07\n",
            "Epoch 244/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4780e-06 - val_loss: 3.3718e-06\n",
            "Epoch 245/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.9421e-06 - val_loss: 2.2305e-09\n",
            "Epoch 246/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.4133e-08 - val_loss: 6.2124e-09\n",
            "Epoch 247/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.1346 - val_loss: 0.0178\n",
            "Epoch 248/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4551 - val_loss: 0.0364\n",
            "Epoch 249/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 1.0726e-04\n",
            "Epoch 250/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.0095e-05 - val_loss: 6.8301e-10\n",
            "Epoch 251/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.4247e-09 - val_loss: 1.0457e-09\n",
            "Epoch 252/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.3264e-10 - val_loss: 3.9568e-10\n",
            "Epoch 253/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4541e-10 - val_loss: 1.1090e-10\n",
            "Epoch 254/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.9226e-10 - val_loss: 2.1628e-11\n",
            "Epoch 255/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.9662e-10 - val_loss: 1.1090e-10\n",
            "Epoch 256/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.9407e-10 - val_loss: 2.1628e-11\n",
            "Epoch 257/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.4492e-10 - val_loss: 2.1628e-11\n",
            "Epoch 258/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6594e-10 - val_loss: 3.9568e-10\n",
            "Epoch 259/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.3979e-10 - val_loss: 1.1090e-10\n",
            "Epoch 260/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.6400e-10 - val_loss: 2.1628e-11\n",
            "Epoch 261/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.6079e-09 - val_loss: 2.2998e-09\n",
            "Epoch 262/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.5777e-09 - val_loss: 8.1972e-09\n",
            "Epoch 263/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.1955e-08 - val_loss: 5.5197e-10\n",
            "Epoch 264/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.3688e-08 - val_loss: 1.7513e-07\n",
            "Epoch 265/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6410e-04 - val_loss: 2.4627e-05\n",
            "Epoch 266/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.2471\n",
            "Epoch 267/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3792 - val_loss: 0.0539\n",
            "Epoch 268/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0168 - val_loss: 1.8352e-05\n",
            "Epoch 269/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6338e-05 - val_loss: 8.1590e-08\n",
            "Epoch 270/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.9289e-08 - val_loss: 9.9495e-09\n",
            "Epoch 271/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.1472e-09 - val_loss: 2.7303e-11\n",
            "Epoch 272/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.3612e-09 - val_loss: 1.2161e-09\n",
            "Epoch 273/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.3862e-09 - val_loss: 3.6509e-09\n",
            "Epoch 274/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.9272e-08 - val_loss: 7.1688e-09\n",
            "Epoch 275/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.0185e-10 - val_loss: 3.9568e-10\n",
            "Epoch 276/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 3.7833e-09 - val_loss: 3.5087e-09\n",
            "Epoch 277/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.4561e-08 - val_loss: 6.3590e-09\n",
            "Epoch 278/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 4.5847e-09 - val_loss: 2.1628e-11\n",
            "Epoch 279/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.9799e-08 - val_loss: 5.1936e-08\n",
            "Epoch 280/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.3436e-05 - val_loss: 1.2511e-06\n",
            "Epoch 281/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 9.0951e-06 - val_loss: 2.8088e-05\n",
            "Epoch 282/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.2231 - val_loss: 0.0231\n",
            "Epoch 283/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0095 - val_loss: 1.5902e-08\n",
            "Epoch 284/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.1635e-07 - val_loss: 4.0543e-10\n",
            "Epoch 285/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 7.4008e-10 - val_loss: 4.2219e-11\n",
            "Epoch 286/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.1844e-10 - val_loss: 2.7303e-11\n",
            "Epoch 287/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.3250e-10 - val_loss: 6.3048e-10\n",
            "Epoch 288/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0788e-09 - val_loss: 7.2754e-10\n",
            "Epoch 289/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.9961e-08 - val_loss: 3.8228e-09\n",
            "Epoch 290/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.1927e-06 - val_loss: 1.6181e-05\n",
            "Epoch 291/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0316 - val_loss: 0.0614\n",
            "Epoch 292/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.0172\n",
            "Epoch 293/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 1.8844e-06\n",
            "Epoch 294/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.0953e-05 - val_loss: 1.1445e-06\n",
            "Epoch 295/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.5846e-04 - val_loss: 1.1483e-04\n",
            "Epoch 296/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.1762 - val_loss: 0.0546\n",
            "Epoch 297/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 2.0507e-04\n",
            "Epoch 298/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.5202e-05 - val_loss: 6.2491e-09\n",
            "Epoch 299/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.2450e-07 - val_loss: 1.2926e-07\n",
            "Epoch 300/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.0563e-08 - val_loss: 8.2590e-09\n",
            "Epoch 301/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.1307e-08 - val_loss: 2.1956e-06\n",
            "Epoch 302/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.1594e-04 - val_loss: 0.0012\n",
            "Epoch 303/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 3.1438e-06\n",
            "Epoch 304/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.6512e-07 - val_loss: 9.5276e-08\n",
            "Epoch 305/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0086 - val_loss: 0.0021\n",
            "Epoch 306/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0540 - val_loss: 0.0134\n",
            "Epoch 307/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 3.9023e-07\n",
            "Epoch 308/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.0073e-08 - val_loss: 1.1425e-09\n",
            "Epoch 309/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.1051e-08 - val_loss: 1.6149e-07\n",
            "Epoch 310/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.5219e-04 - val_loss: 9.8855e-04\n",
            "Epoch 311/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 1.2439e-04\n",
            "Epoch 312/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.7496e-05 - val_loss: 2.4651e-05\n",
            "Epoch 313/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.0016\n",
            "Epoch 314/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.0020\n",
            "Epoch 315/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.7477e-04 - val_loss: 8.4656e-07\n",
            "Epoch 316/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.0053 - val_loss: 0.0771\n",
            "Epoch 317/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4388 - val_loss: 5.6781e-05\n",
            "Epoch 318/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 3.6446e-06 - val_loss: 5.0606e-10\n",
            "Epoch 319/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.7923e-10 - val_loss: 8.7946e-10\n",
            "Epoch 320/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 6.5959e-10 - val_loss: 1.0210e-10\n",
            "Epoch 321/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 9.0692e-11 - val_loss: 2.2792e-11\n",
            "Epoch 322/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 4.0306e-10 - val_loss: 2.1628e-11\n",
            "Epoch 323/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 2.1479e-10 - val_loss: 6.2102e-10\n",
            "Epoch 324/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.5330e-09 - val_loss: 5.3742e-10\n",
            "Epoch 325/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.5910e-09 - val_loss: 6.3048e-10\n",
            "Epoch 326/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.3512e-08 - val_loss: 2.2907e-10\n",
            "Epoch 327/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6201e-09 - val_loss: 1.5900e-09\n",
            "Epoch 328/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6826e-09 - val_loss: 1.8286e-10\n",
            "Epoch 329/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.7343e-08 - val_loss: 1.3957e-10\n",
            "Epoch 330/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.1060e-08 - val_loss: 6.2052e-09\n",
            "Epoch 331/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.2700e-08 - val_loss: 1.6901e-08\n",
            "Epoch 332/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4718e-05 - val_loss: 2.3591e-04\n",
            "Epoch 333/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.1602 - val_loss: 0.0329\n",
            "Epoch 334/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0139 - val_loss: 0.0083\n",
            "Epoch 335/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 7.6693e-04\n",
            "Epoch 336/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.9647e-04 - val_loss: 1.8486e-07\n",
            "Epoch 337/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.5479e-08 - val_loss: 7.0573e-09\n",
            "Epoch 338/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.7958e-10 - val_loss: 4.0377e-09\n",
            "Epoch 339/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0799e-07 - val_loss: 1.9785e-07\n",
            "Epoch 340/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.9146e-07 - val_loss: 4.2529e-06\n",
            "Epoch 341/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.9331e-06 - val_loss: 2.2969e-06\n",
            "Epoch 342/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.9107e-07 - val_loss: 2.1132e-07\n",
            "Epoch 343/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.2846e-04 - val_loss: 1.2654e-04\n",
            "Epoch 344/500\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.0448 - val_loss: 4.6457e-04\n",
            "Epoch 345/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0262 - val_loss: 0.0073\n",
            "Epoch 346/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5567 - val_loss: 0.0186\n",
            "Epoch 347/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 4.9097e-08\n",
            "Epoch 348/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.6863e-08 - val_loss: 4.8300e-10\n",
            "Epoch 349/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.4527e-10 - val_loss: 2.1510e-10\n",
            "Epoch 350/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.8298e-10 - val_loss: 2.1510e-10\n",
            "Epoch 351/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.6140e-10 - val_loss: 2.1510e-10\n",
            "Epoch 352/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.1245e-09 - val_loss: 5.2987e-11\n",
            "Epoch 353/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.3569e-10 - val_loss: 1.0956e-09\n",
            "Epoch 354/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.0257e-10 - val_loss: 3.4766e-10\n",
            "Epoch 355/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 4.6226e-10 - val_loss: 2.1628e-11\n",
            "Epoch 356/500\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 3.3289e-10 - val_loss: 2.1628e-11\n",
            "Epoch 357/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.8717e-08 - val_loss: 1.2044e-08\n",
            "Epoch 358/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.0374e-07 - val_loss: 4.0355e-08\n",
            "Epoch 359/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.7252e-08 - val_loss: 3.3868e-08\n",
            "Epoch 360/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.3656e-08 - val_loss: 3.3820e-08\n",
            "Epoch 361/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 3.3554e-05 - val_loss: 4.7513e-05\n",
            "Epoch 362/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0096\n",
            "Epoch 363/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1697 - val_loss: 0.1408\n",
            "Epoch 364/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0460 - val_loss: 0.0121\n",
            "Epoch 365/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.3224e-04 - val_loss: 1.0346e-08\n",
            "Epoch 366/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.2660e-09 - val_loss: 9.8860e-10\n",
            "Epoch 367/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.0463e-10 - val_loss: 7.2936e-10\n",
            "Epoch 368/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.1523e-10 - val_loss: 4.7718e-10\n",
            "Epoch 369/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.3972e-09 - val_loss: 5.6201e-10\n",
            "Epoch 370/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 3.7606e-10 - val_loss: 1.8970e-10\n",
            "Epoch 371/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.9198e-10 - val_loss: 1.9902e-10\n",
            "Epoch 372/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0036e-09 - val_loss: 1.0399e-10\n",
            "Epoch 373/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.5748e-10 - val_loss: 1.7298e-09\n",
            "Epoch 374/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.3646e-10 - val_loss: 3.6025e-10\n",
            "Epoch 375/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4560e-10 - val_loss: 1.8286e-10\n",
            "Epoch 376/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.0974e-09 - val_loss: 6.0314e-09\n",
            "Epoch 377/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.6091e-09 - val_loss: 4.0543e-10\n",
            "Epoch 378/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.5349e-09 - val_loss: 1.7900e-08\n",
            "Epoch 379/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.5625e-09 - val_loss: 2.1691e-09\n",
            "Epoch 380/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.3736e-08 - val_loss: 5.6263e-08\n",
            "Epoch 381/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.8991e-07 - val_loss: 4.2865e-08\n",
            "Epoch 382/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.9237e-09 - val_loss: 6.3499e-09\n",
            "Epoch 383/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.5771e-09 - val_loss: 2.1628e-11\n",
            "Epoch 384/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.6612e-09 - val_loss: 6.4306e-07\n",
            "Epoch 385/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.5976e-05 - val_loss: 8.7643e-05\n",
            "Epoch 386/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0737 - val_loss: 0.0573\n",
            "Epoch 387/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.2088 - val_loss: 4.2483e-04\n",
            "Epoch 388/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.6535e-05 - val_loss: 3.8171e-09\n",
            "Epoch 389/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.2861e-10 - val_loss: 2.1628e-11\n",
            "Epoch 390/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.7334e-10 - val_loss: 4.6000e-10\n",
            "Epoch 391/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.8045e-10 - val_loss: 2.1510e-10\n",
            "Epoch 392/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.3201e-09 - val_loss: 3.9568e-10\n",
            "Epoch 393/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.2993e-09 - val_loss: 3.6509e-09\n",
            "Epoch 394/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 4.8698e-06 - val_loss: 9.1589e-06\n",
            "Epoch 395/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.1260e-05 - val_loss: 1.7379e-07\n",
            "Epoch 396/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.2618e-08 - val_loss: 1.6606e-10\n",
            "Epoch 397/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 2.2733e-08 - val_loss: 7.5473e-08\n",
            "Epoch 398/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 4.3503e-07 - val_loss: 9.8181e-06\n",
            "Epoch 399/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.0130 - val_loss: 0.0898\n",
            "Epoch 400/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.4719 - val_loss: 0.0042\n",
            "Epoch 401/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 1.1056e-04 - val_loss: 1.4096e-08\n",
            "Epoch 402/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 2.6111e-09 - val_loss: 2.2792e-11\n",
            "Epoch 403/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 3.1097e-10 - val_loss: 1.0210e-10\n",
            "Epoch 404/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.3496e-10 - val_loss: 2.1628e-11\n",
            "Epoch 405/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.9829e-10 - val_loss: 1.1090e-10\n",
            "Epoch 406/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.1225e-10 - val_loss: 3.6543e-11\n",
            "Epoch 407/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.5976e-10 - val_loss: 3.9568e-10\n",
            "Epoch 408/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.1510e-10 - val_loss: 1.4263e-10\n",
            "Epoch 409/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.7684e-10 - val_loss: 4.1155e-10\n",
            "Epoch 410/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.5602e-10 - val_loss: 2.1628e-11\n",
            "Epoch 411/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0790e-09 - val_loss: 3.9568e-10\n",
            "Epoch 412/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4539e-09 - val_loss: 1.2087e-10\n",
            "Epoch 413/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.5278e-07 - val_loss: 6.9403e-09\n",
            "Epoch 414/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0275e-08 - val_loss: 5.8748e-08\n",
            "Epoch 415/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.1695e-06 - val_loss: 1.6482e-04\n",
            "Epoch 416/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.6829e-06 - val_loss: 1.0313e-07\n",
            "Epoch 417/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.1536e-08 - val_loss: 4.9153e-09\n",
            "Epoch 418/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.7776e-10 - val_loss: 1.4263e-10\n",
            "Epoch 419/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.7958e-06 - val_loss: 9.2229e-06\n",
            "Epoch 420/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.1363 - val_loss: 1.0620\n",
            "Epoch 421/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0571 - val_loss: 6.7340e-04\n",
            "Epoch 422/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0104\n",
            "Epoch 423/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0479 - val_loss: 0.0275\n",
            "Epoch 424/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 1.4380e-04\n",
            "Epoch 425/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.1629e-05 - val_loss: 4.2551e-06\n",
            "Epoch 426/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.2513e-04 - val_loss: 7.5772e-04\n",
            "Epoch 427/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.7661e-04 - val_loss: 0.0215\n",
            "Epoch 428/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0358 - val_loss: 5.6495e-05\n",
            "Epoch 429/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 9.5361e-05\n",
            "Epoch 430/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.4427e-05 - val_loss: 9.2936e-07\n",
            "Epoch 431/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.3598e-05 - val_loss: 1.9850e-05\n",
            "Epoch 432/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.8241e-06 - val_loss: 2.2006e-08\n",
            "Epoch 433/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.2939e-07 - val_loss: 1.9045e-05\n",
            "Epoch 434/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.2648e-05 - val_loss: 5.0881e-06\n",
            "Epoch 435/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.0077 - val_loss: 1.2314e-04\n",
            "Epoch 436/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.0079\n",
            "Epoch 437/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.0249 - val_loss: 0.2077\n",
            "Epoch 438/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 0.0242 - val_loss: 2.8746e-04\n",
            "Epoch 439/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1771 - val_loss: 0.0189\n",
            "Epoch 440/500\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.0015 - val_loss: 4.4182e-06\n",
            "Epoch 441/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 5.1782e-07 - val_loss: 4.0543e-10\n",
            "Epoch 442/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 9.4672e-09 - val_loss: 1.9231e-09\n",
            "Epoch 443/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 6.4742e-10 - val_loss: 3.9568e-10\n",
            "Epoch 444/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 2.9648e-09 - val_loss: 6.2802e-09\n",
            "Epoch 445/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6422e-08 - val_loss: 1.9449e-09\n",
            "Epoch 446/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.7814e-07 - val_loss: 3.0538e-09\n",
            "Epoch 447/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.1652e-06 - val_loss: 2.2474e-05\n",
            "Epoch 448/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.9027e-06 - val_loss: 1.0294e-07\n",
            "Epoch 449/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 2.6012e-04\n",
            "Epoch 450/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 1.8663e-04\n",
            "Epoch 451/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.0105\n",
            "Epoch 452/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.3467 - val_loss: 6.9348e-04\n",
            "Epoch 453/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 7.7120e-05 - val_loss: 1.6753e-08\n",
            "Epoch 454/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.1141e-09 - val_loss: 1.0086e-10\n",
            "Epoch 455/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.1384e-10 - val_loss: 6.9364e-10\n",
            "Epoch 456/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 9.7162e-10 - val_loss: 2.1628e-11\n",
            "Epoch 457/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.7739e-09 - val_loss: 1.1333e-09\n",
            "Epoch 458/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.9591e-10 - val_loss: 1.3957e-10\n",
            "Epoch 459/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.9690e-10 - val_loss: 1.1463e-09\n",
            "Epoch 460/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.2254e-10 - val_loss: 1.4263e-10\n",
            "Epoch 461/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.6481e-10 - val_loss: 1.3957e-10\n",
            "Epoch 462/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.5290e-09 - val_loss: 2.1628e-11\n",
            "Epoch 463/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.5962e-09 - val_loss: 1.7122e-10\n",
            "Epoch 464/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.7661e-08 - val_loss: 5.9121e-08\n",
            "Epoch 465/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.8429e-04 - val_loss: 2.6017e-05\n",
            "Epoch 466/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.1457 - val_loss: 0.0014\n",
            "Epoch 467/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 2.7706e-06\n",
            "Epoch 468/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.7460e-04 - val_loss: 1.0272e-04\n",
            "Epoch 469/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.5384e-05 - val_loss: 4.2852e-07\n",
            "Epoch 470/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.9455e-08 - val_loss: 4.8812e-09\n",
            "Epoch 471/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.8991e-08 - val_loss: 2.0979e-09\n",
            "Epoch 472/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.9432e-08 - val_loss: 5.0083e-09\n",
            "Epoch 473/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.6987e-09 - val_loss: 2.2493e-09\n",
            "Epoch 474/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 6.9210e-08 - val_loss: 1.1037e-07\n",
            "Epoch 475/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.9296e-04 - val_loss: 3.0914e-05\n",
            "Epoch 476/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 4.3803e-04 - val_loss: 0.0030\n",
            "Epoch 477/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 0.1889 - val_loss: 1.0188e-04\n",
            "Epoch 478/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 3.3682e-04 - val_loss: 5.2283e-05\n",
            "Epoch 479/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 3.8663e-06 - val_loss: 6.0184e-09\n",
            "Epoch 480/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 2.0603e-07 - val_loss: 5.6873e-08\n",
            "Epoch 481/500\n",
            "150/150 [==============================] - 0s 3ms/step - loss: 3.7103e-07 - val_loss: 1.8362e-08\n",
            "Epoch 482/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 1.4504e-08 - val_loss: 5.9860e-08\n",
            "Epoch 483/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 3.6510e-08 - val_loss: 1.4263e-10\n",
            "Epoch 484/500\n",
            "150/150 [==============================] - 1s 3ms/step - loss: 3.0710e-10 - val_loss: 3.6543e-11\n",
            "Epoch 485/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 8.0093e-09 - val_loss: 4.3522e-09\n",
            "Epoch 486/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.6903e-05 - val_loss: 8.3049e-05\n",
            "Epoch 487/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 488/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.4060 - val_loss: 0.0114\n",
            "Epoch 489/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 2.5946e-07\n",
            "Epoch 490/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.9464e-07 - val_loss: 2.0230e-08\n",
            "Epoch 491/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.0196e-09 - val_loss: 5.1865e-10\n",
            "Epoch 492/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.0256e-09 - val_loss: 3.8091e-09\n",
            "Epoch 493/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 2.5232e-09 - val_loss: 4.2219e-11\n",
            "Epoch 494/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 1.8594e-10 - val_loss: 4.2219e-11\n",
            "Epoch 495/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.3829e-10 - val_loss: 2.7303e-11\n",
            "Epoch 496/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.4254e-10 - val_loss: 1.1425e-09\n",
            "Epoch 497/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 5.6132e-10 - val_loss: 4.0543e-10\n",
            "Epoch 498/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.0470e-09 - val_loss: 3.5150e-08\n",
            "Epoch 499/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 3.4199e-08 - val_loss: 5.9883e-10\n",
            "Epoch 500/500\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 4.7749e-10 - val_loss: 4.6366e-11\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.6957e-11\n",
            "Test Loss: 4.695721242398143e-11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGwCAYAAAC0HlECAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTyElEQVR4nO3de1xUdf4/8NcZLsN1AC8wknjLK4paqDRZlsmKRuaF/WlGiaa5Gng3L1te26KvrqblbatNarfysqtuaWqEiql4Q1G8kRqFJSPeYASRy8zn9wfMkRGzGRs4M/J6Ph7zWOacD2fe58juvPbz+ZzPkYQQAkRERER0TyqlCyAiIiJyBgxNRERERFZgaCIiIiKyAkMTERERkRUYmoiIiIiswNBEREREZAWGJiIiIiIruCpdwIPCZDLh4sWL8PX1hSRJSpdDREREVhBC4MaNGwgODoZKde++JIYmO7l48SJCQkKULoOIiIjuw4ULF9C4ceN7tmFoshNfX18AFRddo9EoXA0RERFZw2AwICQkRP4evxeGJjsxD8lpNBqGJiIiIidjzdQaTgQnIiIisgJDExEREZEVGJqIiIiIrMA5TURE5BBMJhNKS0uVLoMeMG5ubnBxcbHLsRiaiIhIcaWlpcjOzobJZFK6FHoA+fv7Q6vV/uF1FBmaiIhIUUII5ObmwsXFBSEhIb+7wCCRtYQQuHnzJvLy8gAAjRo1+kPHY2giIiJFlZeX4+bNmwgODoaXl5fS5dADxtPTEwCQl5eHwMDAPzRUxzhPRESKMhqNAAB3d3eFK6EHlTmMl5WV/aHjMDQREZFD4HM7qabY62+LoYmIiIjICgxNRERERFZgaCIiInIQzZo1w5IlS6xuv2vXLkiShPz8/BqriW5jaHJwN0vL8cv1m8i7cUvpUoiIqJIkSfd8zZ07976Oe+jQIYwePdrq9o8//jhyc3Ph5+d3X59nLYazClxywMEln7qECWsy8PjD9fHFq48pXQ4REQHIzc2Vf167di1mz56NrKwseZuPj4/8sxACRqMRrq6//5XbsGFDm+pwd3eHVqu16Xfo/rGnycGZZ/wLoXAhRES1RAiBm6XliryElf9jq9Vq5Zefnx8kSZLfnzlzBr6+vti6dSvCw8OhVquxZ88enD9/Hv3790dQUBB8fHzQtWtXfPfddxbHvXN4TpIkfPzxxxg4cCC8vLzQqlUrfPXVV/L+O3uAkpKS4O/vj+3bt6Ndu3bw8fFBnz59LEJeeXk5xo8fD39/f9SvXx/Tp09HXFwcBgwYcN//ZtevX8ewYcMQEBAALy8v9O3bF2fPnpX3//zzz+jXrx8CAgLg7e2N9u3b45tvvpF/NzY2Fg0bNoSnpydatWqF1atX33ctNYk9TQ5OVXmXpABTExHVDcVlRoTO3q7IZ5+aHwUvd/t8Nc6YMQN///vf0aJFCwQEBODChQt49tln8fbbb0OtVuOzzz5Dv379kJWVhSZNmvzmcebNm4cFCxZg4cKF+OCDDxAbG4uff/4Z9erVu2v7mzdv4u9//zv+9a9/QaVS4aWXXsLUqVPx+eefAwD+7//+D59//jlWr16Ndu3aYenSpdi0aRN69ux53+c6fPhwnD17Fl999RU0Gg2mT5+OZ599FqdOnYKbmxvi4+NRWlqK3bt3w9vbG6dOnZJ742bNmoVTp05h69ataNCgAc6dO4fi4uL7rqUmMTQ5OAkVqcnEzERE5FTmz5+PP/3pT/L7evXqoVOnTvL7t956Cxs3bsRXX32FhISE3zzO8OHDMXToUADAO++8g/fffx8HDx5Enz597tq+rKwMq1atwsMPPwwASEhIwPz58+X9H3zwAWbOnImBAwcCAJYtWyb3+twPc1jau3cvHn/8cQDA559/jpCQEGzatAn/7//9P+Tk5CAmJgZhYWEAgBYtWsi/n5OTg0ceeQRdunQBUNHb5qgYmhycvB4XQxMR1RGebi44NT9Ksc+2F3MIMCssLMTcuXOxZcsW5Obmory8HMXFxcjJybnncTp27Cj/7O3tDY1GIz9L7W68vLzkwARUPG/N3L6goACXLl1Ct27d5P0uLi4IDw+/74clnz59Gq6uroiIiJC31a9fH23atMHp06cBAOPHj8fYsWPx7bffIjIyEjExMfJ5jR07FjExMThy5Ah69+6NAQMGyOHL0XBOk4Pj8BwR1TWSJMHL3VWRlz1XJff29rZ4P3XqVGzcuBHvvPMOvv/+e2RkZCAsLAylpaX3PI6bm1u163OvgHO39tbO1aopo0aNwo8//oiXX34ZmZmZ6NKlCz744AMAQN++ffHzzz9j0qRJuHjxInr16oWpU6cqWu9vYWhyeByeIyJ6EOzduxfDhw/HwIEDERYWBq1Wi59++qlWa/Dz80NQUBAOHTokbzMajThy5Mh9H7Ndu3YoLy/HgQMH5G1Xr15FVlYWQkND5W0hISEYM2YMNmzYgClTpuCjjz6S9zVs2BBxcXH497//jSVLluDDDz+873pqEofnHJz5//Qo/f8SiIjoj2nVqhU2bNiAfv36QZIkzJo1676HxP6IcePGITExES1btkTbtm3xwQcf4Pr161b1smVmZsLX11d+L0kSOnXqhP79++PVV1/FP/7xD/j6+mLGjBl46KGH0L9/fwDAxIkT0bdvX7Ru3RrXr1/Hzp070a5dOwDA7NmzER4ejvbt26OkpASbN2+W9zkahiYHxylNREQPhsWLF+OVV17B448/jgYNGmD69OkwGAy1Xsf06dOh1+sxbNgwuLi4YPTo0YiKioKLy+/P5+rRo4fFexcXF5SXl2P16tWYMGECnnvuOZSWlqJHjx745ptv5KFCo9GI+Ph4/PLLL9BoNOjTpw/ee+89ABVrTc2cORM//fQTPD098eSTT2LNmjX2P3E7kAS7MOzCYDDAz88PBQUF0Gg0djvud6cuYdRnh9EpxB//i+9ut+MSETmKW7duITs7G82bN4eHh4fS5dQ5JpMJ7dq1w+DBg/HWW28pXU6NuNffmC3f3+xpcnC3755jtiUioj/u559/xrfffounnnoKJSUlWLZsGbKzs/Hiiy8qXZrD40RwByfPaVK2DCIiekCoVCokJSWha9eu6N69OzIzM/Hdd9857DwiR8KeJgdnnphnYk8TERHZQUhICPbu3at0GU5J0Z6mlStXomPHjtBoNNBoNNDpdNi6dau8/9atW4iPj0f9+vXh4+ODmJgYXLp0yeIYOTk5iI6OhpeXFwIDA/H666+jvLzcos2uXbvw6KOPQq1Wo2XLlkhKSqpWy/Lly9GsWTN4eHggIiICBw8erJFzthVH54iIiByDoqGpcePGePfdd5Geno7Dhw/jmWeeQf/+/XHy5EkAwKRJk/D1119j/fr1SE1NxcWLFzFo0CD5941GI6Kjo1FaWop9+/bh008/RVJSEmbPni23yc7ORnR0NHr27ImMjAxMnDgRo0aNwvbtt59rtHbtWkyePBlz5szBkSNH0KlTJ0RFRd1zxdXawgf2EhEROQjhYAICAsTHH38s8vPzhZubm1i/fr287/Tp0wKASEtLE0II8c033wiVSiX0er3cZuXKlUKj0YiSkhIhhBDTpk0T7du3t/iMIUOGiKioKPl9t27dRHx8vPzeaDSK4OBgkZiYaHXdBQUFAoAoKCiw7YR/x+4f8kTT6ZtFnyW77XpcIiJHUVxcLE6dOiWKi4uVLoUeUPf6G7Pl+9thJoIbjUasWbMGRUVF0Ol0SE9PR1lZGSIjI+U2bdu2RZMmTZCWlgYASEtLQ1hYGIKCguQ2UVFRMBgMcm9VWlqaxTHMbczHKC0tRXp6ukUblUqFyMhIuc3dlJSUwGAwWLxqgvmBvYJdTURERIpSPDRlZmbCx8cHarUaY8aMwcaNGxEaGgq9Xg93d3f4+/tbtA8KCoJerwcA6PV6i8Bk3m/ed682BoMBxcXFuHLlCoxG413bmI9xN4mJifDz85NfISEh93X+v+f2iuA1cngiIiKykuKhqU2bNsjIyMCBAwcwduxYxMXF4dSpU0qX9btmzpyJgoIC+XXhwoUa+RyJD+wlInpgPf3005g4caL8vlmzZliyZMk9f0eSJGzatOkPf7a9jlOXKB6a3N3d0bJlS4SHhyMxMRGdOnXC0qVLodVqUVpaivz8fIv2ly5dglarBQBotdpqd9OZ3/9eG41GA09PTzRo0AAuLi53bWM+xt2o1Wr5rj/zqyZIfGAvEZHD6devH/r06XPXfd9//z0kScLx48dtPu6hQ4cwevToP1qehblz56Jz587Vtufm5qJv3752/aw7JSUlVRsxcmaKh6Y7mUwmlJSUIDw8HG5ubkhJSZH3ZWVlIScnBzqdDgCg0+mQmZlpcZdbcnIyNBqN/GRlnU5ncQxzG/Mx3N3dER4ebtHGZDIhJSVFbqMkPrCXiMjxjBw5EsnJyfjll1+q7Vu9ejW6dOmCjh072nzchg0bwsvLyx4l/i6tVgu1Wl0rn/WgUDQ0zZw5E7t378ZPP/2EzMxMzJw5E7t27UJsbCz8/PwwcuRITJ48GTt37kR6ejpGjBgBnU6Hxx57DADQu3dvhIaG4uWXX8axY8ewfft2vPnmm4iPj5f/EMaMGYMff/wR06ZNw5kzZ7BixQqsW7cOkyZNkuuYPHkyPvroI3z66ac4ffo0xo4di6KiIowYMUKR61KVyrzkgMJ1EBHRbc899xwaNmxYbd2/wsJCrF+/HiNHjsTVq1cxdOhQPPTQQ/Dy8kJYWBi+/PLLex73zuG5s2fPokePHvDw8EBoaCiSk5Or/c706dPRunVreHl5oUWLFpg1axbKysoAVPT0zJs3D8eOHYMkSZAkSa75zuG5zMxMPPPMM/D09ET9+vUxevRoFBYWyvuHDx+OAQMG4O9//zsaNWqE+vXrIz4+Xv6s+5GTk4P+/fvDx8cHGo0GgwcPthj5OXbsGHr27AlfX19oNBqEh4fj8OHDACoeB9OvXz8EBATA29sb7du3xzfffHPftVhD0RXB8/LyMGzYMOTm5sLPzw8dO3bE9u3b8ac//QkA8N5770GlUiEmJgYlJSWIiorCihUr5N93cXHB5s2bMXbsWOh0Onh7eyMuLg7z58+X2zRv3hxbtmzBpEmTsHTpUjRu3Bgff/wxoqKi5DZDhgzB5cuXMXv2bOj1enTu3Bnbtm2rNjlcCZwITkR1jhBA2U1lPtvNq8pDP3+bq6srhg0bhqSkJLzxxhvymnrr16+H0WjE0KFDUVhYiPDwcEyfPh0ajQZbtmzByy+/jIcffhjdunX73c8wmUwYNGgQgoKCcODAARQUFFjMfzLz9fVFUlISgoODkZmZiVdffRW+vr6YNm0ahgwZghMnTmDbtm347rvvAAB+fn7VjlFUVISoqCjodDocOnQIeXl5GDVqFBISEiyC4c6dO9GoUSPs3LkT586dw5AhQ9C5c2e8+uqrv3s+dzs/c2BKTU1FeXk54uPjMWTIEOzatQsAEBsbi0ceeQQrV66Ei4sLMjIy4ObmBgCIj49HaWkpdu/eDW9vb5w6dQo+Pj4212ELRUPTP//5z3vu9/DwwPLly7F8+fLfbNO0adPfTZZPP/00jh49es82CQkJSEhIuGcbJdxeEZypiYjqiLKbwDvBynz2Xy8C7t5WNX3llVewcOFCpKam4umnnwZQMTQXExMj31k9depUuf24ceOwfft2rFu3zqrQ9N133+HMmTPYvn07goMrrsc777xTbR7Sm2++Kf/crFkzTJ06FWvWrMG0adPg6ekJHx8fuLq63nOe7hdffIFbt27hs88+g7d3xfkvW7YM/fr1w//93//JnQgBAQFYtmwZXFxc0LZtW0RHRyMlJeW+QlNKSgoyMzORnZ0t34H+2WefoX379jh06BC6du2KnJwcvP7662jbti0AoFWrVvLv5+TkICYmBmFhYQCAFi1a2FyDrRxuThNZkjg8R0TkkNq2bYvHH38cn3zyCQDg3Llz+P777zFy5EgAFesPvvXWWwgLC0O9evXg4+OD7du3Iycnx6rjnz59GiEhIXJgAnDXubZr165F9+7dodVq4ePjgzfffNPqz6j6WZ06dZIDEwB0794dJpMJWVlZ8rb27dvDxcVFft+oUaP7fnqG+fyqLtkTGhoKf39/nD59GkDF9JlRo0YhMjIS7777Ls6fPy+3HT9+PP72t7+he/fumDNnzn1NvLcVH9jr4My9xHxgLxHVGW5eFT0+Sn22DUaOHIlx48Zh+fLlWL16NR5++GE89dRTAICFCxdi6dKlWLJkCcLCwuDt7Y2JEyeitLTUbuWmpaUhNjYW8+bNQ1RUFPz8/LBmzRosWrTIbp9RlXlozEySJJhMphr5LKDizr8XX3wRW7ZswdatWzFnzhysWbMGAwcOxKhRoxAVFYUtW7bg22+/RWJiIhYtWoRx48bVWD3saXJwfGAvEdU5klQxRKbEy4r5TFUNHjwYKpUKX3zxBT777DO88sor8gjB3r170b9/f7z00kvo1KkTWrRogR9++MHqY7dr1w4XLlxAbm6uvG3//v0Wbfbt24emTZvijTfeQJcuXdCqVSv8/PPPFm3c3d1hNBp/97OOHTuGoqIiedvevXuhUqnQpk0bq2u2hfn8qq5zeOrUKeTn58t3wANA69atMWnSJHz77bcYNGgQVq9eLe8LCQnBmDFjsGHDBkyZMgUfffRRjdRqxtDk4PjAXiIix+Xj44MhQ4Zg5syZyM3NxfDhw+V9rVq1QnJyMvbt24fTp0/jL3/5S7U1Ae8lMjISrVu3RlxcHI4dO4bvv/8eb7zxhkWbVq1aIScnB2vWrMH58+fx/vvvY+PGjRZtmjVrhuzsbGRkZODKlSsoKSmp9lmxsbHw8PBAXFwcTpw4gZ07d2LcuHF4+eWX//BNUUajERkZGRav06dPIzIyEmFhYYiNjcWRI0dw8OBBDBs2DE899RS6dOmC4uJiJCQkYNeuXfj555+xd+9eHDp0CO3atQMATJw4Edu3b0d2djaOHDmCnTt3yvtqCkOTg1NxnSYiIoc2cuRIXL9+HVFRURbzj9588008+uijiIqKwtNPPw2tVosBAwZYfVyVSoWNGzeiuLgY3bp1w6hRo/D2229btHn++ecxadIkJCQkoHPnzti3bx9mzZpl0SYmJgZ9+vRBz5490bBhw7sue+Dl5YXt27fj2rVr6Nq1K/785z+jV69eWLZsmW0X4y4KCwvxyCOPWLz69esHSZLwv//9DwEBAejRowciIyPRokULrF27FkDFHfJXr17FsGHD0Lp1awwePBh9+/bFvHnzAFSEsfj4eLRr1w59+vRB69atLe6wrwmS4LexXRgMBvj5+aGgoMCuq4Nn/lKAfsv2oJGfB9Jm9rLbcYmIHMWtW7eQnZ2N5s2bw8PDQ+ly6AF0r78xW76/2dPk4LhOExERkWNgaHJwvHuOiIjIMTA0OTjzA3sZmYiIiJTF0OTgODxHRETkGBiaHJz8wF6mJiJ6wPF/56im2Otvi6HJwck9TcqWQURUY8yP5bDnStlEVd28WfEA6DtXNLcVH6Pi4PjAXiJ60Lm6usLLywuXL1+Gm5sbVCr+/3myDyEEbt68iby8PPj7+1s8N+9+MDQ5OPOK4CZmJiJ6QEmShEaNGiE7O7vaI0CI7MHf3x9arfYPH4ehycFJXBGciOoAd3d3tGrVikN0ZHdubm5/uIfJjKHJwcnDc4pWQURU81QqFVcEJ4fGgWMHp+JMcCIiIofA0OTguCI4ERGRY2BocnBcEZyIiMgxMDQ5OK4ITkRE5BgYmhwch+eIiIgcA0OTgzOv08TIREREpCyGJgdnXnKAqYmIiEhZDE0OTiWvCM7UREREpCSGJgfHZZqIiIgcA0OTg+MDe4mIiBwDQ5OD4wN7iYiIHANDk4OTpN9vQ0RERDWPocnBVc1MHKIjIiJSDkOTg1NV6WriEB0REZFyGJocXNXhOfY0ERERKYehycFJVQboGJmIiIiUw9Dk4KQq/0Jc4JKIiEg5DE0OznIiuGJlEBER1XkMTQ5O4poDREREDoGhycGpqmQmDs8REREph6HJwVlMBGdmIiIiUgxDk4OzWHJAuTKIiIjqPIYmJ8LhOSIiIuUwNDm4qiuCMzMREREph6HJwVncPMfQREREpBiGJgdXNTNxeI6IiEg5DE0OzmJ4TsE6iIiI6jpFQ1NiYiK6du0KX19fBAYGYsCAAcjKyrJo8/TTT0OSJIvXmDFjLNrk5OQgOjoaXl5eCAwMxOuvv47y8nKLNrt27cKjjz4KtVqNli1bIikpqVo9y5cvR7NmzeDh4YGIiAgcPHjQ7udsKz6wl4iIyDEoGppSU1MRHx+P/fv3Izk5GWVlZejduzeKioos2r366qvIzc2VXwsWLJD3GY1GREdHo7S0FPv27cOnn36KpKQkzJ49W26TnZ2N6Oho9OzZExkZGZg4cSJGjRqF7du3y23Wrl2LyZMnY86cOThy5Ag6deqEqKgo5OXl1fyFuAeJPU1EREQOQRIO1H1x+fJlBAYGIjU1FT169ABQ0dPUuXNnLFmy5K6/s3XrVjz33HO4ePEigoKCAACrVq3C9OnTcfnyZbi7u2P69OnYsmULTpw4If/eCy+8gPz8fGzbtg0AEBERga5du2LZsmUAAJPJhJCQEIwbNw4zZsyo9rklJSUoKSmR3xsMBoSEhKCgoAAajcYu18Os+cwtEAI4+EYvBPp62PXYREREdZnBYICfn59V398ONaepoKAAAFCvXj2L7Z9//jkaNGiADh06YObMmbh586a8Ly0tDWFhYXJgAoCoqCgYDAacPHlSbhMZGWlxzKioKKSlpQEASktLkZ6ebtFGpVIhMjJSbnOnxMRE+Pn5ya+QkJA/cOb3Jvc1OUy8JSIiqntclS7AzGQyYeLEiejevTs6dOggb3/xxRfRtGlTBAcH4/jx45g+fTqysrKwYcMGAIBer7cITADk93q9/p5tDAYDiouLcf36dRiNxru2OXPmzF3rnTlzJiZPniy/N/c01QRJkgAhmJmIiIgU5DChKT4+HidOnMCePXssto8ePVr+OSwsDI0aNUKvXr1w/vx5PPzww7VdpkytVkOtVtfKZ6kkwAguOUBERKQkhxieS0hIwObNm7Fz5040btz4nm0jIiIAAOfOnQMAaLVaXLp0yaKN+b1Wq71nG41GA09PTzRo0AAuLi53bWM+hpLMD+1lZiIiIlKOoqFJCIGEhARs3LgRO3bsQPPmzX/3dzIyMgAAjRo1AgDodDpkZmZa3OWWnJwMjUaD0NBQuU1KSorFcZKTk6HT6QAA7u7uCA8Pt2hjMpmQkpIit1FU5aQmZiYiIiLlKDo8Fx8fjy+++AL/+9//4OvrK89B8vPzg6enJ86fP48vvvgCzz77LOrXr4/jx49j0qRJ6NGjBzp27AgA6N27N0JDQ/Hyyy9jwYIF0Ov1ePPNNxEfHy8Pn40ZMwbLli3DtGnT8Morr2DHjh1Yt24dtmzZItcyefJkxMXFoUuXLujWrRuWLFmCoqIijBgxovYvzB1UlaHJZGJsIiIiUoxQECo6T6q9Vq9eLYQQIicnR/To0UPUq1dPqNVq0bJlS/H666+LgoICi+P89NNPom/fvsLT01M0aNBATJkyRZSVlVm02blzp+jcubNwd3cXLVq0kD+jqg8++EA0adJEuLu7i27duon9+/dbfS4FBQUCQLXa7KHtm1tF0+mbRc7VIrsfm4iIqC6z5fvbodZpcma2rPNgq9DZ23Cz1Ijdr/dEk/pedj02ERFRXea06zTR3ZmfP8e754iIiJTD0OQEzItbMjIREREph6HJGZjvnmNPExERkWIYmpyAuaeJN88REREph6HJCahUHKAjIiJSGkOTE5AjEzMTERGRYhianIAk3z2ncCFERER1GEOTE1DJj1FhaiIiIlIKQ5NT4AN7iYiIlMbQ5AQk87PnmJqIiIgUw9DkBOThOWYmIiIixTA0OQFJvn+OiIiIlMLQ5AQ4PEdERKQ8hiYnYH5gLzMTERGRchianAgzExERkXIYmpyAxAf2EhERKY6hyQmouCI4ERGR4lyVLoB+R3kJ/GCABiXgAB0REZFyGJoc3amv8PXNUdjr1h5CRCpdDRERUZ3F4TlHVzk0J0FweI6IiEhBDE2OTqr4J1JJghPBiYiIFMTQ5OgqQ5MEwRlNRERECmJocnSVw3MqmLgiOBERkYJsCk3l5eWYP38+fvnll5qqh+4k9zSBN88REREpyKbQ5OrqioULF6K8vLym6qE7mec0wcTMREREpCCbh+eeeeYZpKam1kQtdFfm4TnB4TkiIiIF2bxOU9++fTFjxgxkZmYiPDwc3t7eFvuff/55uxVHkHuaAMEH9hIRESnI5tD02muvAQAWL15cbZ8kSTAajX+8KrpNHp7j3XNERERKsjk0mUymmqiDfkuV0MThOSIiIuVwyQFHVzGlCSoI3j1HRESkoPsKTampqejXrx9atmyJli1b4vnnn8f3339v79oIuGNxS6YmIiIipdgcmv79738jMjISXl5eGD9+PMaPHw9PT0/06tULX3zxRU3UWLdVCU0cGSUiIlKOzXOa3n77bSxYsACTJk2St40fPx6LFy/GW2+9hRdffNGuBdZ5XKeJiIjIIdjc0/Tjjz+iX79+1bY///zzyM7OtktRVNXtdZr4wF4iIiLl2ByaQkJCkJKSUm37d999h5CQELsURVVUHZ5jZiIiIlKMzcNzU6ZMwfjx45GRkYHHH38cALB3714kJSVh6dKldi+wzqsSmnj7HBERkXJsDk1jx46FVqvFokWLsG7dOgBAu3btsHbtWvTv39/uBdZ5UtXhOYVrISIiqsNsCk3l5eV455138Morr2DPnj01VRNVxeE5IiIih2DTnCZXV1csWLAA5eXlNVUP3cniMSpMTUREREqxeSJ4r169kJqaWhO10N1UDs9JEofniIiIlGTznKa+fftixowZyMzMRHh4OLy9vS32P//883YrjoDbSw6Y+Ow5IiIiBdkcml577TUAwOLFi6vtkyQJRqPxj1dFt8lzmoiIiEhJNg/PmUym33zZGpgSExPRtWtX+Pr6IjAwEAMGDEBWVpZFm1u3biE+Ph7169eHj48PYmJicOnSJYs2OTk5iI6OhpeXFwIDA/H6669Xm3e1a9cuPProo1Cr1WjZsiWSkpKq1bN8+XI0a9YMHh4eiIiIwMGDB206nxpRdUVwdjQREREpxqbQVFZWBldXV5w4ccIuH56amor4+Hjs378fycnJKCsrQ+/evVFUVCS3mTRpEr7++musX78eqampuHjxIgYNGiTvNxqNiI6ORmlpKfbt24dPP/0USUlJmD17ttwmOzsb0dHR6NmzJzIyMjBx4kSMGjUK27dvl9usXbsWkydPxpw5c3DkyBF06tQJUVFRyMvLs8u53jdOBCciInIMwkbNmzcXGRkZtv6aVfLy8gQAkZqaKoQQIj8/X7i5uYn169fLbU6fPi0AiLS0NCGEEN98841QqVRCr9fLbVauXCk0Go0oKSkRQggxbdo00b59e4vPGjJkiIiKipLfd+vWTcTHx8vvjUajCA4OFomJiVbVXlBQIACIgoICG8/6d+hPCDFHIy7Pbiz+c/iCfY9NRERUx9ny/W3z8Nwbb7yBv/71r7h27Zq98xsKCgoAAPXq1QMApKeno6ysDJGRkXKbtm3bokmTJkhLSwMApKWlISwsDEFBQXKbqKgoGAwGnDx5Um5T9RjmNuZjlJaWIj093aKNSqVCZGSk3OZOJSUlMBgMFq8aUWWdJvYzERERKcfmieDLli3DuXPnEBwcjKZNm1a7e+7IkSP3VYjJZMLEiRPRvXt3dOjQAQCg1+vh7u4Of39/i7ZBQUHQ6/Vym6qBybzfvO9ebQwGA4qLi3H9+nUYjca7tjlz5sxd601MTMS8efPu61xtUnV4jpOaiIiIFGNzaBowYEANlAHEx8fjxIkTTrPS+MyZMzF58mT5vcFgqKEHFt9ecoCZiYiISDk2h6Y5c+bYvYiEhARs3rwZu3fvRuPGjeXtWq0WpaWlyM/Pt+htunTpErRardzmzrvczHfXVW1z5x13ly5dgkajgaenJ1xcXODi4nLXNuZj3EmtVkOtVt/fCduiypIDHKAjIiJSjtVzmg4ePHjPJQVKSkrkB/haSwiBhIQEbNy4ETt27EDz5s0t9oeHh8PNzQ0pKSnytqysLOTk5ECn0wEAdDodMjMzLe5yS05OhkajQWhoqNym6jHMbczHcHd3R3h4uEUbk8mElJQUuY1izCuC84G9REREyrJ2drlKpRKXLl2S3/v6+orz58/L7/V6vVCpVDbNWB87dqzw8/MTu3btErm5ufLr5s2bcpsxY8aIJk2aiB07dojDhw8LnU4ndDqdvL+8vFx06NBB9O7dW2RkZIht27aJhg0bipkzZ8ptfvzxR+Hl5SVef/11cfr0abF8+XLh4uIitm3bJrdZs2aNUKvVIikpSZw6dUqMHj1a+Pv7W9yVdy81dvfc1R+FmKMRhbMbis/3/2zfYxMREdVxtnx/Wz08J+7o5rjz/W9tu5eVK1cCAJ5++mmL7atXr8bw4cMBAO+99x5UKhViYmJQUlKCqKgorFixQm7r4uKCzZs3Y+zYsdDpdPD29kZcXBzmz58vt2nevDm2bNmCSZMmYenSpWjcuDE+/vhjREVFyW2GDBmCy5cvY/bs2dDr9ejcuTO2bdtWbXJ4rZPMc5q4ThMREZGSJGFl0lGpVNDr9QgMDAQA+Pr64tixY2jRogWAivk/wcHBdfYxKgaDAX5+figoKIBGo7HfgfNzgCVhuCXc8J++6Xjpsab2OzYREVEdZ8v3t83rNFEtq7pOEyc1ERERKcamu+dOnTolr30khMCZM2dQWFgIALhy5Yr9q6M7HqNCRERESrEpNPXq1cuit+O5554DAEiSBCEEpMr5N2RPXKeJiIjIEVgdmrKzs2uyDvotVdZpMjE1ERERKcbq0NS0KScgK8I8PCcJCBNDExERkVI4EdzRVRny5KwmIiIi5TA0OTqpyj+RqW4u50BEROQIGJocXdWeJs5pIiIiUgxDk6Or2tMEk2JlEBER1XUMTQ6vSk+TiaGJiIhIKVbdPffII49YvQbTkSNH/lBBdIeqPU0cniMiIlKMVaFpwIAB8s+3bt3CihUrEBoaCp1OBwDYv38/Tp48iddee61GiqzTqoQmIdjTREREpBSrQtOcOXPkn0eNGoXx48fjrbfeqtbmwoUL9q2O7uhpYmgiIiJSis1zmtavX49hw4ZV2/7SSy/hv//9r12KoiqqDosyNBERESnG5tDk6emJvXv3Vtu+d+9eeHh42KUoqsJinSbOaSIiIlKKTQ/sBYCJEydi7NixOHLkCLp16wYAOHDgAD755BPMmjXL7gXWeVxygIiIyCHYHJpmzJiBFi1aYOnSpfj3v/8NAGjXrh1Wr16NwYMH271Aqrq4JUMTERGRUmwOTQAwePBgBqTaUnVOE9dpIiIiUsx9LW6Zn5+Pjz/+GH/9619x7do1ABXrM/366692LY4ASBJMlb1NfGAvERGRcmzuaTp+/DgiIyPh5+eHn376CaNGjUK9evWwYcMG5OTk4LPPPquJOus0ARUAI++eIyIiUpDNPU2TJ0/G8OHDcfbsWYu75Z599lns3r3brsXRHTg8R0REpBibQ9OhQ4fwl7/8pdr2hx56CHq93i5FkSUh30HH4TkiIiKl2Bya1Go1DAZDte0//PADGjZsaJeiyJIwz2ni8BwREZFibA5Nzz//PObPn4+ysjIAgCRJyMnJwfTp0xETE2P3AqlKTxMXtyQiIlKMzaFp0aJFKCwsRGBgIIqLi/HUU0+hZcuW8PX1xdtvv10TNZK8VpNR0SqIiIjqMpvvnvPz80NycjL27t2LY8eOobCwEI8++igiIyNroj7C7eE5CPY0ERERKcWm0FRWVgZPT09kZGSge/fu6N69e03VRVXIw3Oc00RERKQYm4bn3Nzc0KRJExiNHCaqTcL8z8QlB4iIiBRj85ymN954w2IlcKoF8ugch+eIiIiUYvOcpmXLluHcuXMIDg5G06ZN4e3tbbH/yJEjdiuOKsg9TWBPExERkVJsDk0DBgyogTLoXoTEieBERERKszk0zZkzpybqoHuqDE0mziUjIiJSis1zmqj23R6eY08TERGRUmzuaTIajXjvvfewbt065OTkoLS01GI/J4jb3+0lBxiaiIiIlGJzT9O8efOwePFiDBkyBAUFBZg8eTIGDRoElUqFuXPn1kCJdHtxS04EJyIiUorNoenzzz/HRx99hClTpsDV1RVDhw7Fxx9/jNmzZ2P//v01USOZJ4Lz7jkiIiLF2Bya9Ho9wsLCAAA+Pj4oKCgAADz33HPYsmWLfasjAFXmNHF4joiISDE2h6bGjRsjNzcXAPDwww/j22+/BQAcOnQIarXavtVRBXNPE1cEJyIiUozNoWngwIFISUkBAIwbNw6zZs1Cq1atMGzYMLzyyit2L5Cq9jQxNBERESnF5rvn3n33XfnnIUOGoEmTJkhLS0OrVq3Qr18/uxZHlTiniYiISHE2h6Y76XQ66HQ6e9RCv+H23XOc00RERKQUm0PTZ599ds/9w4YNu+9i6O64ThMREZHybJ7TNGHCBIvXa6+9huHDh2P06NGYOHGiTcfavXs3+vXrh+DgYEiShE2bNlnsHz58OCRJsnj16dPHos21a9cQGxsLjUYDf39/jBw5EoWFhRZtjh8/jieffBIeHh4ICQnBggULqtWyfv16tG3bFh4eHggLC8M333xj07nULA7PERERKc3m0HT9+nWLV2FhIbKysvDEE0/gyy+/tOlYRUVF6NSpE5YvX/6bbfr06YPc3Fz5dednxMbG4uTJk0hOTsbmzZuxe/dujB49Wt5vMBjQu3dvNG3aFOnp6Vi4cCHmzp2LDz/8UG6zb98+DB06FCNHjsTRo0cxYMAADBgwACdOnLDpfGpMZU+TxJ4mIiIi5Qg7OXTokGjTps19/z4AsXHjRottcXFxon///r/5O6dOnRIAxKFDh+RtW7duFZIkiV9//VUIIcSKFStEQECAKCkpkdtMnz7dotbBgweL6Ohoi2NHRESIv/zlL1bXX1BQIACIgoICq3/HWlcWhgsxRyOWf/yh3Y9NRERUl9ny/W23B/a6urri4sWL9jqcbNeuXQgMDESbNm0wduxYXL16Vd6XlpYGf39/dOnSRd4WGRkJlUqFAwcOyG169OgBd3d3uU1UVBSysrJw/fp1uU1kZKTF50ZFRSEtLe036yopKYHBYLB41RQuOUBERKQ8myeCf/XVVxbvhRDIzc3FsmXL0L17d7sVBlQMzQ0aNAjNmzfH+fPn8de//hV9+/ZFWloaXFxcoNfrERgYaPE7rq6uqFevHvR6PYCKFcybN29u0SYoKEjeFxAQAL1eL2+r2sZ8jLtJTEzEvHnz7HGav0/is+eIiIiUZnNoGjBggMV7SZLQsGFDPPPMM1i0aJG96gIAvPDCC/LPYWFh6NixIx5++GHs2rULvXr1sutn2WrmzJmYPHmy/N5gMCAkJKRmPox3zxERESnO5tBkUvBRHi1atECDBg1w7tw59OrVC1qtFnl5eRZtysvLce3aNWi1WgCAVqvFpUuXLNqY3/9eG/P+u1Gr1bX42JiKnibBx6gQEREpxm5zmmrDL7/8gqtXr6JRo0YAKhbWzM/PR3p6utxmx44dMJlMiIiIkNvs3r0bZWVlcpvk5GS0adMGAQEBchvzo2GqtnGYRTslzmkiIiJSms09TVWHpH7P4sWL77m/sLAQ586dk99nZ2cjIyMD9erVQ7169TBv3jzExMRAq9Xi/PnzmDZtGlq2bImoqCgAQLt27dCnTx+8+uqrWLVqFcrKypCQkIAXXngBwcHBAIAXX3wR8+bNw8iRIzF9+nScOHECS5cuxXvvvSd/7oQJE/DUU09h0aJFiI6Oxpo1a3D48GGLZQkUVTmnSTA0ERERKcbm0HT06FEcPXoUZWVlaNOmDQDghx9+gIuLCx599FG5nSQ/L+23HT58GD179pTfmwNZXFwcVq5ciePHj+PTTz9Ffn4+goOD0bt3b7z11lsWw2Kff/45EhIS0KtXL6hUKsTExOD999+X9/v5+eHbb79FfHw8wsPD0aBBA8yePdtiLafHH38cX3zxBd5880389a9/RatWrbBp0yZ06NDB1stTMziniYiISHGSELZ9Ey9evBi7du3Cp59+Kg9vXb9+HSNGjMCTTz6JKVOm1Eihjs5gMMDPzw8FBQXQaDR2Pfbl93ui4bUj+KDBbIxLqJvXl4iIqCbY8v1t85ymRYsWITExUQ5MABAQEIC//e1vdr97jipI4JIDRERESrM5NBkMBly+fLna9suXL+PGjRt2KYruUDk8xzlNREREyrE5NA0cOBAjRozAhg0b8Msvv+CXX37Bf//7X4wcORKDBg2qiRrrPMHQREREpDibJ4KvWrUKU6dOxYsvvijfxu/q6oqRI0di4cKFdi+QAIkTwYmIiBRnc2jy8vLCihUrsHDhQpw/fx4A8PDDD8Pb29vuxVEl852IXNySiIhIMfe9uKW3tzc6duwIPz8//Pzzz4quFP7A4+KWREREirM6NH3yySfVFqscPXo0WrRogbCwMHTo0AEXLlywe4GE24tbgsNzRERESrE6NH344YcWywxs27YNq1evxmeffYZDhw7B398f8+bNq5Ei67zKniaJPU1ERESKsXpO09mzZ9GlSxf5/f/+9z/0798fsbGxAIB33nkHI0aMsH+FdHtOE0MTERGRYqzuaSouLrZYKXPfvn3o0aOH/L5FixbQ6/X2rY4A3L57zsbF24mIiMiOrA5NTZs2RXp6OgDgypUrOHnyJLp37y7v1+v18PPzs3+FxIngREREDsDq4bm4uDjEx8fj5MmT2LFjB9q2bYvw8HB5/759+xznAbcPmsrhOc5pIiIiUo7VoWnatGm4efMmNmzYAK1Wi/Xr11vs37t3L4YOHWr3AglVHqPC4TkiIiKlWB2aVCoV5s+fj/nz5991/50hiuxH4t1zREREirvvxS2pFvHZc0RERIpjaHICknlOExiaiIiIlMLQ5AxUlT1NJs5pIiIiUgpDkxOQJJeK/+RjVIiIiBTD0OQMuE4TERGR4qy+e87MaDQiKSkJKSkpyMvLg8lk+UW+Y8cOuxVHFSQ+RoWIiEhxNoemCRMmICkpCdHR0ejQocPtL3SqMeYlB8DhOSIiIsXYHJrWrFmDdevW4dlnn62JeuhuODxHRESkOJvnNLm7u6Nly5Y1UQv9BkllfowKe5qIiIiUYnNomjJlCpYuXcpHetQmLm5JRESkOJuH5/bs2YOdO3di69ataN++Pdzc3Cz2b9iwwW7FUQX5MSqc00RERKQYm0OTv78/Bg4cWBO10G9R8dlzRERESrM5NK1evbom6qB74N1zREREyuPilk6A6zQREREpz+aeJgD4z3/+g3Xr1iEnJwelpaUW+44cOWKXwug2SXX7MSpCCK6NRUREpACbe5ref/99jBgxAkFBQTh69Ci6deuG+vXr48cff0Tfvn1rosY6zzw8p4IAn9lLRESkDJtD04oVK/Dhhx/igw8+gLu7O6ZNm4bk5GSMHz8eBQUFNVFjnWfuWaoITUxNRERESrA5NOXk5ODxxx8HAHh6euLGjRsAgJdffhlffvmlfasjAICkuj0R3MiuJiIiIkXYHJq0Wi2uXbsGAGjSpAn2798PAMjOzuaClzVFqpjTpIIALzEREZEybA5NzzzzDL766isAwIgRIzBp0iT86U9/wpAhQ7h+Uw1RqczDcyYYmZqIiIgUYfPdcx9++CFMpopb3+Pj41G/fn3s27cPzz//PP7yl7/YvUCC/BgVzmkiIiJSjs2hSaVSQaW63UH1wgsv4IUXXrBrUWRJVeUxKibOaSIiIlLEfS1u+f333+Oll16CTqfDr7/+CgD417/+hT179ti1OKpgnggucSI4ERGRYmwOTf/9738RFRUFT09PHD16FCUlJQCAgoICvPPOO3YvkLhOExERkSOwOTT97W9/w6pVq/DRRx/Bzc1N3t69e3euBl5Tqg7PcU4TERGRImwOTVlZWejRo0e17X5+fsjPz7dHTXQnTgQnIiJS3H2t03Tu3Llq2/fs2YMWLVrYpSi6g9zTZOKcJiIiIoXYHJpeffVVTJgwAQcOHIAkSbh48SI+//xzTJ06FWPHjq2JGqny+bxc3JKIiEg5Ni85MGPGDJhMJvTq1Qs3b95Ejx49oFarMXXqVIwbN64maiS5pwnsaSIiIlKIzT1NkiThjTfewLVr13DixAns378fly9fxltvvWXzh+/evRv9+vVDcHAwJEnCpk2bLPYLITB79mw0atQInp6eiIyMxNmzZy3aXLt2DbGxsdBoNPD398fIkSNRWFho0eb48eN48skn4eHhgZCQECxYsKBaLevXr0fbtm3h4eGBsLAwfPPNNzafT42R5zSZOKeJiIhIIfe1ThMAuLu7IzQ0FN26dYOPj899HaOoqAidOnXC8uXL77p/wYIFeP/997Fq1SocOHAA3t7eiIqKwq1bt+Q2sbGxOHnyJJKTk7F582bs3r0bo0ePlvcbDAb07t0bTZs2RXp6OhYuXIi5c+fiww8/lNvs27cPQ4cOxciRI3H06FEMGDAAAwYMwIkTJ+7rvOyv8jEqEieCExERKUUSVj5l95VXXrHqgJ988sn9FSJJ2LhxIwYMGACgopcpODgYU6ZMwdSpUwFUrAUVFBSEpKQkvPDCCzh9+jRCQ0Nx6NAhdOnSBQCwbds2PPvss/jll18QHByMlStX4o033oBer4e7uzuAiiHGTZs24cyZMwCAIUOGoKioCJs3b5breeyxx9C5c2esWrXqrvWWlJTIa1QBFeEsJCQEBQUF0Gg093UNftPe94HkWdhgfAIdEtagdZCvfY9PRERURxkMBvj5+Vn1/W11T1NSUhJ27tyJ/Px8XL9+/Tdf9pKdnQ29Xo/IyEh5m5+fHyIiIpCWlgYASEtLg7+/vxyYACAyMhIqlQoHDhyQ2/To0UMOTAAQFRWFrKwsud60tDSLzzG3MX/O3SQmJsLPz09+hYSE/PGT/i1Vhuc4p4mIiEgZVk8EHzt2LL788ktkZ2djxIgReOmll1CvXr0aK0yv1wMAgoKCLLYHBQXJ+/R6PQIDAy32u7q6ol69ehZtmjdvXu0Y5n0BAQHQ6/X3/Jy7mTlzJiZPniy/N/c01QiVCwDAhXOaiIiIFGN1T9Py5cuRm5uLadOm4euvv0ZISAgGDx6M7du3w8oRvgeKWq2GRqOxeNUYVUW2VcEEk6nmPoaIiIh+m00TwdVqNYYOHYrk5GScOnUK7du3x2uvvYZmzZpVu2Ptj9JqtQCAS5cuWWy/dOmSvE+r1SIvL89if3l5Oa5du2bR5m7HqPoZv9XGvF9xlT1NruxpIiIiUsx93z2nUqkgSRKEEDAajfasCQDQvHlzaLVapKSkyNsMBgMOHDgAnU4HANDpdMjPz0d6errcZseOHTCZTIiIiJDb7N69G2VlZXKb5ORktGnTBgEBAXKbqp9jbmP+HMVV9jS5wAgjQxMREZEibApNJSUl+PLLL/GnP/0JrVu3RmZmJpYtW4acnJz7WnagsLAQGRkZyMjIAFAx+TsjIwM5OTmQJAkTJ07E3/72N3z11VfIzMzEsGHDEBwcLN9h165dO/Tp0wevvvoqDh48iL179yIhIQEvvPACgoODAQAvvvgi3N3dMXLkSJw8eRJr167F0qVLLeYjTZgwAdu2bcOiRYtw5swZzJ07F4cPH0ZCQoLN51QjpNs9TXVxKJSIiMghCCuNHTtWBAQEiI4dO4olS5aIy5cvW/urv2nnzp0CQLVXXFycEEIIk8kkZs2aJYKCgoRarRa9evUSWVlZFse4evWqGDp0qPDx8REajUaMGDFC3Lhxw6LNsWPHxBNPPCHUarV46KGHxLvvvlutlnXr1onWrVsLd3d30b59e7FlyxabzqWgoEAAEAUFBbZdBGtkrBFijkakvtldHPjxqv2PT0REVEfZ8v1t9TpNKpUKTZo0wSOPPAJJkn6z3YYNG/54knNCtqzzYLPM/wD/HYl9xlBIwzdD93B9+x6fiIiojrLl+9vqJQeGDRt2z7BENcg8p0kycU4TERGRQqwOTUlJSTVYBt2TfPecEaUMTURERIq477vnqBbJd8+ZwAXBiYiIlMHQ5AyqLDlgYmoiIiJSBEOTM+DilkRERIpjaHIGkvnZc0Y+sJeIiEghDE3OgHOaiIiIFMfQ5AwsQhNTExERkRIYmpxBZWhylYwMTURERAphaHIGqop/JheYOKeJiIhIIQxNzqDK8Bw7moiIiJTB0OQMqqzTxJ4mIiIiZTA0OQPznCZwThMREZFSGJqcgcq8ThPvniMiIlIKQ5MzkKqGJoVrISIiqqMYmpwB5zQREREpjqHJGchzmjg8R0REpBSGJmdQGZpUkoDJaFS4GCIiorqJockZqG7/M5lMDE1ERERKYGhyBpU9TQAgmcoVLISIiKjuYmhyBlVCkzAyNBERESmBockZVA1NHJ4jIiJSBEOTM6hcpwkAINjTREREpASGJmegUsEEqeJnDs8REREpgqHJSZgqe5sEJ4ITEREpgqHJSZhQOUTHOU1ERESKYGhyEkLuaWJoIiIiUgJDk5MwD89xnSYiIiJlMDQ5Cc5pIiIiUhZDk5MwD8+BoYmIiEgRDE1OwtzTxAf2EhERKYOhyVnIoalM4UKIiIjqJoYmJyHfPcfFLYmIiBTB0OQkROXz50wMTURERIpgaHIS5p4mI0MTERGRIhianIXKPDzHOU1ERERKYGhyEkKqGJ4TvHuOiIhIEQxNzsLc02RiTxMREZESGJqchTwRnD1NRERESmBochYqPkaFiIhISQxNTkKq7GniY1SIiIiUwdDkLOS75zg8R0REpASHDk1z586FJEkWr7Zt28r7b926hfj4eNSvXx8+Pj6IiYnBpUuXLI6Rk5OD6OhoeHl5ITAwEK+//jrKyy17a3bt2oVHH30UarUaLVu2RFJSUm2cnm1cKu+eY08TERGRIhw6NAFA+/btkZubK7/27Nkj75s0aRK+/vprrF+/Hqmpqbh48SIGDRok7zcajYiOjkZpaSn27duHTz/9FElJSZg9e7bcJjs7G9HR0ejZsycyMjIwceJEjBo1Ctu3b6/V8/w9ksqt4geGJiIiIkW4Kl3A73F1dYVWq622vaCgAP/85z/xxRdf4JlnngEArF69Gu3atcP+/fvx2GOP4dtvv8WpU6fw3XffISgoCJ07d8Zbb72F6dOnY+7cuXB3d8eqVavQvHlzLFq0CADQrl077NmzB++99x6ioqJq9VzvRaocnmNoIiIiUobD9zSdPXsWwcHBaNGiBWJjY5GTkwMASE9PR1lZGSIjI+W2bdu2RZMmTZCWlgYASEtLQ1hYGIKCguQ2UVFRMBgMOHnypNym6jHMbczH+C0lJSUwGAwWr5okuZgngnNOExERkRIcOjRFREQgKSkJ27Ztw8qVK5GdnY0nn3wSN27cgF6vh7u7O/z9/S1+JygoCHq9HgCg1+stApN5v3nfvdoYDAYUFxf/Zm2JiYnw8/OTXyEhIX/0dO/p9t1zDE1ERERKcOjhub59+8o/d+zYEREREWjatCnWrVsHT09PBSsDZs6cicmTJ8vvDQZDjQYnVWVPk8ThOSIiIkU4dE/Tnfz9/dG6dWucO3cOWq0WpaWlyM/Pt2hz6dIleQ6UVqutdjed+f3vtdFoNPcMZmq1GhqNxuJVkySXyjlNgj1NRERESnCq0FRYWIjz58+jUaNGCA8Ph5ubG1JSUuT9WVlZyMnJgU6nAwDodDpkZmYiLy9PbpOcnAyNRoPQ0FC5TdVjmNuYj+Eoqt49J4RQthgiIqI6yKFD09SpU5GamoqffvoJ+/btw8CBA+Hi4oKhQ4fCz88PI0eOxOTJk7Fz506kp6djxIgR0Ol0eOyxxwAAvXv3RmhoKF5++WUcO3YM27dvx5tvvon4+Hio1WoAwJgxY/Djjz9i2rRpOHPmDFasWIF169Zh0qRJSp56NebhOReYUG5iaCIiIqptDj2n6ZdffsHQoUNx9epVNGzYEE888QT279+Phg0bAgDee+89qFQqxMTEoKSkBFFRUVixYoX8+y4uLti8eTPGjh0LnU4Hb29vxMXFYf78+XKb5s2bY8uWLZg0aRKWLl2Kxo0b4+OPP3ao5QYAQOVa8U/lKplQZjTBzcWh8y4REdEDRxIc67ELg8EAPz8/FBQU1Mj8JuM30+By8B9YVt4fL7/xT/h5utn9M4iIiOoaW76/2V3hJFSVSw64oqKniYiIiGoXQ5OTkFwqepZcUc7QREREpACGJmfhVrH8gQdKUVbOEVUiIqLaxtDkLCpDk6dUilL2NBEREdU6hiZn4VoRmtQoRbmJoYmIiKi2MTQ5C3NPE4fniIiIFMHQ5CyqzGni8BwREVHtY2hyFvKcphLePUdERKQAhiZn4eoBAPBAGUMTERGRAhianIWbFwDAA+xpIiIiUgJDk7Nwq+xpkkpRZuREcCIiotrG0OQsKnuaPFHKniYiIiIFMDQ5C3lOE0MTERGREhianIW5p0kqRVkZQxMREVFtY2hyFpVzmgDAWFasYCFERER1E0OTs6h8jAoAoPSmcnUQERHVUQxNzsLFFeVwBQCUl7KniYiIqLYxNDmRcpeKIbrS4kKFKyEiIqp7GJqcSLlKDQC4VVykcCVERER1D0OTEzFVzmsqvcXQREREVNsYmpyIqByeK2NPExERUa1jaHIiwq2ip6mcd88RERHVOoYmJyJVhiZjCUMTERFRbWNociIq94pVwU3saSIiIqp1DE1OxEVd0dMkym4pXAkREVHdw9DkRFzV3gAAN+NNlJbz+XNERES1iaHJibhqggAADaUC3LhVpnA1REREdQtDkxNR+TUGAARLV2G4Va5wNURERHULQ5Mz0QQDALTSNRQUs6eJiIioNjE0OZPKnqZG0lUYGJqIiIhqFUOTM6nsaQrCdVwvKla4GCIiorqFocmZ+ATBCBe4SiZcvPCT0tUQERHVKQxNzkTlglsegQAA/YUfFS6GiIiobmFocjZ+FUN05Zd/gMkkFC6GiIio7mBocjIerZ4GAPQ17sIPeTeULYaIiKgOYWhyMi5dhsMECU+4nMS3/1qAX68VKl0SERFRncDQ5Gz8m+Bm6AsAgPFFH6BsaTjWr5qLzJ/0ChdGRET0YJOEEJwYYwcGgwF+fn4oKCiARqOp2Q8zGXF5+9/hcfAD+IqKIbqrwhfJPv0R2CseT3duB5VKqtkaiIiIHgC2fH8zNNlJrYYms9Ii/LrzQ3gcXoX6ZRU9TcXCHdvcI6F6PAFRTzwGDzeX2qmFiIjICTE0KUCR0GRmLEf+kf/g1q7F0BZlVWwSEnaruuJGxxHo2efP8PV0r92aiIiInABDkwIUDU1mQqD4h524+u1CNL66T958TjTGseDBCHl6BLq0CuHQHRERUSWGJgU4RGiqokx/GjnbliD4p03wxC0AgEF4YptrL5R2eAE9nnwGTRp4K1wlERGRsmz5/ubdc3dYvnw5mjVrBg8PD0RERODgwYNKl3Rf3LTt8PDwf8BjehZyus3GFffG0EjFGGzcjJeOvQS83wlfLxyBlO2bcMVwU+lyiYiIHB57mqpYu3Ythg0bhlWrViEiIgJLlizB+vXrkZWVhcDAwHv+rqP1NFVjMqH0h2RcSf0QDfS74S5K5V3XhA9+cG8PQ8NH4dFCh6ahOoRoG3IYj4iIHngcnrtPERER6Nq1K5YtWwYAMJlMCAkJwbhx4zBjxox7/q7Dh6aqSotw/fhW5B36Lx7KS4WPKKrWJFfUQ56LFjfUWpR6N4KrTwO4+TaAi6cfXD184O7pAzdPH7i4ukKSVHBRqSCpVJAkqfK9CyBJkFQqQGL4IiKiP07t6YN6gQ/Z9Zi2fH+72vWTnVhpaSnS09Mxc+ZMeZtKpUJkZCTS0tKqtS8pKUFJSYn83mAw1EqdduHujYAuf0ZAlz8DxjJcO3sAuSd2Q/XrQQQWHEd901U0kq6hkekaUHwKKAZwRemiiYiorjvs2wv1pmxQ7PMZmipduXIFRqMRQUFBFtuDgoJw5syZau0TExMxb9682iqv5ri4oV7bJ1Cv7RPyJmPRNVz7+SQK9D/i1pWfYMy/CNPNq3AtyYe6vBBupltwNxXDXZRABRNUEAAEVDBBAiCJyv+sfK+CSaGTIyKiB4lQKRtbGJru08yZMzF58mT5vcFgQEhIiIIV2Y+Ldz00DH0SDUOfVLoUIiIiWVeFP5+hqVKDBg3g4uKCS5cuWWy/dOkStFpttfZqtRpqtbq2yiMiIiKFccmBSu7u7ggPD0dKSoq8zWQyISUlBTqdTsHKiIiIyBGwp6mKyZMnIy4uDl26dEG3bt2wZMkSFBUVYcSIEUqXRkRERApjaKpiyJAhuHz5MmbPng29Xo/OnTtj27Zt1SaHExERUd3DdZrsxKnWaSIiIiIAfIwKERERkd0xNBERERFZgaGJiIiIyAoMTURERERWYGgiIiIisgJDExEREZEVGJqIiIiIrMDQRERERGQFhiYiIiIiK/AxKnZiXljdYDAoXAkRERFZy/y9bc0DUhia7OTGjRsAgJCQEIUrISIiIlvduHEDfn5+92zDZ8/ZiclkwsWLF+Hr6wtJkux6bIPBgJCQEFy4cIHPtatBvM61g9e59vBa1w5e59pTE9daCIEbN24gODgYKtW9Zy2xp8lOVCoVGjduXKOfodFo+F/IWsDrXDt4nWsPr3Xt4HWuPfa+1r/Xw2TGieBEREREVmBoIiIiIrICQ5MTUKvVmDNnDtRqtdKlPNB4nWsHr3Pt4bWuHbzOtUfpa82J4ERERERWYE8TERERkRUYmoiIiIiswNBEREREZAWGJiIiIiIrMDQ5uOXLl6NZs2bw8PBAREQEDh48qHRJTmX37t3o168fgoODIUkSNm3aZLFfCIHZs2ejUaNG8PT0RGRkJM6ePWvR5tq1a4iNjYVGo4G/vz9GjhyJwsLCWjwLx5eYmIiuXbvC19cXgYGBGDBgALKysiza3Lp1C/Hx8ahfvz58fHwQExODS5cuWbTJyclBdHQ0vLy8EBgYiNdffx3l5eW1eSoOb+XKlejYsaO8uJ9Op8PWrVvl/bzONePdd9+FJEmYOHGivI3X+o+bO3cuJEmyeLVt21be72jXmKHJga1duxaTJ0/GnDlzcOTIEXTq1AlRUVHIy8tTujSnUVRUhE6dOmH58uV33b9gwQK8//77WLVqFQ4cOABvb29ERUXh1q1bcpvY2FicPHkSycnJ2Lx5M3bv3o3Ro0fX1ik4hdTUVMTHx2P//v1ITk5GWVkZevfujaKiIrnNpEmT8PXXX2P9+vVITU3FxYsXMWjQIHm/0WhEdHQ0SktLsW/fPnz66adISkrC7NmzlTglh9W4cWO8++67SE9Px+HDh/HMM8+gf//+OHnyJABe55pw6NAh/OMf/0DHjh0ttvNa20f79u2Rm5srv/bs2SPvc7hrLMhhdevWTcTHx8vvjUajCA4OFomJiQpW5bwAiI0bN8rvTSaT0Gq1YuHChfK2/Px8oVarxZdffimEEOLUqVMCgDh06JDcZuvWrUKSJPHrr7/WWu3OJi8vTwAQqampQoiK6+rm5ibWr18vtzl9+rQAINLS0oQQQnzzzTdCpVIJvV4vt1m5cqXQaDSipKSkdk/AyQQEBIiPP/6Y17kG3LhxQ7Rq1UokJyeLp556SkyYMEEIwb9pe5kzZ47o1KnTXfc54jVmT5ODKi0tRXp6OiIjI+VtKpUKkZGRSEtLU7CyB0d2djb0er3FNfbz80NERIR8jdPS0uDv748uXbrIbSIjI6FSqXDgwIFar9lZFBQUAADq1asHAEhPT0dZWZnFtW7bti2aNGlica3DwsIQFBQkt4mKioLBYJB7UciS0WjEmjVrUFRUBJ1Ox+tcA+Lj4xEdHW1xTQH+TdvT2bNnERwcjBYtWiA2NhY5OTkAHPMa84G9DurKlSswGo0WfwgAEBQUhDNnzihU1YNFr9cDwF2vsXmfXq9HYGCgxX5XV1fUq1dPbkOWTCYTJk6ciO7du6NDhw4AKq6ju7s7/P39Ldreea3v9m9h3ke3ZWZmQqfT4datW/Dx8cHGjRsRGhqKjIwMXmc7WrNmDY4cOYJDhw5V28e/afuIiIhAUlIS2rRpg9zcXMybNw9PPvkkTpw44ZDXmKGJiOwqPj4eJ06csJiXQPbVpk0bZGRkoKCgAP/5z38QFxeH1NRUpct6oFy4cAETJkxAcnIyPDw8lC7ngdW3b1/5544dOyIiIgJNmzbFunXr4OnpqWBld8fhOQfVoEEDuLi4VLtL4NKlS9BqtQpV9WAxX8d7XWOtVltt4n15eTmuXbvGf4e7SEhIwObNm7Fz5040btxY3q7ValFaWor8/HyL9nde67v9W5j30W3u7u5o2bIlwsPDkZiYiE6dOmHp0qW8znaUnp6OvLw8PProo3B1dYWrqytSU1Px/vvvw9XVFUFBQbzWNcDf3x+tW7fGuXPnHPLvmaHJQbm7uyM8PBwpKSnyNpPJhJSUFOh0OgUre3A0b94cWq3W4hobDAYcOHBAvsY6nQ75+flIT0+X2+zYsQMmkwkRERG1XrOjEkIgISEBGzduxI4dO9C8eXOL/eHh4XBzc7O41llZWcjJybG41pmZmRYhNTk5GRqNBqGhobVzIk7KZDKhpKSE19mOevXqhczMTGRkZMivLl26IDY2Vv6Z19r+CgsLcf78eTRq1Mgx/57tPrWc7GbNmjVCrVaLpKQkcerUKTF69Gjh7+9vcZcA3duNGzfE0aNHxdGjRwUAsXjxYnH06FHx888/CyGEePfdd4W/v7/43//+J44fPy769+8vmjdvLoqLi+Vj9OnTRzzyyCPiwIEDYs+ePaJVq1Zi6NChSp2SQxo7dqzw8/MTu3btErm5ufLr5s2bcpsxY8aIJk2aiB07dojDhw8LnU4ndDqdvL+8vFx06NBB9O7dW2RkZIht27aJhg0bipkzZypxSg5rxowZIjU1VWRnZ4vjx4+LGTNmCEmSxLfffiuE4HWuSVXvnhOC19oepkyZInbt2iWys7PF3r17RWRkpGjQoIHIy8sTQjjeNWZocnAffPCBaNKkiXB3dxfdunUT+/fvV7okp7Jz504BoNorLi5OCFGx7MCsWbNEUFCQUKvVolevXiIrK8viGFevXhVDhw4VPj4+QqPRiBEjRogbN24ocDaO627XGIBYvXq13Ka4uFi89tprIiAgQHh5eYmBAweK3Nxci+P89NNPom/fvsLT01M0aNBATJkyRZSVldXy2Ti2V155RTRt2lS4u7uLhg0bil69esmBSQhe55p0Z2jitf7jhgwZIho1aiTc3d3FQw89JIYMGSLOnTsn73e0aywJIYT9+6+IiIiIHiyc00RERERkBYYmIiIiIiswNBERERFZgaGJiIiIyAoMTURERERWYGgiIiIisgJDExEREZEVGJqIiIiIrMDQRERUQyRJwqZNm5Qug4jshKGJiB5Iw4cPhyRJ1V59+vRRujQiclKuShdARFRT+vTpg9WrV1tsU6vVClVDRM6OPU1E9MBSq9XQarUWr4CAAAAVQ2crV65E37594enpiRYtWuA///mPxe9nZmbimWeegaenJ+rXr4/Ro0ejsLDQos0nn3yC9u3bQ61Wo1GjRkhISLDYf+XKFQwcOBBeXl5o1aoVvvrqq5o9aSKqMQxNRFRnzZo1CzExMTh27BhiY2Pxwgsv4PTp0wCAoqIiREVFISAgAIcOHcL69evx3XffWYSilStXIj4+HqNHj0ZmZia++uortGzZ0uIz5s2bh8GDB+P48eN49tlnERsbi2vXrtXqeRKRnQgiogdQXFyccHFxEd7e3havt99+WwghBAAxZswYi9+JiIgQY8eOFUII8eGHH4qAgABRWFgo79+yZYtQqVRCr9cLIYQIDg4Wb7zxxm/WAEC8+eab8vvCwkIBQGzdutVu50lEtYdzmojogdWzZ0+sXLnSYlu9evXkn3U6ncU+nU6HjIwMAMDp06fRqVMneHt7y/u7d+8Ok8mErKwsSJKEixcvolevXvesoWPHjvLP3t7e0Gg0yMvLu99TIiIFMTQR0QPL29u72nCZvXh6elrVzs3NzeK9JEkwmUw1URIR1TDOaSKiOmv//v3V3rdr1w4A0K5dOxw7dgxFRUXy/r1790KlUqFNmzbw9fVFs2bNkJKSUqs1E5Fy2NNERA+skpIS6PV6i22urq5o0KABAGD9+vXo0qULnnjiCXz++ec4ePAg/vnPfwIAYmNjMWfOHMTFxWHu3Lm4fPkyxo0bh5dffhlBQUEAgLlz52LMmDEIDAxE3759cePGDezduxfjxo2r3RMlolrB0ERED6xt27ahUaNGFtvatGmDM2fOAKi4s23NmjV47bXX0KhRI3z55ZcIDQ0FAHh5eWH79u2YMGECunbtCi8vL8TExGDx4sXyseLi4nDr1i289957mDp1Kho0aIA///nPtXeCRFSrJCGEULoIIqLaJkkSNm7ciAEDBihdChE5Cc5pIiIiIrICQxMRERGRFTiniYjqJM5MICJbsaeJiIiIyAoMTURERERWYGgiIiIisgJDExEREZEVGJqIiIiIrMDQRERERGQFhiYiIiIiKzA0EREREVnh/wPka2LmrgAQ3AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a new Celsius value for prediction (not present in the larger dataset)\n",
        "new_celsius_value_large = 150.0\n",
        "\n",
        "# Reshape the input for prediction\n",
        "new_celsius_value_large = np.array([new_celsius_value_large]).reshape(-1, 1)\n",
        "\n",
        "# Use the trained model to predict the corresponding Fahrenheit value\n",
        "predicted_fahrenheit_large = model_final.predict(new_celsius_value_large)\n",
        "\n",
        "# Print the predicted Fahrenheit value\n",
        "print(f\"The predicted Fahrenheit value for {new_celsius_value_large[0, 0]} degrees Celsius is {predicted_fahrenheit_large[0, 0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnsJZCb7gLmx",
        "outputId": "4ac51a30-8c7e-4af3-ca71-6abd42c264f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 40ms/step\n",
            "The predicted Fahrenheit value for 150.0 degrees Celsius is 302.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the weights of the trained model\n",
        "weights = model_final.get_weights()\n",
        "\n",
        "# Print the weights\n",
        "for layer_num, layer_weights in enumerate(weights):\n",
        "    print(f\"Layer {layer_num + 1} Weights:\")\n",
        "    print(layer_weights)\n",
        "    print(\"------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jsjYfESgPNm",
        "outputId": "c7204932-f813-4113-d0bd-620e2e2258d9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 Weights:\n",
            "[[1.8]]\n",
            "------------------------\n",
            "Layer 2 Weights:\n",
            "[31.999998]\n",
            "------------------------\n"
          ]
        }
      ]
    }
  ]
}